import torch
import torch.nn as nn
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from typing import Dict

import sys
from pathlib import Path
def setup_miniqmt_import_root():
    """
    é€’å½’æŸ¥æ‰¾ 'miniQMT' æ–‡ä»¶å¤¹ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° sys.path ä¸­ï¼Œ
    ä»è€Œå…è®¸ä½¿ç”¨ miniQMT ä¸ºæ ¹çš„ç»å¯¹å¯¼å…¥ã€‚
    """
    # 1. è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
    # stack[0] æ˜¯å½“å‰æ­£åœ¨æ‰§è¡Œçš„å¸§ï¼Œå…¶ f_globals['__file__'] æ˜¯è„šæœ¬è·¯å¾„
    try:
        # è·å–è°ƒç”¨æ­¤å‡½æ•°çš„è„šæœ¬çš„è·¯å¾„
        calling_script_path = Path(sys._getframe(1).f_globals['__file__']).resolve()
    except KeyError:
        # å¦‚æœåœ¨äº¤äº’å¼ç¯å¢ƒæˆ–æŸäº›ç‰¹æ®Šç¯å¢ƒä¸­ï¼Œå¯èƒ½æ— æ³•è·å–æ–‡ä»¶è·¯å¾„ï¼Œåˆ™é€€å‡º
        print("âš ï¸ è­¦å‘Š: æ— æ³•ç¡®å®šå½“å‰è„šæœ¬è·¯å¾„ï¼Œè·³è¿‡è·¯å¾„è®¾ç½®ã€‚")
        return
    
    current_path = calling_script_path
    miniqmt_root = None
    
    # 2. å‘ä¸Šé€’å½’æŸ¥æ‰¾
    # current_path.parents æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰çˆ¶ç›®å½•çš„åºåˆ—
    for parent in [current_path] + list(current_path.parents):
        if parent.name == 'miniQMT':
            miniqmt_root = parent
            break
        
    # 3. æ£€æŸ¥å¹¶æ·»åŠ è·¯å¾„
    if miniqmt_root:
        # å°†æ‰¾åˆ°çš„ miniQMT ç›®å½•æ·»åŠ åˆ° sys.path
        miniqmt_root_str = str(miniqmt_root)
        if miniqmt_root_str not in sys.path:
            sys.path.insert(0, miniqmt_root_str)
            print(f"âœ… æˆåŠŸå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°æœç´¢è·¯å¾„: {miniqmt_root_str}")
        else:
            # å·²ç»æ·»åŠ è¿‡ï¼Œæ— éœ€é‡å¤æ·»åŠ 
            # print(f"â„¹ï¸ é¡¹ç›®æ ¹ç›®å½•å·²åœ¨æœç´¢è·¯å¾„ä¸­: {miniqmt_root_str}")
            pass
    else:
        print("âŒ é”™è¯¯: æœªèƒ½åœ¨å½“å‰è·¯å¾„æˆ–å…¶ä»»ä½•çˆ¶ç›®å½•ä¸­æ‰¾åˆ° 'miniQMT' æ–‡ä»¶å¤¹ã€‚")
setup_miniqmt_import_root()
from DL.preTrain.preMOE import PreMOE 

class ViewProjector(nn.Module):
    def __init__(self, high_dim, low_dim, out_dim=48):
        super().__init__()
        self.high_net = nn.Sequential(
            nn.LayerNorm(high_dim),
            nn.Linear(high_dim, out_dim),
        )
        self.low_net = nn.Sequential(
            nn.LayerNorm(low_dim),
            nn.Linear(low_dim, 32),
            nn.GELU(),
        )
        self.fusion = nn.Sequential(
            nn.Linear(out_dim + 32, out_dim),
            nn.LayerNorm(out_dim),
        )

    def forward(self, x_high: torch.Tensor, x_low: torch.Tensor):
        h = self.high_net(x_high)
        l = self.low_net(x_low)
        return self.fusion(torch.cat([h, l], dim=-1))


class MultiViewAdapter(nn.Module):
    def __init__(self, dims_dict: Dict[str, int], final_dim=128, view_dim=48):
        super().__init__()
        self.varma_proj = ViewProjector(dims_dict["varma_h"], dims_dict["varma_l"], out_dim=view_dim)
        self.basis_proj = ViewProjector(dims_dict["basis_h"], dims_dict["basis_l"], out_dim=view_dim)
        self.itrans_proj = ViewProjector(dims_dict["itrans_h"], dims_dict["itrans_l"], out_dim=view_dim)
        self.router_proj = nn.Sequential(
            nn.LayerNorm(dims_dict["router"]),
            nn.Linear(dims_dict["router"], 32),
        )
        self.final_net = nn.Sequential(
            nn.Linear(view_dim * 3 + 32, final_dim),
            nn.LayerNorm(final_dim),
        )

    def forward(self, tok: Dict[str, torch.Tensor]):
        v_varma = self.varma_proj(tok["varma_h"], tok["varma_l"])
        v_basis = self.basis_proj(tok["basis_h"], tok["basis_l"])
        v_itrans = self.itrans_proj(tok["itrans_h"], tok["itrans_l"])
        v_router = self.router_proj(tok["router"])
        return self.final_net(torch.cat([v_varma, v_basis, v_itrans, v_router], dim=-1))


class HybridOptionExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, cfg):
        # 1. æå–åŸºç¡€ç»´åº¦ (ä¸èµ‹å€¼ç»™ selfï¼Œä»…ä½œä¸ºå±€éƒ¨å˜é‡è®¡ç®—)
        curr_dim = observation_space["curr"].shape[0]
        hist_total_dim = observation_space["hist"].shape[1]
        n_variates = hist_total_dim // 2
        adapter_final_dim = 128 

        self.device = cfg.device
        
        # 2. ä¸´æ—¶åˆ›å»ºä¸€ä¸ª PreMOE å®ä¾‹æ¥æ¨å¯¼ç»´åº¦
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¿˜æ²¡è°ƒç”¨ super().__init__ï¼Œæ‰€ä»¥ä¸èƒ½èµ‹ç»™ self.pre_moe
        temp_moe = PreMOE(
            seq_len=cfg.window_size, 
            pred_len=cfg.pre_len, 
            n_variates=n_variates, 
            d_router=cfg.d_router
        )
        
        with torch.no_grad():
            dummy_input = torch.zeros(1, cfg.window_size, n_variates)
            toks = temp_moe.encode_tokens(dummy_input)
            dims_dict = {k: v.shape[-1] for k, v in toks.items()}
            
        # 3. è‡ªåŠ¨è®¡ç®—æ€»ç‰¹å¾ç»´åº¦
        # è´¦æˆ·(9) + ç‰©ç†ç›´è¿(26) + è¯­ä¹‰(128*2) = 291
        total_features_dim = curr_dim + hist_total_dim + (adapter_final_dim * 2)
        
        # 4. ğŸ”¥ ã€æ ¸å¿ƒä¿®æ­£ã€‘å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œå†è¿›è¡Œæ¨¡å—èµ‹å€¼
        super(HybridOptionExtractor, self).__init__(observation_space, features_dim=total_features_dim)
        
        # 5. ç°åœ¨å¯ä»¥å®‰å…¨åœ°èµ‹å€¼äº†
        self.curr_dim = curr_dim
        self.hist_total_dim = hist_total_dim
        self.pre_moe = temp_moe # å°†åˆšæ‰åˆ›å»ºçš„å®ä¾‹æŒ‚è½½åˆ° self
        
        # åŠ è½½æƒé‡å¹¶å†»ç»“
        self.pre_moe.load_state_dict(torch.load(cfg.pretrained_path, map_location=self.device), strict=False)
        self.pre_moe.eval()
        for p in self.pre_moe.parameters():
            p.requires_grad = False
            
        # åˆå§‹åŒ– Adapter
        self.adapter = MultiViewAdapter(dims_dict=dims_dict, final_dim=adapter_final_dim).to(self.device)
        
        print(f"[Network] Auto-calculated feature_dim: {total_features_dim}")

    def forward(self, observations):
        hist = observations["hist"] 
        curr = observations["curr"] 
        
        # 1. æ‹†åˆ†åºåˆ—
        call_seq, put_seq = torch.chunk(hist, 2, dim=2)
        
        # 2. ç‰©ç†ç°çŠ¶ç›´è¿ (æœ€åä¸€å¸§)
        phys_call = call_seq[:, -1, :] 
        phys_put = put_seq[:, -1, :]
        
        # 3. Transformer è¯­ä¹‰é™ç»´
        with torch.no_grad():
            c_tok = self.pre_moe.encode_tokens(call_seq)
            p_tok = self.pre_moe.encode_tokens(put_seq)
            
        c_latent = self.adapter(c_tok)
        p_latent = self.adapter(p_tok)
        
        # 4. æœ€ç»ˆæ‹¼æ¥
        combined = torch.cat([
            curr, 
            phys_call, 
            phys_put, 
            c_latent, 
            p_latent
        ], dim=-1)
        
        return combined