"""
    PPOç®—æ³• (æ ‡å‡†ç²¾åº¦ Float32 ç‰ˆ) - åŠ¨æ€å¹¶è¡Œè®­ç»ƒé‡æ„ç‰ˆ (Full Optimized)
    åŒ…å«: Multiprocessing Parallellism + Excel Export + Dynamic Environment Loading + Data Caching (Shared Memory)
    ä¿®å¤: 
    1. DynamicWindowEnv å¢åŠ  close æ–¹æ³•ï¼Œä¿®å¤ AttributeErrorã€‚
    2. DataCache ä½¿ç”¨ multiprocessing.Manager å…±äº«å†…å­˜ï¼Œè§£å†³å¤šè¿›ç¨‹é‡å¤è¯»å–å¯¼è‡´çš„ Miss åˆ·å±ã€‚

    è¿™ä¸ªæ˜¯2025.12.14ç‰ˆæœ¬, ç›®å‰å·²ç»å®ç°äº†å¤šçº¿ç¨‹, æ•´ä¸ªrolloutæ›´æ–°çš„æ“ä½œ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.distributions import Categorical
from windowEnv_parallel_fast import windowEnv
import time, json
import sys
import pandas as pd
from datetime import datetime, timedelta

from typing import Any, Dict, List, Optional, Tuple
from tools.Norm import Normalization, RewardNormalization, RewardScaling
from preTrain.preMOE import PreMOE
from dataclasses import dataclass, field
import random
import multiprocessing as mp
from finTool.single_window_account_fast import single_Account
import os
import traceback
from datetime import datetime, timedelta

import warnings
# å¿½ç•¥æ‰€æœ‰ FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning)


# =========================
# Constants / actions
# =========================
A_HOLD, A_LONG, A_SHORT, A_CLOSE = 0, 1, 2, 3
WEIGHT_BINS = np.array([0.00, 0.25, 0.50, 0.75, 1.00], dtype=np.float32)


# è¾“å‡ºç±», è¾“å‡ºæ—¥å¿—, é˜²æ­¢ä¸­æ–­åçœ‹ä¸åˆ°ä¿¡æ¯
class outPut():
    """
    è‡ªå®šä¹‰è¾“å‡ºç±»ï¼Œå°†è¾“å‡ºåŒæ—¶å†™å…¥ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶ã€‚
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š'w' (è¦†ç›–é‡å†™) å’Œ 'a' (ç»­å†™)ã€‚
    """
    def __init__(self, filename, mode='w'):
        """
        åˆå§‹åŒ– outPut å®ä¾‹ã€‚

        :param filename: æ—¥å¿—æ–‡ä»¶åã€‚
        :param mode: æ–‡ä»¶æ‰“å¼€æ¨¡å¼ï¼Œ'w' ä¸ºè¦†ç›–é‡å†™ï¼Œ'a' ä¸ºç»­å†™ã€‚é»˜è®¤ä¸º 'w'ã€‚
        """
        # æ£€æŸ¥ mode å‚æ•°æ˜¯å¦åˆæ³•
        if mode not in ['w', 'a']:
            raise ValueError("mode å‚æ•°å¿…é¡»æ˜¯ 'w' (è¦†ç›–) æˆ– 'a' (ç»­å†™)")

        self.terminal = sys.stdout
        # æ ¹æ® mode å‚æ•°æ‰“å¼€æ–‡ä»¶
        self.logfile = open(filename, mode, encoding="utf-8")
        
        # å¯é€‰ï¼šæ‰“å°å½“å‰æ¨¡å¼åˆ°ç»ˆç«¯ï¼Œæ–¹ä¾¿è°ƒè¯•
        print(f"æ—¥å¿—æ–‡ä»¶ '{filename}' å·²ä»¥æ¨¡å¼ '{mode}' æ‰“å¼€ã€‚")


    def write(self, message):
        """å°†æ¶ˆæ¯åŒæ—¶å†™å…¥ç»ˆç«¯å’Œæ—¥å¿—æ–‡ä»¶ã€‚"""
        self.terminal.write(message)
        self.logfile.write(message)

    def flush(self):
        """å¼ºåˆ¶å°†ç¼“å†²åŒºå†…å®¹å†™å…¥ç›®æ ‡ï¼ˆç»ˆç«¯å’Œæ–‡ä»¶ï¼‰ã€‚"""
        self.terminal.flush()
        self.logfile.flush()

    def close(self):
        """å…³é—­æ—¥å¿—æ–‡ä»¶ã€‚"""
        self.logfile.close()
        print("æ—¥å¿—æ–‡ä»¶å·²å…³é—­ã€‚")

# =========================
# Normalization (n==0 -> return x)
# =========================
# class RunningMeanStd:
#     def __init__(self, shape, dtype=torch.float32, eps=1e-8, device="cpu"):
#         self.eps = float(eps)
#         self.n = 0  # keep int
#         self.mean = torch.zeros(shape, dtype=dtype, device=device)
#         self.var = torch.ones(shape, dtype=dtype, device=device)
#         self.std = torch.sqrt(self.var).clamp_min(self.eps)

#     def update(self, x: torch.Tensor):
#         if x.dim() == 1:
#             x = x.unsqueeze(0)
#         B = int(x.shape[0])
#         if B <= 0:
#             return

#         n_old = int(self.n)
#         n_new = n_old + B

#         if n_old == 0:
#             mean = x.mean(dim=0)
#             var = x.var(dim=0, unbiased=False)
#             self.mean = mean
#             self.var = var
#             self.std = torch.sqrt(self.var.clamp_min(self.eps))
#             self.n = n_new
#             return

#         old_mean = self.mean
#         old_var = self.var
#         batch_mean = x.mean(dim=0)
#         batch_var = x.var(dim=0, unbiased=False)

#         delta = batch_mean - old_mean
#         mean = old_mean + delta * (B / n_new)

#         m2_old = old_var * n_old
#         m2_batch = batch_var * B
#         m2 = m2_old + m2_batch + (delta ** 2) * (n_old * B / n_new)
#         var = m2 / n_new

#         self.mean = mean
#         self.var = var
#         self.std = torch.sqrt(self.var.clamp_min(self.eps))
#         self.n = n_new


# class Normalization:
#     def __init__(self, shape, dtype=torch.float32, eps=1e-8, device="cpu"):
#         self.running_ms = RunningMeanStd(shape=shape, dtype=dtype, eps=eps, device=device)
#         self.eps = float(eps)

#     def __call__(self, x: torch.Tensor, update=True):
#         if update:
#             self.running_ms.update(x.detach())
#         if int(self.running_ms.n) == 0:
#             return x
#         return (x - self.running_ms.mean) / (self.running_ms.std + self.eps)

#     def state_dict(self):
#         return {
#             "n": int(self.running_ms.n),
#             "mean": self.running_ms.mean.detach().cpu(),
#             "var": self.running_ms.var.detach().cpu(),
#             "std": self.running_ms.std.detach().cpu(),
#             "eps": self.eps,
#         }

#     def load_state_dict(self, d: Dict[str, Any], device="cpu"):
#         self.eps = float(d.get("eps", self.eps))
#         self.running_ms.n = int(d["n"])
#         self.running_ms.mean = d["mean"].to(device)
#         self.running_ms.var = d["var"].to(device)
#         self.running_ms.std = d.get("std", torch.sqrt(self.running_ms.var)).to(device)


# =========================
# Networks
# =========================
class ActorDualHead(nn.Module):
    def __init__(self, state_dim, hidden_dim=256, n_actions=4, n_weights=5):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.action_head = nn.Linear(hidden_dim, n_actions)
        self.weight_head = nn.Linear(hidden_dim, n_weights)

    def forward(self, x: torch.Tensor):
        if x.dim() == 1:
            x = x.unsqueeze(0)
        z = self.backbone(x.float())
        return self.action_head(z), self.weight_head(z)


class ValueNet(nn.Module):
    def __init__(self, state_dim, hidden_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
        )

    def forward(self, x: torch.Tensor):
        if x.dim() == 1:
            x = x.unsqueeze(0)
        return self.net(x.float())


class ViewProjector(nn.Module):
    def __init__(self, high_dim, low_dim, out_dim=48):
        super().__init__()
        self.high_net = nn.Sequential(
            nn.LayerNorm(high_dim),
            nn.Linear(high_dim, out_dim),
        )
        self.low_net = nn.Sequential(
            nn.LayerNorm(low_dim),
            nn.Linear(low_dim, 32),
            nn.GELU(),
        )
        self.fusion = nn.Sequential(
            nn.Linear(out_dim + 32, out_dim),
            nn.LayerNorm(out_dim),
        )

    def forward(self, x_high: torch.Tensor, x_low: torch.Tensor):
        h = self.high_net(x_high)
        l = self.low_net(x_low)
        return self.fusion(torch.cat([h, l], dim=-1))


class MultiViewAdapter(nn.Module):
    def __init__(self, dims_dict: Dict[str, int], final_dim=128, view_dim=48):
        super().__init__()
        self.varma_proj = ViewProjector(dims_dict["varma_high"], dims_dict["varma_low"], out_dim=view_dim)
        self.basis_proj = ViewProjector(dims_dict["basis_high"], dims_dict["basis_low"], out_dim=view_dim)
        self.itrans_proj = ViewProjector(dims_dict["itrans_high"], dims_dict["itrans_low"], out_dim=view_dim)
        self.router_proj = nn.Sequential(
            nn.LayerNorm(dims_dict["router"]),
            nn.Linear(dims_dict["router"], 32),
        )
        self.final_net = nn.Sequential(
            nn.Linear(view_dim * 3 + 32, final_dim),
            nn.LayerNorm(final_dim),
        )

    def forward(self, tok: Dict[str, torch.Tensor]):
        v_varma = self.varma_proj(tok["varma_h"], tok["varma_l"])
        v_basis = self.basis_proj(tok["basis_h"], tok["basis_l"])
        v_itrans = self.itrans_proj(tok["itrans_h"], tok["itrans_l"])
        v_router = self.router_proj(tok["router"])
        return self.final_net(torch.cat([v_varma, v_basis, v_itrans, v_router], dim=-1))


# =========================
# Feature pipeline (encode_tokens ONLY)
# =========================
class FeaturePipeline:
    def __init__(self, extractor: PreMOE, device="cpu", adapter_dim=128):
        self.device = device
        self.extractor = extractor.to(device)
        self.extractor.eval()
        for p in self.extractor.parameters():
            p.requires_grad = False

        self.adapter_dim = adapter_dim
        self.adapter: Optional[MultiViewAdapter] = None
        self.norm: Optional[Normalization] = None

    @torch.no_grad()
    def _encode_tokens_only(self, call_state: torch.Tensor, put_state: torch.Tensor):
        # IMPORTANT: only encode_tokens (no forward/predict)
        call_tok = self.extractor.encode_tokens(call_state)
        put_tok = self.extractor.encode_tokens(put_state)
        return call_tok, put_tok

    def build_adapter_from_dims(self, dims: Dict[str, int]):
        if self.adapter is None:
            self.adapter = MultiViewAdapter(dims, final_dim=self.adapter_dim).to(self.device)

    def build_norm_if_needed(self, state_dim: int):
        if self.norm is None:
            self.norm = Normalization(shape=(state_dim,), device=self.device)

    @torch.no_grad()
    def obs_to_state_raw(self, curr: torch.Tensor, hist: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, int]]:
        """
        curr: (1,Dc)  hist: (1,L,Dh)
        returns raw_state (1, state_dim) and dims_dict (for adapter init)
        """
        if curr.dim() == 1:
            curr = curr.unsqueeze(0)
        if hist.dim() == 2:
            hist = hist.unsqueeze(0)

        call_state, put_state = torch.chunk(hist, chunks=2, dim=2)
        physical_call, physical_put = call_state[:, -1, :], put_state[:, -1, :]
        call_tok, put_tok = self._encode_tokens_only(call_state, put_state)

        dims = {
            "varma_high": int(call_tok["varma_h"].shape[-1]),
            "varma_low": int(call_tok["varma_l"].shape[-1]),
            "basis_high": int(call_tok["basis_h"].shape[-1]),
            "basis_low": int(call_tok["basis_l"].shape[-1]),
            "itrans_high": int(call_tok["itrans_h"].shape[-1]),
            "itrans_low": int(call_tok["itrans_l"].shape[-1]),
            "router": int(call_tok["router"].shape[-1]),
        }
        self.build_adapter_from_dims(dims)
        assert self.adapter is not None

        reduce_call = self.adapter(call_tok)
        reduce_put = self.adapter(put_tok)
        # raw = torch.cat([curr.float(), reduce_call.float(), reduce_put.float()], dim=-1)
        raw = torch.cat([curr.float(),  physical_call.float(), physical_put.float(), reduce_call.float(), reduce_put.float()], dim=-1)
        return raw, dims

    @torch.no_grad()
    def obs_to_state_normed(self, curr: torch.Tensor, hist: torch.Tensor, update_norm=False) -> Tuple[torch.Tensor, Dict[str, int]]:
        raw, dims = self.obs_to_state_raw(curr, hist)
        self.build_norm_if_needed(state_dim=int(raw.shape[-1]))
        assert self.norm is not None
        return self.norm(raw, update=update_norm), dims


# =========================
# Env wrapper (task switching)
# =========================
class DynamicWindowEnv:
    def __init__(self, option_pairs: List[Dict[str, Any]], cfg: Dict[str, Any], seed=0):
        self.all_pairs = option_pairs
        self.cfg = cfg
        self.fixed_idx: Optional[int] = None
        self.current_env = None
        random.seed(seed)

    def set_task(self, idx: int):
        self.fixed_idx = int(idx)

    def set_fee(self, fee: float):
        self.cfg['fee'] = fee

    def reset(self):
        if self.current_env is not None:
            try:
                self.current_env.close()
            except Exception:
                pass
            self.current_env = None

        if self.fixed_idx is None:
            pair = random.choice(self.all_pairs)
        else:
            pair = self.all_pairs[self.fixed_idx % len(self.all_pairs)]


        self.current_env = windowEnv(
            init_capital=self.cfg["init_capital"],
            call=pair["call"],
            put=pair["put"],
            fee=self.cfg["fee"],
            start_time=pair.get("start_time", self.cfg["start_time"]),
            end_time=pair.get("end_time", self.cfg["end_time"]),
            benchmark=self.cfg["benchmark"],
            timesteps=pair['steps'] + 1,
        )
        return self.current_env.reset()

    def step(self, action: int, weight: float):
        return self.current_env.step(action, weight)

    def close(self):
        if self.current_env is not None:
            try:
                self.current_env.close()
            except Exception:
                pass
            self.current_env = None

    @property
    def account_controller(self):
        return self.current_env.account_controller


# =========================
# IPC wrapper
# =========================
class CloudpickleWrapper:
    def __init__(self, x):
        self.x = x

    def __getstate__(self):
        import cloudpickle
        return cloudpickle.dumps(self.x)

    def __setstate__(self, ob):
        import pickle
        self.x = pickle.loads(ob)


def _set_worker_threads():
    try:
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)
    except Exception:
        pass

_set_worker_threads()

# =========================
# Worker process
# =========================
def worker(remote, parent_remote, env_fn_wrapper, worker_cfg: Dict[str, Any]):
    parent_remote.close()
    try:
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)
    except Exception:
        pass

    try:
        env = env_fn_wrapper.x()
    except Exception:
        tb = traceback.format_exc()
        remote.send(("__init_error__", tb))
        remote.close()
        return

    # build extractor (CPU)
    try:
        extractor = PreMOE(
            seq_len=worker_cfg["window_size"],
            pred_len=worker_cfg["pre_len"],
            n_variates=worker_cfg["n_variates"],
            d_router=worker_cfg["d_router"],
        ).to("cpu")
        if worker_cfg.get("pretrained_path") and os.path.exists(worker_cfg["pretrained_path"]):
            sd = torch.load(worker_cfg["pretrained_path"], map_location="cpu")
            extractor.load_state_dict(sd, strict=True)
        extractor.eval()
        for p in extractor.parameters():
            p.requires_grad = False
    except Exception:
        tb = traceback.format_exc()
        remote.send(("__init_error__", tb))
        remote.close()
        return

    feat = FeaturePipeline(extractor, device="cpu", adapter_dim=worker_cfg["adapter_dim"])
    actor: Optional[ActorDualHead] = None
    critic: Optional[ValueNet] = None

    pending_payload: Optional[Dict[str, Any]] = None
    adapter_dims: Optional[Dict[str, int]] = None

    def ensure_policy(curr_np: np.ndarray, hist_np: np.ndarray):
        nonlocal actor, critic, pending_payload, adapter_dims
        curr = torch.from_numpy(curr_np).float().unsqueeze(0)
        hist = torch.from_numpy(hist_np).float().unsqueeze(0)
        s, dims = feat.obs_to_state_normed(curr, hist, update_norm=False)
        adapter_dims = dims
        state_dim = int(s.shape[-1])
        if actor is None:
            actor = ActorDualHead(state_dim, hidden_dim=worker_cfg["hidden_dim"]).to("cpu")
            critic = ValueNet(state_dim, hidden_dim=worker_cfg["hidden_dim"]).to("cpu")
        if pending_payload is not None:
            apply_payload(pending_payload)
            pending_payload = None

    def apply_payload(payload: Dict[str, Any]):
        nonlocal actor, critic, adapter_dims
        if adapter_dims is None and payload.get("adapter_dims") is not None:
            adapter_dims = payload["adapter_dims"]

        if adapter_dims is not None:
            feat.build_adapter_from_dims(adapter_dims)

        if payload.get("norm_state") is not None:
            feat.build_norm_if_needed(int(payload["norm_state"]["mean"].numel()))
            assert feat.norm is not None
            feat.norm.load_state_dict(payload["norm_state"], device="cpu")

        if payload.get("adapter_state") is not None:
            assert feat.adapter is not None
            feat.adapter.load_state_dict(payload["adapter_state"], strict=True)

        if actor is not None and payload.get("actor_state") is not None:
            actor.load_state_dict(payload["actor_state"], strict=True)
        if critic is not None and payload.get("critic_state") is not None:
            critic.load_state_dict(payload["critic_state"], strict=True)

    def sample_action_weight(state_1d: torch.Tensor, deterministic=False) -> Tuple[int, int, float, float, float]:
        with torch.no_grad():
            assert actor is not None and critic is not None
            logits_a, logits_w = actor(state_1d)
            logits_a = logits_a.squeeze(0)
            logits_w = logits_w.squeeze(0)

            dist_a = Categorical(logits=logits_a)
            
            # --- åŠ¨ä½œ A é€‰æ‹© ---
            if deterministic:
                a = int(torch.argmax(logits_a, dim=-1).item())
            else:
                a = int(dist_a.sample().item())
            
            logp_a = float(dist_a.log_prob(torch.tensor(a)).item())

            # æ©ç é€»è¾‘
            allowed = torch.zeros(5, dtype=torch.bool)
            if a in (A_LONG, A_SHORT, A_CLOSE):
                allowed[1:] = True
                need_w = 1.0
            else:
                allowed[0] = True
                need_w = 0.0

            masked = logits_w.clone()
            masked[~allowed] = -1e9
            dist_w = Categorical(logits=masked)

            # --- æƒé‡ WI é€‰æ‹© ---
            if deterministic:
                wi = int(torch.argmax(masked, dim=-1).item())
            else:
                wi = int(dist_w.sample().item())
                
            logp_w = float(dist_w.log_prob(torch.tensor(wi)).item())

            wv = float(WEIGHT_BINS[wi])
            logp_joint = logp_a + need_w * logp_w
            v = float(critic(state_1d).squeeze(-1).item())
            return a, wi, wv, logp_joint, v

    def sample_action_weight_old(state_1d: torch.Tensor, deterministic=False) -> Tuple[int, int, float, float, float]:
        """
        æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°æœ¬èº«ä¸åŒ… no_gradï¼Œä½†å®ƒåªä¼šåœ¨ rollout å¾ªç¯çš„ with torch.no_grad() å†…è¢«è°ƒç”¨
        æ‰€ä»¥ä¸ä¼šäº§ç”Ÿæ¢¯åº¦å›¾ï¼Œä¹Ÿä¸ä¼šå  GPU/CPU çš„åä¼ å¼€é”€ã€‚
        """
        with torch.no_grad():
            assert actor is not None and critic is not None
            logits_a, logits_w = actor(state_1d)
            logits_a = logits_a.squeeze(0)
            logits_w = logits_w.squeeze(0)

            dist_a = Categorical(logits=logits_a)

            a = int(dist_a.sample().item())
            logp_a = float(dist_a.log_prob(torch.tensor(a)).item())

            allowed = torch.zeros(5, dtype=torch.bool)
            if a in (A_LONG, A_SHORT, A_CLOSE):
                allowed[1:] = True
                need_w = 1.0
            else:
                allowed[0] = True
                need_w = 0.0

            masked = logits_w.clone()
            masked[~allowed] = -1e9
            dist_w = Categorical(logits=masked)
            wi = int(dist_w.sample().item())
            logp_w = float(dist_w.log_prob(torch.tensor(wi)).item())

            wv = float(WEIGHT_BINS[wi])
            logp_joint = logp_a + need_w * logp_w
            v = float(critic(state_1d).squeeze(-1).item())
            return a, wi, wv, logp_joint, v

    # ç”¨äº rewardScaling çš„ gamma
    gamma = float(worker_cfg.get("gamma", 0.99))

    while True:
        try:
            cmd, data = remote.recv()
        except EOFError:
            break

        try:
            if cmd == "__ping__":
                remote.send(("__pong__", None))

            elif cmd == "set_task":
                idx = int(data)
                if hasattr(env, "set_task"):
                    env.set_task(idx)
                remote.send(("ok", None))
            
            elif cmd == 'set_fee':
                new_fee = float(data)
                env.set_fee(new_fee)

                remote.send(("ok", None))

            elif cmd == "set_weights":
                payload = data
                if actor is None or critic is None or feat.adapter is None or feat.norm is None:
                    pending_payload = payload
                else:
                    apply_payload(payload)
                remote.send(("ok", None))

            elif cmd == "rollout":
                T = int(data["T"])

                # ---- reset env ----
                reset_out = env.reset()
                if isinstance(reset_out, (tuple, list)) and len(reset_out) >= 2:
                    curr_np, hist_np = reset_out[0], reset_out[1]
                else:
                    raise RuntimeError(f"env.reset unexpected: {type(reset_out)}")

                curr_np = np.asarray(curr_np, np.float32)
                hist_np = np.asarray(hist_np, np.float32)

                ensure_policy(curr_np, hist_np)

                Dc = int(curr_np.shape[-1])
                L = int(hist_np.shape[-2])
                Dh = int(hist_np.shape[-1])

                # ---- buffers ----
                raw_curr = np.zeros((T, Dc), np.float32)
                raw_hist = np.zeros((T, L, Dh), np.float32)
                actions = np.zeros((T,), np.int64)
                w_idx = np.zeros((T,), np.int64)
                w_val = np.zeros((T,), np.float32)
                logp_old = np.zeros((T,), np.float32)
                value_old = np.zeros((T,), np.float32)

                rewards = np.zeros((T,), np.float32)

                done = np.zeros((T,), np.bool_)    # done[t] å¯¹åº” â€œt åŠ¨ä½œä¹‹åæ˜¯å¦ç»ˆæ­¢â€
                valid = np.zeros((T,), np.bool_)   # valid[t] è¡¨ç¤ºè¿™ä¸€æ­¥ transition æ˜¯å¦å¯ç”¨äºè®­ç»ƒï¼ˆå¿…é¡»æœ‰å¯¹é½åçš„ rewardï¼‰

                # ---- per-episode RewardScaling (æ¯ä¸ª worker/episode ç‹¬ç«‹) ----
                r_scaler = RewardScaling(shape=1, gamma=gamma)
                try:
                    r_scaler.reset()
                except Exception:
                    pass

                terminated_early = False

                # ä½ è¿™ä¸ªç¯å¢ƒçš„ reward å»¶è¿Ÿï¼št è¿”å› R_tï¼Œä½†å±äº t-1 åŠ¨ä½œ
                # å› æ­¤ï¼št=0 çš„ reward æ— æ„ä¹‰ï¼›æœ€åä¸€ä¸ªåŠ¨ä½œæ²¡æœ‰ rewardï¼ˆé™¤éä½ é¢å¤–å† step ä¸€æ¬¡ï¼‰
                for t in range(T):
                    # 1) record state at time t
                    raw_curr[t] = curr_np
                    raw_hist[t] = hist_np

                    if terminated_early:
                        # padding
                        actions[t] = A_HOLD
                        w_idx[t] = 0
                        w_val[t] = 0.0
                        logp_old[t] = 0.0
                        value_old[t] = 0.0
                        rewards[t] = 0.0
                        done[t] = True
                        valid[t] = False
                        continue

                    # 2) decide action using policy (NO GRAD)
                    with torch.no_grad():
                        curr = torch.from_numpy(curr_np).unsqueeze(0)
                        hist = torch.from_numpy(hist_np).unsqueeze(0)
                        s, _ = feat.obs_to_state_normed(curr, hist, update_norm=False)
                        a, wi, wv, lp, v = sample_action_weight(s)

                    # 3) env step
                    step_out = env.step(a, wv)
                    if isinstance(step_out, (tuple, list)) and len(step_out) >= 5:
                        next_curr, next_hist, r, term, trunc = step_out[0], step_out[1], step_out[2], step_out[3], step_out[4]
                    else:
                        raise RuntimeError(f"env.step return length={len(step_out)} unexpected")

                    d = bool(term or trunc)

                    # 4) write transition fields for time t (but reward for t will come at t+1)
                    actions[t] = a
                    w_idx[t] = wi
                    w_val[t] = wv
                    logp_old[t] = lp
                    value_old[t] = v
                    done[t] = d

                    # 5) reward alignment + rewardScaling:
                    # å½“å‰ step è¿”å›çš„ r å±äº (t-1) çš„åŠ¨ä½œï¼Œæ‰€ä»¥å†™å…¥ rewards[t-1]
                    # å¹¶ä¸” t=0 çš„ r ä¸¢å¼ƒ
                    try:
                        r_in = torch.as_tensor([float(r)], dtype=torch.float32)
                        # r_scaled = float(r_scaler(r_in).item())
                    except Exception:
                        # å…œåº•ï¼šå¦‚æœ RewardScaling æ”¯æŒ float è¾“å…¥
                        # r_scaled = float(r_scaler(float(r)))
                        pass
                    
                    # rewards[t] = r_scaled
                    rewards[t] = r_in
                    valid[t] = True
                    # if t > 0:
                    #     # åªæœ‰ (t-1) è¿™ä¸ª transition æ‰çœŸæ­£æ‹¿åˆ°äº†å±äºå®ƒçš„ rewardï¼Œæ‰€ä»¥æ‰ valid
                    #     rewards[t - 1] = r_scaled
                    #     # æ³¨æ„ï¼št-1 è¿™æ­¥æ˜¯å¦æœ‰æ•ˆï¼Œè¿˜å¾—çœ‹ t-1 è‡ªå·±æ˜¯å¦æ˜¯â€œæœ€åä¸€æ­¥â€/æ˜¯å¦è¢«æå‰ç»ˆæ­¢
                    #     # è¿™é‡Œåªä¿è¯ reward å·²å¯¹é½åˆ° t-1
                    #     valid[t - 1] = True
                    

                    # å½“å‰ t è¿™æ­¥ï¼šreward è¿˜æ²¡æ¥ï¼Œæ‰€ä»¥å…ˆä¸ç½® valid[t]
                    # å¦‚æœè¿™ä¸€åˆ»ç»ˆæ­¢äº†ï¼Œé‚£ä¹ˆè¿™ä¸€æ­¥æ°¸è¿œç­‰ä¸åˆ° reward -> valid[t] å¿…é¡» False
                    if d:
                        terminated_early = True
                        valid[t] = False  # å¼ºåˆ¶
                        # ç»ˆæ­¢æ—¶ä¸å†æ›´æ–° curr_np/hist_np
                        continue

                    # 6) move to next state
                    curr_np = np.asarray(next_curr, np.float32)
                    hist_np = np.asarray(next_hist, np.float32)

                # ---- å…³é”®ï¼šæœ€åä¸€æ­¥æ°¸è¿œæ²¡æœ‰ä¸‹ä¸€æ­¥ rewardï¼ˆå»¶è¿Ÿæœºåˆ¶ä¸‹ï¼‰ï¼Œå¿…é¡» mask æ‰ ----
                valid[T - 1] = False
                rewards[T - 1] = 0.0

                # ---- bootstrap last_valueï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰ ----
                if not terminated_early:
                    with torch.no_grad():
                        curr = torch.from_numpy(curr_np).unsqueeze(0)
                        hist = torch.from_numpy(hist_np).unsqueeze(0)
                        s_last, _ = feat.obs_to_state_normed(curr, hist, update_norm=False)
                        assert critic is not None
                        last_value = float(critic(s_last).squeeze(-1).item())
                else:
                    last_value = 0.0

                # equity_endï¼šå°½é‡ä» env.account_controller å–
                equity_end = None
                try:
                    if hasattr(env, "account_controller") and hasattr(env.account_controller, "equity"):
                        equity_end = float(env.account_controller.equity)
                except Exception:
                    equity_end = None
                if equity_end is None:
                    equity_end = float("nan")

                env_sharpe = env.account_controller.get_sharpe_ratio()
                remote.send(("traj", {
                    "raw_curr": raw_curr,
                    "raw_hist": raw_hist,
                    "actions": actions,
                    "w_idx": w_idx,
                    "w_val": w_val,
                    "logp_old": logp_old,
                    "value_old": value_old,
                    "rewards": rewards,
                    "done": done,
                    "valid": valid,
                    "last_value": np.array([last_value], np.float32),
                    "equity_end": np.array([equity_end], np.float32),
                    "env_sharpe": env_sharpe, # ğŸ”¥ æ–°å¢
                }))

            elif cmd == "close":
                try:
                    env.close()
                except Exception:
                    pass
                remote.send(("ok", None))
                remote.close()
                break

            else:
                raise NotImplementedError(cmd)

        except Exception:
            tb = traceback.format_exc()
            try:
                remote.send(("error", tb))
            except Exception:
                pass
            break



# =========================
# SubprocVectorEnv (rollout in one IPC)
# =========================
class SubprocVectorEnv:
    def __init__(self, env_fns: List, worker_cfg: Dict[str, Any]):
        self.closed = False
        self.num_envs = len(env_fns)
        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(self.num_envs)])
        self.ps = []

        for work_remote, remote, env_fn in zip(self.work_remotes, self.remotes, env_fns):
            p = mp.Process(
                target=worker,
                args=(work_remote, remote, CloudpickleWrapper(env_fn), worker_cfg),
                daemon=True,
            )
            p.start()
            self.ps.append(p)
            work_remote.close()

        # detect init error early
        for r in self.remotes:
            r.send(("__ping__", None))
        for r in self.remotes:
            tag, payload = r.recv()
            if tag != "__pong__":
                raise RuntimeError("worker did not respond to ping")

    def set_tasks(self, task_indices: List[int]):
        for r, idx in zip(self.remotes, task_indices):
            r.send(("set_task", int(idx)))
        for r in self.remotes:
            tag, payload = r.recv()
            if tag == "error":
                raise RuntimeError(payload)

    def set_fee_all(self, fee: float):
        for r in self.remotes:
            r.send(("set_fee", fee))
        for r in self.remotes:
            tag, _ = r.recv()
            if tag == "error":
                raise RuntimeError("Failed to set fee in worker")
            
    def set_weights_all(self, payload: Dict[str, Any]):
        for r in self.remotes:
            r.send(("set_weights", payload))
        for r in self.remotes:
            tag, payload2 = r.recv()
            if tag == "error":
                raise RuntimeError(payload2)

    def rollout(self, T: int) -> List[Dict[str, Any]]:
        # 1. å‘é€å‘½ä»¤
        for r in self.remotes:
            r.send(("rollout", {"T": int(T)}))

        trajs = []
        # 2. é€ä¸ªæ¥æ”¶ï¼Œæ¥æ”¶å®Œç«‹åˆ»æ–­å¼€å¼•ç”¨ï¼Œé˜²æ­¢ç§¯å‹
        for r in self.remotes:
            try:
                # recv è¿™ä¸€æ­¥æ˜¯æœ€è€—å†…å­˜çš„ï¼Œå› ä¸ºå®ƒè¦ååºåˆ—åŒ– huge object
                tag, payload = r.recv()
                
                if tag == "error":
                    raise RuntimeError(payload)
                if tag != "traj":
                    raise RuntimeError(f"unexpected tag from worker: {tag}")
                
                trajs.append(payload)
                
                # ğŸ”¥ æ˜¾å¼åˆ é™¤ payload å¼•ç”¨ (è™½ç„¶ Python æœ‰ GCï¼Œä½†æ˜¾å¼åˆ é™¤æ›´ä¿é™©)
                del payload 
            except Exception as e:
                # é‡åˆ°é”™è¯¯ä¹Ÿè¦å°è¯•æŠŠå‰©ä¸‹çš„æ”¶å®Œï¼Œé˜²æ­¢æ­»é”
                print(f"[Error] Recv failed: {e}")
                raise e
                
        return trajs


    def close(self):
        if self.closed:
            return
        for r in self.remotes:
            r.send(("close", None))
        for r in self.remotes:
            try:
                r.recv()
            except Exception:
                pass
        for p in self.ps:
            p.join()
        self.closed = True


# =========================
# Learner PPO (GPU update)
# =========================
class LearnerPPO:
    def __init__(
        self,
        device: str,
        window_size: int,
        pre_len: int,
        n_variates: int,
        d_router: int,
        pretrained_path: str,
        adapter_dim: int,
        hidden_dim: int,
        gamma: float,
        gae_lambda: float,
        clip_eps: float,
        k_epochs: int,
        actor_lr: float = 3e-4,
        critic_lr: float = 5e-4,
        check_path: str='./miniQMT/DL/checkout/check_data_parallel.pt',
        update_mb_size: int=2048,
        total_epochs: int=1000,
    ):
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_eps = clip_eps
        self.k_epochs = k_epochs
        self.adapter_dim = adapter_dim
        self.hidden_dim = hidden_dim

        # self.check_path = f'{check_path}/check_data_parallel.pt'
        self.check_path = check_path
        self.update_mb_size = update_mb_size

        # frozen extractor on GPU
        self.extractor = PreMOE(
            seq_len=window_size, pred_len=pre_len, n_variates=n_variates, d_router=d_router
        ).to(device)
        if pretrained_path and os.path.exists(pretrained_path):
            sd = torch.load(pretrained_path, map_location=device)
            self.extractor.load_state_dict(sd, strict=True)
        else:
            print(f"[Learner] WARNING: pretrained_path not found: {pretrained_path}")
        self.extractor.eval()
        for p in self.extractor.parameters():
            p.requires_grad = False

        self.adapter: Optional[MultiViewAdapter] = None
        self.actor: Optional[ActorDualHead] = None
        self.critic: Optional[ValueNet] = None

        # çŠ¶æ€å½’ä¸€åŒ–æ¨¡å—
        self.norm: Optional[Normalization] = None
        self.adapter_dims: Optional[Dict[str, int]] = None

        self.opt_adapter = None
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr
        self.opt_actor = None
        self.opt_critic = None

        # å­¦ä¹ ç‡è¡°å‡
        self.total_epochs = total_epochs

    def save(self, epoch: int = None, best_reward: float = None, path: str = None):
            save_path = path or self.check_path

            scheduler_state = {
            'actor': self.scheduler_actor.state_dict(),
            'critic': self.scheduler_critic.state_dict(),
            'adapter': self.scheduler_adapter.state_dict(),
        }
            data = {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "actor_state": self.actor.state_dict(),
                "value_state": self.critic.state_dict(),
                "adpter_state": self.adapter.state_dict(),
                "opt_actor_state": self.opt_actor.state_dict(),
                "opt_critic_state": self.opt_critic.state_dict(),
                "opt_adpter_state": self.opt_adapter.state_dict(),
                "scheduler_state": scheduler_state,
                "h_params": {
                    "gamma": self.gamma,
                    "clip_eps": self.clip_eps,
                    "k_epochs": self.k_epochs,
                    "device": self.device,
                },
                "epoch": epoch,
                "best_reward": best_reward.item() if hasattr(best_reward, 'item') else best_reward,
                "state_norm": self.norm,
            }
            torch.save(data, save_path)
            print(f"[PPO] checkpoint saved to: {save_path}")

    def load_checkpoint(self, path: str=None):
        if path is None:
            path = self.check_path
        if not os.path.exists(path):
            print(f"[Warn] Checkpoint not found at {path}")
            return None, None
        
        print(f"[Resume] Loading checkpoint from {path}...")
        # è¿™é‡Œçš„ map_location éå¸¸é‡è¦ï¼Œé˜²æ­¢è·¨è®¾å¤‡åŠ è½½æŠ¥é”™
        data = torch.load(path, map_location=self.device, weights_only=False)
        
        # 1. åŠ è½½ç½‘ç»œæƒé‡
        self.actor.load_state_dict(data['actor_state'])
        self.critic.load_state_dict(data['value_state'])
        self.adapter.load_state_dict(data['adpter_state'])
        
        # 2. åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if self.opt_actor: self.opt_actor.load_state_dict(data['opt_actor_state'])
        if self.opt_critic: self.opt_critic.load_state_dict(data['opt_critic_state'])
        if self.opt_adapter and 'adapter_state' in data:
            st = data['opt_adapter_state']
            if st is not None: 
                self.opt_adapter.load_state_dict(st)

        # 3. [æ–°å¢] åŠ è½½ Normalization çŠ¶æ€
        if 'state_norm' in data:
            # ç›´æ¥è¦†ç›–å½“å‰çš„ self.state_norm
            self.norm = data['state_norm']
            print(f"[Resume] State Norm loaded. (count={self.norm.running_ms.n if hasattr(self.norm.running_ms, 'n') else '?'})")
        else:
            print("[Resume] Warning: No state_norm in checkpoint! Training might be unstable.")

        # 4. [æ–°å¢] åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state' in data:
            self.scheduler_actor.load_state_dict(data['scheduler_state']['actor'])
            self.scheduler_critic.load_state_dict(data['scheduler_state']['critic'])
            self.scheduler_adapter.load_state_dict(data['scheduler_state']['adapter'])
            print("[Resume] Schedulers loaded.")    


        epoch = data.get('epoch', 0)
        best_reward = data.get('best_reward', -float('inf'))
        
        print(f"[Resume] Success! Resuming from Epoch {epoch + 1}, Best Reward: {best_reward:.4f}")
        return epoch, best_reward


    @torch.no_grad()
    def _encode_tokens_only(self, call_state, put_state):
        call_tok = self.extractor.encode_tokens(call_state)
        put_tok = self.extractor.encode_tokens(put_state)
        return call_tok, put_tok

    def _maybe_build(self, curr: torch.Tensor, hist: torch.Tensor):
        if curr.dim() == 1:
            curr = curr.unsqueeze(0)
        if hist.dim() == 2:
            hist = hist.unsqueeze(0)

        call_state, put_state = torch.chunk(hist, chunks=2, dim=2)

        physical_call, physical_put = call_state[:, -1, :], put_state[:, -1, :]

        call_tok, put_tok = self._encode_tokens_only(call_state, put_state)

        if self.adapter_dims is None:
            self.adapter_dims = {
                "varma_high": int(call_tok["varma_h"].shape[-1]),
                "varma_low": int(call_tok["varma_l"].shape[-1]),
                "basis_high": int(call_tok["basis_h"].shape[-1]),
                "basis_low": int(call_tok["basis_l"].shape[-1]),
                "itrans_high": int(call_tok["itrans_h"].shape[-1]),
                "itrans_low": int(call_tok["itrans_l"].shape[-1]),
                "router": int(call_tok["router"].shape[-1]),
            }

        if self.adapter is None:
            self.adapter = MultiViewAdapter(self.adapter_dims, final_dim=self.adapter_dim).to(self.device)

        reduce_call = self.adapter(call_tok)
        reduce_put = self.adapter(put_tok)
        
        raw = torch.cat([curr.float(),  physical_call.float(), physical_put.float(), reduce_call.float(), reduce_put.float()], dim=-1)
        state_dim = int(raw.shape[-1])


        if self.norm is None:
            self.norm = Normalization(shape=(state_dim,), device=self.device)

        if self.actor is None:
            self.actor = ActorDualHead(state_dim, hidden_dim=self.hidden_dim).to(self.device)
            self.critic = ValueNet(state_dim, hidden_dim=self.hidden_dim).to(self.device)
            self.opt_adapter = torch.optim.Adam(self.adapter.parameters(), lr=self.actor_lr)
            self.opt_actor = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)
            self.opt_critic = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)

            self.scheduler_actor = torch.optim.lr_scheduler.LinearLR(self.opt_actor, start_factor=1.0, end_factor=0.01, total_iters=self.total_epochs)
            self.scheduler_critic = torch.optim.lr_scheduler.LinearLR(self.opt_critic, start_factor=1.0, end_factor=0.01, total_iters=self.total_epochs)
            self.scheduler_adapter = torch.optim.lr_scheduler.LinearLR(self.opt_adapter, start_factor=1.0, end_factor=0.01, total_iters=self.total_epochs)


    def export_payload(self) -> Dict[str, Any]:
        assert self.adapter is not None and self.actor is not None and self.critic is not None
        norm_state = self.norm.state_dict() if self.norm is not None else None

        return {
            "adapter_dims": self.adapter_dims,
            "adapter_state": {k: v.detach().cpu() for k, v in self.adapter.state_dict().items()},
            "actor_state": {k: v.detach().cpu() for k, v in self.actor.state_dict().items()},
            "critic_state": {k: v.detach().cpu() for k, v in self.critic.state_dict().items()},
            "norm_state": norm_state,
        }

    def _build_state(self, curr: torch.Tensor, hist: torch.Tensor, norm_update: bool):
        self._maybe_build(curr, hist)
        assert self.adapter is not None and self.norm is not None

        # 1. æ‹†åˆ†çŠ¶æ€
        call_state, put_state = torch.chunk(hist, chunks=2, dim=2)
        
        # 2. æå–ç‰©ç†ç‰¹å¾ (ä¸ _maybe_build å¯¹é½)
        physical_call, physical_put = call_state[:, -1, :], put_state[:, -1, :]
        
        # 3. æå–ç‰¹å¾ Token
        call_tok, put_tok = self._encode_tokens_only(call_state, put_state)

        # 4. Adapter æŠ•å½±
        reduce_call = self.adapter(call_tok)
        reduce_put = self.adapter(put_tok)
        
        # 5. ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè¿™é‡Œå¿…é¡»æ‹¼ 5 ä¸ªéƒ¨åˆ†ï¼Œæ€»ç»´åº¦æ‰æ˜¯ 547
        raw = torch.cat([
            curr.float(), 
            physical_call.float(), 
            physical_put.float(), 
            reduce_call.float(), 
            reduce_put.float()
        ], dim=-1)
        
        s = self.norm(raw, update=norm_update)
        return raw, s
    
    def update_from_trajs(self, trajs: List[Dict[str, Any]], target_kl: float = 0.015):
            """
            é’ˆå¯¹ 4090D ä¼˜åŒ–çš„å¹³ç¨³æ›´æ–°ç‰ˆï¼š
            1. å¼•å…¥ Explained Variance ç›‘æ§ä»·å€¼ç½‘ç»œè´¨é‡
            2. ä½¿ç”¨ Huber Loss å¤„ç†ä»·å€¼æŸå¤±ï¼ŒæŠ‘åˆ¶æç«¯æƒ©ç½šå¯¼è‡´çš„æ¢¯åº¦æŠ–åŠ¨
            3. è°ƒæ•´ Loss æƒé‡åˆ†é…ï¼šVF=0.5 (å¢å¼ºé¢„åˆ¤), Ent=0.02 (ä¿æŒæ¢ç´¢)
            """
            assert self.actor is not None and self.critic is not None
            
            # ---------------------------------------------------------------------
            # 1. æ•°æ®å †å  (CPU)
            # ---------------------------------------------------------------------
            def stack_key(key, dtype):
                N = len(trajs)
                if N == 0: return torch.tensor([], dtype=dtype)
                first_val = trajs[0][key]
                sample_shape = torch.as_tensor(first_val, dtype=dtype).shape
                target_shape = list(sample_shape)
                target_shape.insert(1, N)
                out = torch.empty(target_shape, dtype=dtype)
                for i, tr in enumerate(trajs):
                    out.select(1, i).copy_(torch.as_tensor(tr[key], dtype=dtype))
                return out

            raw_curr   = stack_key("raw_curr", torch.float32)
            raw_hist   = stack_key("raw_hist", torch.float32)
            actions    = stack_key("actions", torch.long)
            w_idx      = stack_key("w_idx", torch.long)
            logp_old   = stack_key("logp_old", torch.float32)
            value_old  = stack_key("value_old", torch.float32)
            rewards    = stack_key("rewards", torch.float32)
            done       = stack_key("done", torch.float32)
            valid      = stack_key("valid", torch.float32)
            last_value = torch.stack([torch.as_tensor(tr["last_value"], dtype=torch.float32) for tr in trajs], dim=0).squeeze()

            # ---------------------------------------------------------------------
            # 2. è®¡ç®— GAE (CPU)
            # ---------------------------------------------------------------------
            T, N = raw_curr.shape[:2]
            with torch.no_grad():
                adv = torch.zeros((T, N), dtype=torch.float32)
                last_gae = torch.zeros((N,), dtype=torch.float32)
                for t in reversed(range(T)):
                    m = (1.0 - done[t]) * valid[t]
                    v_tp1 = last_value if t == T - 1 else value_old[t + 1]
                    delta = rewards[t] + self.gamma * v_tp1 * m - value_old[t]
                    last_gae = delta + self.gamma * self.gae_lambda * m * last_gae
                    adv[t] = last_gae * valid[t]
                ret = adv + value_old

            # ---------------------------------------------------------------------
            # 3. Flatten & Filter
            # ---------------------------------------------------------------------
            mask_flat = valid.view(-1) > 0.5
            if not mask_flat.any(): return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
            
            def flat_and_filter(tensor_in):
                return tensor_in.flatten(0, 1)[mask_flat]

            curr_flat = flat_and_filter(raw_curr)
            hist_flat = flat_and_filter(raw_hist)
            act_flat  = flat_and_filter(actions)
            widx_flat = flat_and_filter(w_idx)
            logp_old_flat = flat_and_filter(logp_old)
            value_old_flat = flat_and_filter(value_old)
            adv_raw_flat = flat_and_filter(adv)
            ret_flat  = flat_and_filter(ret)

            # è®¡ç®— Explained Variance (è¡¡é‡æ—§ Value å¯¹å®é™… Return çš„é¢„æµ‹å‡†ç¡®åº¦)
            with torch.no_grad():
                y_true = ret_flat.numpy()
                y_pred = value_old_flat.numpy()
                var_y = np.var(y_true)
                explained_var = 1.0 - np.var(y_true - y_pred) / (var_y + 1e-8) if var_y > 0 else 0.0
                explained_var = max(-1.0, float(explained_var)) # æˆªæ–­å¼‚å¸¸è´Ÿå€¼

            # ä¼˜åŠ¿å½’ä¸€åŒ–
            adv_flat = (adv_raw_flat - adv_raw_flat.mean()) / (adv_raw_flat.std() + 1e-8)
            adv_flat = torch.clamp(adv_flat, -5.0, 5.0)

            # ---------------------------------------------------------------------
            # 4. PPO Training Loop (Mini-batch)
            # ---------------------------------------------------------------------
            M = curr_flat.shape[0]
            mb = self.update_mb_size
            stats = {"loss": [], "kl": [], "actor_loss": [], "value_loss": [], "entropy": []}
            
            for _ep in range(self.k_epochs):
                indices = torch.randperm(M)
                epoch_kl = []
                for st in range(0, M, mb):
                    idx = indices[st:st + mb]
                    if len(idx) == 0: continue
                    
                    # å¼‚æ­¥æ¨é€åˆ° GPU (4090D ä¼˜åŠ¿)
                    curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                    hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                    act_b  = act_flat[idx].to(self.device, non_blocking=True)
                    widx_b = widx_flat[idx].to(self.device, non_blocking=True)
                    logp_old_b = logp_old_flat[idx].to(self.device, non_blocking=True)
                    adv_b  = adv_flat[idx].to(self.device, non_blocking=True)
                    ret_b  = ret_flat[idx].to(self.device, non_blocking=True)
                    v_old_b = value_old_flat[idx].to(self.device, non_blocking=True)

                    _, s = self._build_state(curr_b, hist_b, norm_update=False)
                    logits_a, logits_w = self.actor(s)
                    
                    # Actor Logic
                    dist_a = Categorical(logits=logits_a)
                    new_logp_a = dist_a.log_prob(act_b)
                    ent_a = dist_a.entropy().mean()

                    # Weight Logic
                    need_w = ((act_b == 1) | (act_b == 2) | (act_b == 3)).float()
                    lw = logits_w.clone()
                    if need_w.sum() > 0:
                        maskw = torch.zeros_like(lw, dtype=torch.bool)
                        maskw[need_w.bool(), 1:] = True
                        maskw[~need_w.bool(), 0] = True
                        lw[~maskw] = -1e9
                    dist_w = Categorical(logits=lw)
                    new_logp_w = dist_w.log_prob(widx_b)
                    ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)

                    logp_new = new_logp_a + need_w * new_logp_w
                    ratio = torch.exp(logp_new - logp_old_b)

                    surr1 = ratio * adv_b
                    surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv_b
                    actor_loss = -torch.min(surr1, surr2).mean()

                    # --- ğŸ”¥ å…³é”®æ”¹è¿›ï¼šHuber Loss + Value Clipping ---
                    v_pred = self.critic(s).squeeze(-1)
                    # ä½¿ç”¨ Huber Loss ä»£æ›¿ MSEï¼Œç¼“è§£ -150 ç­‰æç«¯å€¼å†²å‡»
                    v_loss_unclipped = F.smooth_l1_loss(v_pred, ret_b)
                    
                    v_pred_clipped = v_old_b + torch.clamp(v_pred - v_old_b, -self.clip_eps, self.clip_eps)
                    v_loss_clipped = F.smooth_l1_loss(v_pred_clipped, ret_b)
                    value_loss = torch.max(v_loss_unclipped, v_loss_clipped)

                    entropy = ent_a + 0.5 * ent_w
                    
                    # --- ğŸ”¥ æ€» Loss è®¡ç®— (VFæƒé‡æå‡è‡³0.5) ---
                    loss = actor_loss + 0.5 * value_loss - 0.05 * entropy
                    
                    self.opt_adapter.zero_grad(set_to_none=True)
                    self.opt_actor.zero_grad(set_to_none=True)
                    self.opt_critic.zero_grad(set_to_none=True)
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                    torch.nn.utils.clip_grad_norm_(self.adapter.parameters(), 0.5)
                    
                    self.opt_adapter.step()
                    self.opt_actor.step()
                    self.opt_critic.step()
                    
                    with torch.no_grad():
                        kl_b = (logp_old_b - logp_new).mean().abs().item()
                        epoch_kl.append(kl_b)
                        stats["loss"].append(loss.item())
                        stats["kl"].append(kl_b)
                        stats["actor_loss"].append(actor_loss.item())
                        stats["value_loss"].append(value_loss.item())
                        stats["entropy"].append(entropy.item())

                if (sum(epoch_kl)/len(epoch_kl)) > 1.5 * target_kl: break

            # 5. æ›´æ–°çŠ¶æ€å½’ä¸€åŒ–
            if M > 0:
                with torch.no_grad():
                    for st in range(0, M, mb):
                        self._build_state(curr_flat[st:st+mb].to(self.device), hist_flat[st:st+mb].to(self.device), norm_update=True)

            self.scheduler_actor.step()
            self.scheduler_critic.step()
            self.scheduler_adapter.step()
            
            def get_avg(k): return sum(stats[k]) / len(stats[k]) if stats[k] else 0.0
            return get_avg("loss"), get_avg("kl"), get_avg("actor_loss"), get_avg("value_loss"), get_avg("entropy"), explained_var


    def oldold_update_from_trajs(self, trajs: List[Dict[str, Any]], target_kl: float = 0.015):
        """
        é’ˆå¯¹ 110 Epoch åçš„å¼‚å¸¸ä¿®æ­£ç‰ˆï¼š
        1. ä½¿ç”¨ Huber Loss æ›¿æ¢ MSEï¼Œå¹³æ»‘å¼‚å¸¸æ”¶ç›Šå†²å‡»
        2. è¿›ä¸€æ­¥å‰Šå‡ Value æƒé‡ (0.01)ï¼Œå¢åŠ ç†µæƒé‡ (0.02) ä»¥æ‰“ç ´ Hold/Close åƒµå±€
        3. å¼ºåˆ¶ Value Clipping
        """
        assert self.actor is not None and self.critic is not None
        
        # ---------------------------------------------------------------------
        # 1. ç»Ÿä¸€å †å  (CPU)
        # ---------------------------------------------------------------------
        def stack_key(key, dtype):
            N = len(trajs)
            if N == 0: return torch.tensor([], dtype=dtype)
            first_val = trajs[0][key]
            sample_shape = torch.as_tensor(first_val, dtype=dtype).shape
            target_shape = list(sample_shape)
            target_shape.insert(1, N)
            out = torch.empty(target_shape, dtype=dtype)
            for i, tr in enumerate(trajs):
                out.select(1, i).copy_(torch.as_tensor(tr[key], dtype=dtype))
            return out

        raw_curr  = stack_key("raw_curr", torch.float32)
        raw_hist  = stack_key("raw_hist", torch.float32)
        actions   = stack_key("actions", torch.long)
        w_idx     = stack_key("w_idx", torch.long)
        logp_old  = stack_key("logp_old", torch.float32)
        value_old = stack_key("value_old", torch.float32)
        rewards   = stack_key("rewards", torch.float32)
        done      = stack_key("done", torch.float32)
        valid     = stack_key("valid", torch.float32)
        last_value = torch.stack([torch.as_tensor(tr["last_value"], dtype=torch.float32) for tr in trajs], dim=0).squeeze()

        # ---------------------------------------------------------------------
        # 2. è®¡ç®— GAE
        # ---------------------------------------------------------------------
        T, N = raw_curr.shape[:2]
        with torch.no_grad():
            adv = torch.zeros((T, N), dtype=torch.float32)
            last_gae = torch.zeros((N,), dtype=torch.float32)
            for t in reversed(range(T)):
                m = (1.0 - done[t]) * valid[t]
                v_tp1 = last_value if t == T - 1 else value_old[t + 1]
                delta = rewards[t] + self.gamma * v_tp1 * m - value_old[t]
                last_gae = delta + self.gamma * self.gae_lambda * m * last_gae
                adv[t] = last_gae * valid[t]
            ret = adv + value_old

        # ---------------------------------------------------------------------
        # 3. Flatten & Filter
        # ---------------------------------------------------------------------
        mask_flat = valid.view(-1) > 0.5
        if not mask_flat.any(): return 0.0, 0.0, 0.0, 0.0, 0.0
        
        def flat_and_filter(tensor_in):
            return tensor_in.flatten(0, 1)[mask_flat]

        curr_flat = flat_and_filter(raw_curr)
        hist_flat = flat_and_filter(raw_hist)
        act_flat  = flat_and_filter(actions)
        widx_flat = flat_and_filter(w_idx)
        logp_old_flat = flat_and_filter(logp_old)
        value_old_flat = flat_and_filter(value_old)
        adv_raw_flat = flat_and_filter(adv)
        ret_flat  = flat_and_filter(ret)

        # ä¼˜åŠ¿å½’ä¸€åŒ–ä¸è£å‰ª
        adv_flat = (adv_raw_flat - adv_raw_flat.mean()) / (adv_raw_flat.std() + 1e-8)
        adv_flat = torch.clamp(adv_flat, -5.0, 5.0)

        # ---------------------------------------------------------------------
        # 4. PPO Training Loop
        # ---------------------------------------------------------------------
        M = curr_flat.shape[0]
        mb = self.update_mb_size
        stats = {"loss": [], "kl": [], "actor_loss": [], "value_loss": [], "entropy": []}
        
        for _ep in range(self.k_epochs):
            indices = torch.randperm(M)
            epoch_kl = []
            for st in range(0, M, mb):
                idx = indices[st:st + mb]
                if len(idx) == 0: continue
                
                curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                act_b  = act_flat[idx].to(self.device, non_blocking=True)
                widx_b = widx_flat[idx].to(self.device, non_blocking=True)
                logp_old_b = logp_old_flat[idx].to(self.device, non_blocking=True)
                adv_b  = adv_flat[idx].to(self.device, non_blocking=True)
                ret_b  = ret_flat[idx].to(self.device, non_blocking=True)
                v_old_b = value_old_flat[idx].to(self.device, non_blocking=True)

                _, s = self._build_state(curr_b, hist_b, norm_update=False)
                logits_a, logits_w = self.actor(s)
                
                # Actor é€»è¾‘
                dist_a = Categorical(logits=logits_a)
                new_logp_a = dist_a.log_prob(act_b)
                ent_a = dist_a.entropy().mean()

                need_w = ((act_b == 1) | (act_b == 2) | (act_b == 3)).float()
                lw = logits_w.clone()
                if need_w.sum() > 0:
                    maskw = torch.zeros_like(lw, dtype=torch.bool)
                    maskw[need_w.bool(), 1:] = True
                    maskw[~need_w.bool(), 0] = True
                    lw[~maskw] = -1e9
                
                dist_w = Categorical(logits=lw)
                new_logp_w = dist_w.log_prob(widx_b)
                ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)

                logp_new = new_logp_a + need_w * new_logp_w
                ratio = torch.exp(logp_new - logp_old_b)

                surr1 = ratio * adv_b
                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv_b
                actor_loss = -torch.min(surr1, surr2).mean()

                # --- ğŸ”¥ æ”¹è¿›ï¼šHuber Loss + Value Clipping ---
                v_pred = self.critic(s).squeeze(-1)
                v_pred_clipped = v_old_b + torch.clamp(v_pred - v_old_b, -self.clip_eps, self.clip_eps)
                # ä½¿ç”¨ Huber Loss (smooth_l1_loss) å¯¹æç«¯ Return ä¸æ•æ„Ÿ
                v_loss1 = F.smooth_l1_loss(v_pred, ret_b)
                v_loss2 = F.smooth_l1_loss(v_pred_clipped, ret_b)
                value_loss = torch.max(v_loss1, v_loss2)

                entropy = ent_a + 0.5 * ent_w
                
                # --- ğŸ”¥ æ€» Loss è®¡ç®— (è¿›ä¸€æ­¥è°ƒä½ VF æƒé‡) ---
                loss = actor_loss + 0.5 * value_loss - 0.02 * entropy
                
                self.opt_adapter.zero_grad(set_to_none=True)
                self.opt_actor.zero_grad(set_to_none=True)
                self.opt_critic.zero_grad(set_to_none=True)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ªä¿æŒ 0.5
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.adapter.parameters(), 0.5)
                
                self.opt_adapter.step()
                self.opt_actor.step()
                self.opt_critic.step()
                
                with torch.no_grad():
                    kl_b = (logp_old_b - logp_new).mean().abs().item()
                    epoch_kl.append(kl_b)
                    stats["loss"].append(loss.item())
                    stats["kl"].append(kl_b)
                    stats["actor_loss"].append(actor_loss.item())
                    stats["value_loss"].append(value_loss.item())
                    stats["entropy"].append(entropy.item())

            if (sum(epoch_kl)/len(epoch_kl)) > 1.5 * target_kl: break

        # æ›´æ–° Norm & Scheduler
        if M > 0:
            with torch.no_grad():
                for st in range(0, M, mb):
                    self._build_state(curr_flat[st:st+mb].to(self.device), hist_flat[st:st+mb].to(self.device), norm_update=True)

        self.scheduler_actor.step()
        self.scheduler_critic.step()
        self.scheduler_adapter.step()
        
        def get_avg(k): return sum(stats[k]) / len(stats[k]) if stats[k] else 0.0
        return get_avg("loss"), get_avg("kl"), get_avg("actor_loss"), get_avg("value_loss"), get_avg("entropy")


    def p_update_from_trajs(self, trajs: List[Dict[str, Any]], target_kl: float = 0.015):
        """
        é‡æ„å®Œæ•´ç‰ˆï¼š
        1. åŠ å…¥ Value Clipping é˜²æ­¢ä»·å€¼ç½‘ç»œé¢„æµ‹ç‚¸è£‚å¯¼è‡´ loss é£™å‡
        2. è°ƒæ•´ Loss æƒé‡åˆ†é… (VF Coeff=0.1, Entropy Coeff=0.01)
        3. å…¨ç¨‹ä½¿ç”¨ torch.stack/as_tensor ä¼˜åŒ–å†…å­˜
        """
        assert self.actor is not None and self.critic is not None
        
        # ---------------------------------------------------------------------
        # 1. ç»Ÿä¸€å †å  (Stacking) - CPU ä¸Šå®Œæˆï¼Œé¿å…å†…å­˜å³°å€¼
        # ---------------------------------------------------------------------
        def stack_key(key, dtype):
            N = len(trajs)
            if N == 0: return torch.tensor([], dtype=dtype)
            
            first_val = trajs[0][key]
            sample_tensor = torch.as_tensor(first_val, dtype=dtype)
            sample_shape = sample_tensor.shape
            
            target_shape = list(sample_shape)
            target_shape.insert(1, N) # (T, N, ...)
            
            out = torch.empty(target_shape, dtype=dtype)
            for i, tr in enumerate(trajs):
                out.select(1, i).copy_(torch.as_tensor(tr[key], dtype=dtype))
            return out

        # æå–æ‰€æœ‰åŸºç¡€æ•°æ®
        raw_curr  = stack_key("raw_curr", torch.float32) # (T, N, Dc)
        raw_hist  = stack_key("raw_hist", torch.float32) # (T, N, L, Dh)
        actions   = stack_key("actions", torch.long)     # (T, N)
        w_idx     = stack_key("w_idx", torch.long)       # (T, N)
        logp_old  = stack_key("logp_old", torch.float32) # (T, N)
        value_old = stack_key("value_old", torch.float32)# (T, N)
        rewards   = stack_key("rewards", torch.float32)  # (T, N)
        done      = stack_key("done", torch.float32)     # (T, N)
        valid     = stack_key("valid", torch.float32)    # (T, N)
        
        last_value = torch.stack([torch.as_tensor(tr["last_value"], dtype=torch.float32) for tr in trajs], dim=0).squeeze()

        T, N = raw_curr.shape[:2]

        # ---------------------------------------------------------------------
        # 2. è®¡ç®— GAE (CPU è®¡ç®—)
        # ---------------------------------------------------------------------
        with torch.no_grad():
            adv = torch.zeros((T, N), dtype=torch.float32)
            last_gae = torch.zeros((N,), dtype=torch.float32)
            
            for t in reversed(range(T)):
                # m: maskï¼Œåªæœ‰åœ¨æœªå®Œæˆä¸”æœ‰æ•ˆçš„æ­¥æ•°æ‰ç´¯ç§¯ GAE
                m = (1.0 - done[t]) * valid[t]
                v_tp1 = last_value if t == T - 1 else value_old[t + 1]
                delta = rewards[t] + self.gamma * v_tp1 * m - value_old[t]
                last_gae = delta + self.gamma * self.gae_lambda * m * last_gae
                adv[t] = last_gae * valid[t]

            # è¿™é‡Œçš„ ret æ˜¯ Critic çš„æ‹Ÿåˆç›®æ ‡ (Returns)
            ret = adv + value_old

        # ---------------------------------------------------------------------
        # 3. Flatten & Filter (åŸºäº valid è¿‡æ»¤æ— æ•ˆæ•°æ®)
        # ---------------------------------------------------------------------
        mask_flat = valid.view(-1) > 0.5
        if not mask_flat.any():
            return 0.0, 0.0, 0.0, 0.0, 0.0
        
        def flat_and_filter(tensor_in):
            return tensor_in.flatten(0, 1)[mask_flat]

        curr_flat = flat_and_filter(raw_curr)
        hist_flat = flat_and_filter(raw_hist)
        act_flat  = flat_and_filter(actions)
        widx_flat = flat_and_filter(w_idx)
        logp_old_flat = flat_and_filter(logp_old)
        value_old_flat = flat_and_filter(value_old) # å…³é”®ï¼šç”¨äº Value Clipping
        adv_raw_flat = flat_and_filter(adv)
        ret_flat  = flat_and_filter(ret)

        # ä¼˜åŠ¿å½’ä¸€åŒ–ä¸å¼ºåˆ¶è£å‰ª [-5, 5] é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        adv_flat = (adv_raw_flat - adv_raw_flat.mean()) / (adv_raw_flat.std() + 1e-8)
        adv_flat = torch.clamp(adv_flat, -5.0, 5.0)

        # æ‰“å°ç›‘æ§æ•°æ®
        with torch.no_grad():
            x1 = (torch.abs(adv_flat) > 3.0).sum().item() / adv_flat.shape[0]
            x2 = (torch.abs(adv_flat) > 2.0).sum().item() / adv_flat.shape[0]
            x3 = (torch.abs(adv_flat) > 1.0).sum().item() / adv_flat.shape[0]
            print(f"ã€Adv Ratioã€‘ >3.0: {x1:.2%} | >2.0: {x2:.2%} | >1.0: {x3:.2%}")
            print(f"Advantage Max: {adv_flat.max().item():.4f} | Min: {adv_flat.min().item():.4f}")

        # ---------------------------------------------------------------------
        # 4. PPO Training Loop (Mini-batch)
        # ---------------------------------------------------------------------
        M = curr_flat.shape[0]
        mb = self.update_mb_size
        stats = {"loss": [], "kl": [], "actor_loss": [], "value_loss": [], "entropy": []}
        
        for _ep in range(self.k_epochs):
            indices = torch.randperm(M)
            epoch_kl = []
            
            for st in range(0, M, mb):
                idx = indices[st:st + mb]
                if len(idx) == 0: continue
                
                # --- å¼‚æ­¥åŠ è½½åˆ° GPU ---
                curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                act_b  = act_flat[idx].to(self.device, non_blocking=True)
                widx_b = widx_flat[idx].to(self.device, non_blocking=True)
                logp_old_b = logp_old_flat[idx].to(self.device, non_blocking=True)
                adv_b  = adv_flat[idx].to(self.device, non_blocking=True)
                ret_b  = ret_flat[idx].to(self.device, non_blocking=True)
                v_old_b = value_old_flat[idx].to(self.device, non_blocking=True) # ç”¨äºè®¡ç®— Value Clipping

                # --- Actor Forward ---
                _, s = self._build_state(curr_b, hist_b, norm_update=False)
                logits_a, logits_w = self.actor(s)
                
                # Action A åˆ†å¸ƒ
                dist_a = Categorical(logits=logits_a)
                new_logp_a = dist_a.log_prob(act_b)
                ent_a = dist_a.entropy().mean()

                # Weight Action æ©ç é€»è¾‘
                need_w = ((act_b == 1) | (act_b == 2) | (act_b == 3)).float() # 1:Long, 2:Short, 3:Close
                lw = logits_w.clone()
                if need_w.sum() > 0:
                    maskw = torch.zeros_like(lw, dtype=torch.bool)
                    maskw[need_w.bool(), 1:] = True
                    maskw[~need_w.bool(), 0] = True
                    lw[~maskw] = -1e9
                
                dist_w = Categorical(logits=lw)
                new_logp_w = dist_w.log_prob(widx_b)
                ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)

                # è®¡ç®— PPO Ratio
                logp_new = new_logp_a + need_w * new_logp_w
                ratio = torch.exp(logp_new - logp_old_b)

                # Actor Loss (Clipped Surrogate Objective)
                surr1 = ratio * adv_b
                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv_b
                actor_loss = -torch.min(surr1, surr2).mean()

                # --- ğŸ”¥ æ”¹è¿›çš„ Critic Forward (Value Clipping) ---
                v_pred = self.critic(s).squeeze(-1)
                
                # v_loss1: åŸå§‹ MSE
                v_loss_unclipped = (v_pred - ret_b) ** 2
                # v_loss2: åŸºäºé‡‡æ ·æ—¶æ—§ Value çš„è£å‰ª MSE
                v_pred_clipped = v_old_b + torch.clamp(v_pred - v_old_b, -self.clip_eps, self.clip_eps)
                v_loss_clipped = (v_pred_clipped - ret_b) ** 2
                
                # ç»¼åˆ Value Lossï¼Œå–æœ€å¤§å€¼èƒ½å¸¦æ¥æ›´ç¨³å¥çš„æ¢¯åº¦
                value_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()

                # ç†µæ­£åˆ™åŒ–
                entropy = ent_a + 0.5 * ent_w
                
                # --- ğŸ”¥ æ€» Loss è®¡ç®— (VF Coeff=0.1, Ent Coeff=0.01) ---
                # é™ä½ä»·å€¼ç½‘ç»œæƒé‡ï¼Œä» 0.5 é™åˆ° 0.1ï¼Œå¹³æ»‘ç”±äº Return æ³¢åŠ¨å¸¦æ¥çš„ loss é£™å‡
                loss = actor_loss + 0.1 * value_loss - 0.01 * entropy
                
                # --- Backward & Step ---
                self.opt_adapter.zero_grad(set_to_none=True)
                self.opt_actor.zero_grad(set_to_none=True)
                self.opt_critic.zero_grad(set_to_none=True)
                
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.adapter.parameters(), 0.5)
                
                self.opt_adapter.step()
                self.opt_actor.step()
                self.opt_critic.step()
                
                # ç»Ÿè®¡
                with torch.no_grad():
                    kl_b = (logp_old_b - logp_new).mean().abs().item()
                    epoch_kl.append(kl_b)
                    stats["loss"].append(loss.item())
                    stats["kl"].append(kl_b)
                    stats["actor_loss"].append(actor_loss.item())
                    stats["value_loss"].append(value_loss.item())
                    stats["entropy"].append(entropy.item())

            # æ—©åœæ£€æŸ¥
            avg_kl = sum(epoch_kl) / len(epoch_kl) if epoch_kl else 0
            if avg_kl > 1.5 * target_kl: break

        # ---------------------------------------------------------------------
        # 5. æ›´æ–°çŠ¶æ€å½’ä¸€åŒ–ç»Ÿè®¡é‡ (RunningMeanStd)
        # ---------------------------------------------------------------------
        if M > 0:
            with torch.no_grad():
                for st in range(0, M, mb):
                    idx = slice(st, min(st + mb, M))
                    curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                    hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                    self._build_state(curr_b, hist_b, norm_update=True)

        # å­¦ä¹ ç‡ Scheduler æ­¥è¿›
        self.scheduler_actor.step()
        self.scheduler_critic.step()
        self.scheduler_adapter.step()
        
        def get_avg(k):
            return sum(stats[k]) / len(stats[k]) if stats[k] else 0.0

        return get_avg("loss"), get_avg("kl"), get_avg("actor_loss"), get_avg("value_loss"), get_avg("entropy")


    def old_update_from_trajs(self, trajs: List[Dict[str, Any]], target_kl: float = 0.015):
        """
        returns: loss, kl, actor_loss, value_loss, entropy
        é‡æ„é‡ç‚¹ï¼š
        1. å…¨ç¨‹ç§»é™¤ np.stackï¼Œå…¨éƒ¨æ”¹ä¸º torch.stack/as_tensorï¼Œå½»åº•è§£å†³ _ArrayMemoryErrorã€‚
        2. ç»Ÿä¸€æ•°æ®æµï¼šList -> CPU Tensor -> Flatten & Filter -> GPU Mini-batchã€‚
        """
        assert self.actor is not None and self.critic is not None
        
        # ---------------------------------------------------------------------
        # 1. ç»Ÿä¸€å †å  (Stacking) - å…¨éƒ¨åœ¨ CPU ä¸Šå®Œæˆï¼Œé¿å… Numpy å†…å­˜å³°å€¼
        # ---------------------------------------------------------------------
        # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†ä¸åŒç±»å‹çš„æ•°æ®å¹¶è½¬ä¸º Tensor

        def stack_key(key, dtype):
            # --- ä¼˜åŒ–å¼€å§‹ ---
            # è·å– trajs çš„é•¿åº¦ (N)
            N = len(trajs)
            if N == 0:
                return torch.tensor([], dtype=dtype)

            # 1. æ‹¿ç¬¬ä¸€ä¸ªå…ƒç´ æ¥ç¡®å®šå½¢çŠ¶ (é¿å…æŠŠæ‰€æœ‰å…ƒç´ éƒ½è½¬æˆ Tensor æ”¾åœ¨åˆ—è¡¨é‡Œ)
            # æ³¨æ„ï¼šè¿™é‡Œåªè½¬ç¬¬ä¸€ä¸ªï¼Œå¼€é”€å¾ˆå°
            first_val = trajs[0][key]
            sample_tensor = torch.as_tensor(first_val, dtype=dtype)
            sample_shape = sample_tensor.shape

            # 2. è®¡ç®—ç›®æ ‡ Tensor çš„å½¢çŠ¶
            # åŸä»£ç æ˜¯ dim=1ï¼Œæ„å‘³ç€åœ¨ç¬¬ 1 ç»´æ’å…¥ N
            # ä¾‹å¦‚: å•ä¸ªæ˜¯ (T, L, Dh)ï¼Œç»“æœå°±æ˜¯ (T, N, L, Dh)
            target_shape = list(sample_shape)
            target_shape.insert(1, N)

            # 3. é¢„åˆ†é…å†…å­˜
            # ä½¿ç”¨ empty åˆ†é…å†…å­˜æ¯” zeros å¿«ï¼Œå› ä¸ºæˆ‘ä»¬è¦é©¬ä¸Šè¦†ç›–å®ƒ
            out = torch.empty(target_shape, dtype=dtype)

            # 4. é€ä¸ªå¡«å…… (è¿™æ˜¯çœå†…å­˜çš„å…³é”®)
            # è¿™æ ·æ¯æ¬¡å¾ªç¯åªå¤„ç†ä¸€ä¸ª trajectory çš„æ•°æ®ï¼Œå¤„ç†å®Œå°±é‡Šæ”¾ä¸­é—´å˜é‡
            for i, tr in enumerate(trajs):
                # out.select(1, i) é€‰ä¸­ç¬¬ i ä¸ªåˆ‡ç‰‡ï¼Œå°†æ•°æ® copy è¿›å»
                # torch.as_tensor ä¼šå°½å¯èƒ½å…±äº«å†…å­˜ï¼Œå‡å°‘å¤åˆ¶
                val = tr[key]
                # å¦‚æœ val å·²ç»æ˜¯ tensor ä¸”è®¾å¤‡ä¸ out ä¸€è‡´ï¼Œcopy_ å¾ˆå¿«
                out.select(1, i).copy_(torch.as_tensor(val, dtype=dtype))
                
            return out
        # --- ä¼˜åŒ–ç»“æŸ ---

        # åªè¦ trajs é‡Œçš„ raw_hist æ˜¯ numpy arrayï¼Œtorch.as_tensor ä¼šå¾ˆé«˜æ•ˆ
        raw_curr  = stack_key("raw_curr", torch.float32) # (T, N, Dc)
        raw_hist  = stack_key("raw_hist", torch.float32) # (T, N, L, Dh)
        
        actions   = stack_key("actions", torch.long)     # (T, N)
        w_idx     = stack_key("w_idx", torch.long)       # (T, N)
        logp_old  = stack_key("logp_old", torch.float32) # (T, N)
        value_old = stack_key("value_old", torch.float32)# (T, N)
        rewards   = stack_key("rewards", torch.float32)  # (T, N)
        done      = stack_key("done", torch.float32)     # (T, N)
        valid     = stack_key("valid", torch.float32)    # (T, N)
        
        # last_value å¤„ç†ç¨å¾®ä¸åŒï¼Œå®ƒæ˜¯ (N,)
        last_value = torch.stack([torch.as_tensor(tr["last_value"], dtype=torch.float32) for tr in trajs], dim=0).squeeze()

        T, N = raw_curr.shape[:2]

        # ---------------------------------------------------------------------
        # 2. è®¡ç®— GAE (ä»åœ¨ CPU ä¸Šï¼Œè®¡ç®—é‡å¾ˆå°ï¼Œä¸éœ€è¦ GPU)
        # ---------------------------------------------------------------------
        with torch.no_grad():
            adv = torch.zeros((T, N), dtype=torch.float32)
            last_gae = torch.zeros((N,), dtype=torch.float32)
            
            # è¿™é‡Œçš„è®¡ç®—é€»è¾‘å®Œå…¨ä¸éœ€è¦æ”¹ï¼Œä½†ç°åœ¨å˜é‡å…¨æ˜¯ Tensor äº†
            for t in reversed(range(T)):
                m = (1.0 - done[t]) * valid[t]
                v_tp1 = last_value if t == T - 1 else value_old[t + 1]
                delta = rewards[t] + self.gamma * v_tp1 * m - value_old[t]
                last_gae = delta + self.gamma * self.gae_lambda * m * last_gae
                adv[t] = last_gae * valid[t]

            ret = adv + value_old

            # -----------------------------------------------------------------
            # 3. Flatten & Filter (åŸºäº valid mask è¿‡æ»¤æ— æ•ˆæ­¥)
            # -----------------------------------------------------------------
            # åˆ›å»º bool mask (T*N)
            mask_flat = valid.view(-1) > 0.5
            if not mask_flat.any():
                return 0.0, 0.0, 0.0, 0.0, 0.0
            
            # å®šä¹‰ Flatten å‡½æ•°
            def flat_and_filter(tensor_in):
                # reshape åˆ° (T*N, ...) ç„¶åå– mask
                return tensor_in.flatten(0, 1)[mask_flat]

            # å¤§å—å†…å­˜æ“ä½œï¼šè¿™é‡Œä¼šäº§ç”Ÿä¸€ä»½æ–°çš„è¿‡æ»¤åçš„å†…å­˜ï¼Œä½†å› ä¸ºè¿‡æ»¤æ‰äº†æ— æ•ˆæ•°æ®ï¼Œé€šå¸¸æ¯”åŸæ•°æ®å°
            curr_flat = flat_and_filter(raw_curr)       # (M, Dc)
            hist_flat = flat_and_filter(raw_hist)       # (M, L, Dh)
            act_flat  = flat_and_filter(actions)        # (M,)
            widx_flat = flat_and_filter(w_idx)          # (M,)
            logp_old_flat = flat_and_filter(logp_old)   # (M,)
            adv_flat  = flat_and_filter(adv)            # (M,)
            ret_flat  = flat_and_filter(ret)            # (M,)

            # å½’ä¸€åŒ– Advantage
            adv_flat = (adv_flat - adv_flat.mean()) / (adv_flat.std() + 1e-8)
        
        advs = adv_flat
        adv_flat = torch.clamp(advs, -5.0, 5.0)
        x1 = (torch.abs(advs) > 3.0).sum().item() / advs.shape[0]
        x2 = (torch.abs(advs) > 2.0).sum().item() / advs.shape[0]
        x3 = (torch.abs(advs) > 1.0).sum().item() / advs.shape[0]
        print(f"ã€Ratioã€‘ (abs(adv) > 3.0 | 2.0 | 1.0) : {x1} | {x2} | {x3}")
        print(f"Advantage Max: {advs.max().item():.4f} | Min: {advs.min().item():.4f} | Mean: {advs.mean().item():.4f}")

        # ---------------------------------------------------------------------
        # 4. PPO Training Loop (Mini-batch)
        # ---------------------------------------------------------------------
        M = curr_flat.shape[0] # æœ‰æ•ˆæ ·æœ¬æ€»æ•°
        mb = self.update_mb_size
        
        # è®°å½•å™¨
        stats = {"loss": [], "kl": [], "actor_loss": [], "value_loss": [], "entropy": []}
        
        for _ep in range(self.k_epochs):
            # ç”Ÿæˆéšæœºç´¢å¼•
            indices = torch.randperm(M)
            
            epoch_kl = []
            
            for st in range(0, M, mb):
                idx = indices[st:st + mb]
                if len(idx) == 0: continue
                
                # --- Move to GPU (æ˜¾å­˜åªåœ¨è¿™é‡Œæ¶ˆè€—) ---
                curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                act_b  = act_flat[idx].to(self.device, non_blocking=True)
                widx_b = widx_flat[idx].to(self.device, non_blocking=True)
                logp_old_b = logp_old_flat[idx].to(self.device, non_blocking=True)
                adv_b  = adv_flat[idx].to(self.device, non_blocking=True)
                ret_b  = ret_flat[idx].to(self.device, non_blocking=True)
                
                # --- Forward ---
                # norm_update=False: è®­ç»ƒé˜¶æ®µä¸æ›´æ–° running_mean/stdï¼Œä¿æŒç¨³å®š
                _, s = self._build_state(curr_b, hist_b, norm_update=False)

                logits_a, logits_w = self.actor(s)
                dist_a = Categorical(logits=logits_a)
                new_logp_a = dist_a.log_prob(act_b)
                ent_a = dist_a.entropy().mean()

                # Weight Action å¤„ç†é€»è¾‘
                need_w = ((act_b == A_LONG) | (act_b == A_SHORT) | (act_b == A_CLOSE)).float()
                
                # ä¼˜åŒ– maskw çš„åˆ›å»ºï¼Œé¿å… clone æ•´ä¸ª logits
                lw = logits_w  # å¦‚æœä¸éœ€è¦ inplace ä¿®æ”¹ï¼Œç›´æ¥ç”¨
                # è¿™é‡Œä¸ºäº†å®‰å…¨è¿˜æ˜¯ clone ä¸€ä¸‹ï¼Œé˜²æ­¢ inplace æŠ¥é”™ï¼Œä½†æ“ä½œå¯ä»¥ç®€åŒ–
                if need_w.sum() > 0:
                    # åªæœ‰å½“éœ€è¦å¤„ç†æƒé‡æ—¶æ‰è¿›è¡Œå¤æ‚çš„ mask æ“ä½œ
                    # (ç”±äºä½ çš„é€»è¾‘æ¯”è¾ƒç‰¹æ®Šï¼Œè¿™é‡Œä¿ç•™åŸé€»è¾‘ï¼Œä½†åŠ ä¸Š clone)
                    lw = logits_w.clone()
                    maskw = torch.zeros_like(lw, dtype=torch.bool)
                    maskw[need_w.bool(), 1:] = True
                    maskw[~need_w.bool(), 0] = True
                    lw[~maskw] = -1e9
                
                dist_w = Categorical(logits=lw)
                new_logp_w = dist_w.log_prob(widx_b)
                ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)

                logp_new = new_logp_a + need_w * new_logp_w
                ratio = torch.exp(logp_new - logp_old_b)

                surr1 = ratio * adv_b
                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * adv_b
                actor_loss = -torch.min(surr1, surr2).mean()

                v_pred = self.critic(s).squeeze(-1)
                
                value_loss = F.mse_loss(v_pred, ret_b)

                entropy = ent_a + 0.5 * ent_w
                loss = actor_loss + 0.5 * value_loss - 0.005 * entropy
                
                # --- Backward ---
                self.opt_adapter.zero_grad(set_to_none=True)
                self.opt_actor.zero_grad(set_to_none=True)
                self.opt_critic.zero_grad(set_to_none=True)
                
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                torch.nn.utils.clip_grad_norm_(self.adapter.parameters(), 0.5)
                
                self.opt_adapter.step()
                self.opt_actor.step()
                self.opt_critic.step()
                
                # Stats
                with torch.no_grad():
                    kl_b = (logp_old_b - logp_new).mean().abs().item()
                    epoch_kl.append(kl_b)
                    
                    stats["loss"].append(loss.item())
                    stats["kl"].append(kl_b)
                    stats["actor_loss"].append(actor_loss.item())
                    stats["value_loss"].append(value_loss.item())
                    stats["entropy"].append(entropy.item())

            # Early Stopping Check (Epoch Level)
            avg_kl = sum(epoch_kl) / len(epoch_kl) if epoch_kl else 0
            if avg_kl > 1.5 * target_kl:
                break
        
        # ---------------------------------------------------------------------
        # 5. Update Norm Stats (å•ç‹¬ä¸€è½®ï¼Œä¸å†è®¡ç®—æ¢¯åº¦ï¼Œä»…æ›´æ–° Running Mean/Std)
        # ---------------------------------------------------------------------
        # è¿™ä¸€æ­¥å¦‚æœä¸åš backwardï¼Œå…¶å®éå¸¸å¿«
        if M > 0:
            with torch.no_grad():
                # ä¸éœ€è¦ shuffleï¼Œç›´æ¥é¡ºåºè¿‡ä¸€é
                for st in range(0, M, mb):
                    idx = slice(st, min(st + mb, M))
                    curr_b = curr_flat[idx].to(self.device, non_blocking=True)
                    hist_b = hist_flat[idx].to(self.device, non_blocking=True)
                    # åªè¦å‰å‘ä¼ æ’­ï¼ŒAdapter å†…éƒ¨çš„ RunningMeanStd å°±ä¼šæ›´æ–°
                    self._build_state(curr_b, hist_b, norm_update=True)

        # Learning Rate Schedule
        self.scheduler_actor.step()
        self.scheduler_critic.step()
        self.scheduler_adapter.step()
        
        # Helper to get average
        def get_avg(k):
            return sum(stats[k]) / len(stats[k]) if stats[k] else 0.0

        return get_avg("loss"), get_avg("kl"), get_avg("actor_loss"), get_avg("value_loss"), get_avg("entropy")
   


# =========================
# Agent
# =========================
@dataclass
class AgentConfig:
    option_pairs: List[Dict[str, Any]]
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # env
    start_time: str = "20250408100000"
    end_time: str = "20250924150000"
    benchmark: str = "510050"
    fee: float = 1.3
    init_capital: float = 100000.0
    max_timesteps: int = 2000

    # rollout chunk
    rollout_T: int = 512
    num_workers: int = field(default_factory=lambda: max(1, min(mp.cpu_count() - 2, 12)))

    # model
    window_size: int = 32
    pre_len: int = 4
    n_variates: int = 13
    d_router: int = 128
    # adapter_dim: int = 128
    adapter_dim: int = 256

    # hidden_dim: int = 256
    hidden_dim: int = 512
    pretrained_path: str = "./miniQMT/DL/preTrain/weights/preMOE_best_dummy_data_32_4.pth"

    # PPO
    gamma: float = 0.99
    gae_lambda: float = 0.95
    clip_eps: float = 0.2
    k_epochs: int = 8
    epochs: int = 50,
    actor_lr: float = 3e-4
    critic_lr: float = 5e-4
    check_path: str = './miniQMT/DL/checkout/check_data_parallel.pt'

    # logging
    save_excel: bool = False
    excel_path: str = "./miniQMT/DL/results/PPO_training_data.xlsx"

    mini_batch: int=2048


class Agent:
    def __init__(self, cfg: AgentConfig):
        self.cfg = cfg
        self.cfg.pretrained_path = f"./miniQMT/DL/preTrain/weights/preMOE_best_dummy_data_{cfg.window_size}_{cfg.pre_len}.pth"
        self.device = cfg.device
        self.pairs = cfg.option_pairs
        assert len(self.pairs) > 0, "option_pairs is empty"

        # build learner
        self.learner = LearnerPPO(
            device=cfg.device,
            window_size=cfg.window_size,
            pre_len=cfg.pre_len,
            n_variates=cfg.n_variates,
            d_router=cfg.d_router,
            pretrained_path=cfg.pretrained_path,
            adapter_dim=cfg.adapter_dim,
            hidden_dim=cfg.hidden_dim,
            gamma=cfg.gamma,
            gae_lambda=cfg.gae_lambda,
            clip_eps=cfg.clip_eps,
            k_epochs=cfg.k_epochs,
            update_mb_size=cfg.mini_batch,
            total_epochs = cfg.epochs,
            actor_lr=cfg.actor_lr,
            critic_lr=cfg.critic_lr,
            check_path=cfg.check_path
        )

        # build vector env
        workers = min(cfg.num_workers, len(self.pairs))
        workers = max(1, workers)

        env_fns = []
        base_env_cfg = {
            "start_time": cfg.start_time,    # å¦‚æœ cfg æ²¡æœ‰ï¼Œå°±åˆ æ‰è¿™ä¸¤è¡Œ
            "end_time": cfg.end_time,
            "benchmark": cfg.benchmark,
            "fee": cfg.fee,
            "init_capital": cfg.init_capital,
        }

        for i in range(workers):
            # å…³é”®ï¼šæ¯ä¸ª make_env ç”¨ä¸€ä¸ªâ€œç‹¬ç«‹æ‹·è´â€çš„ cfgï¼Œé¿å…é—­åŒ…/å¼•ç”¨é—®é¢˜
            c = dict(base_env_cfg)

            def make_env(seed=i, pairs=self.pairs, c=c):
                return DynamicWindowEnv(pairs, c, seed=seed)
            env_fns.append(make_env)


        worker_cfg = {
            "window_size": cfg.window_size,
            "pre_len": cfg.pre_len,
            "n_variates": cfg.n_variates,
            "d_router": cfg.d_router,
            "pretrained_path": cfg.pretrained_path,
            "adapter_dim": cfg.adapter_dim,
            "hidden_dim": cfg.hidden_dim,
        }

        self.vec_env = SubprocVectorEnv(env_fns, worker_cfg=worker_cfg)
        self.workers = workers

        # warmup build learner modules
        tmp = env_fns[0]()
        c0, h0, _ = tmp.reset()
        tmp.close()
        c0_t = torch.from_numpy(np.asarray(c0, np.float32)).to(self.device)
        h0_t = torch.from_numpy(np.asarray(h0, np.float32)).to(self.device)

        self.learner._maybe_build(c0_t.unsqueeze(0), h0_t.unsqueeze(0))

        self.records = {
            'epoch': [], 'reward': [], 'avg_equity': [], 'loss': [], 'kl': [],
            'hold_ratio': [], 'long_ratio': [], 'short_ratio': [], 'close_ratio': [],
            'actor_loss': [], 'value_loss': [], 'entropy': [],
            'ratio_0': [], 'ratio_25': [], 'ratio_50': [], 'ratio_75': [], 'ratio_100': [],
            'sharpe': [],
            'right_sharpe_ratio': [],
            'golden_task_count': [],   # âœ… æ–°å¢ï¼šè¿›å…¥é»„é‡‘åŒºé—´çš„ä»»åŠ¡æ•°é‡
            'gambling_task_count': [], # âœ… æ–°å¢ï¼šåœ¨èµŒåšåŒºé—´çš„ä»»åŠ¡æ•°é‡
        }


        # Warmup Normalization (ä¿®å¤ç‰ˆ)
        print("[Info] Warming up Normalization layers...")
        # 1. éšæœºæ´¾å‘ä»»åŠ¡
        self.vec_env.set_tasks([random.randint(0, len(self.pairs)-1) for _ in range(workers)])
        
        # 2. è·‘æ•°æ®
        trajs = self.vec_env.rollout(512)
        
        # 3. ğŸ”¥å…³é”®ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰ raw data å¹¶æ›´æ–° Learner çš„ Norm
        # æˆ‘ä»¬å€Ÿç”¨ learner å†…éƒ¨çš„æ–¹æ³•æ¥æ„å»ºçŠ¶æ€ï¼Œå¼ºåˆ¶ update=True
        all_curr = np.concatenate([t["raw_curr"] for t in trajs], axis=0)
        all_hist = np.concatenate([t["raw_hist"] for t in trajs], axis=0)
        
        # è½¬ä¸º Tensor
        c_t = torch.from_numpy(all_curr).float().to(self.device)
        h_t = torch.from_numpy(all_hist).float().to(self.device)
        
        # å¼ºåˆ¶æ›´æ–° Norm
        self.learner._build_state(c_t, h_t, norm_update=True)
        print(f"[Info] Warmup done. Norm counts: {self.learner.norm.running_ms.n}")


    def train_dynamic(self, from_check_point: bool = False):
        print(f'shape = {self.learner.norm.running_ms.mean.shape}')
        if from_check_point:
            sys.stdout = outPut("./miniQMT/DL/results/PPO_records.txt", mode='a')
        else:
            sys.stdout = outPut("./miniQMT/DL/results/PPO_records.txt", mode='w')
        

        """
        âœ… æ–°é€»è¾‘ï¼šæŒ‰ steps æ’åº -> æŒ‰ num_workers åˆ†ç»„ -> æŒ‰â€œç»„å†… steps æ€»å’Œâ€æ¯”ä¾‹éšæœºæŠ½ç»„
        ç´¯ç§¯æŠ½åˆ°çš„ç»„çš„æ€» steps >= rollout_T_big(é»˜è®¤8192) åï¼Œæ‰æ‰§è¡Œä¸€æ¬¡ PPO updateã€‚

        æ³¨æ„ï¼š
        - æ¯æ¬¡æŠ½åˆ°ä¸€ä¸ªç»„ï¼Œä¼šè®©æ‰€æœ‰ worker å„è·‘ä¸€ä¸ª pairï¼ˆç»„ä¸æ»¡ workers ä¼š padding é‡å¤æœ€åä¸€ä¸ª idxï¼Œä½† update/ç»Ÿè®¡æ—¶ä¼šä¸¢å¼ƒé‡å¤éƒ¨åˆ†ï¼‰
        - æ¯ä¸ªç»„çš„ rollout_len = è¯¥ç»„å†… max(steps)ï¼ˆä¿è¯ç»„å†…æ¯ä¸ª pair éƒ½èƒ½è·‘å®Œæ•´å‘¨æœŸï¼›çŸ­çš„ä¼šæå‰ done -> valid=False paddingï¼‰
        - æ”¶é›†å¤šæ¬¡ rollout å¾—åˆ°çš„ traj é•¿åº¦ä¸åŒï¼šupdate å‰ç»Ÿä¸€ pad åˆ° Tmaxï¼Œå¹¶ç”¨ valid mask å±è”½ paddingã€‚
        """

        # -------------------------
        # 0) åŸºæœ¬å‚æ•°
        # -------------------------
        total_pairs = len(self.pairs)
        print(f'[Agent-init-train] æœŸæƒç»„åˆæ•°é‡ = {total_pairs} | è¿›ç¨‹æ•°é‡ = {self.cfg.num_workers}')
        current_time = datetime.now()
        formatted_time_string = current_time.strftime("%Y-%m-%d %H:%M:%S")

        print(f' ------ Start to train PPO on {self.device}, Time_stamp: {formatted_time_string} --- ')
        
        # PPO ç®—æ³•æ ¸å¿ƒå‚æ•°
        print(f'PPO Hyperparams 1: gamma = {self.learner.gamma}, gae_lambda = {self.learner.gae_lambda}, clip_eps = {self.learner.clip_eps}')
        
        # è®­ç»ƒè¿­ä»£å’Œæ‰¹æ¬¡å¤§å°
        print(f'PPO Hyperparams 2: k_epochs = {self.learner.k_epochs}, update_mb_size = {self.learner.update_mb_size}, total_epochs = {self.learner.total_epochs}')
        
        # å­¦ä¹ ç‡é…ç½®
        print(f'PPO Hyperparams 3: actor_lr = {self.learner.actor_lr}, critic_lr = {self.learner.critic_lr}')
        
        # æ¨¡å‹ç»“æ„ç»´åº¦
        print(f'Model Dims: adapter_dim = {self.learner.adapter_dim}, hidden_dim = {self.learner.hidden_dim}')
        print(f'------------------------------------------------------------------------------')
        workers = self.workers

        # å¤§çš„é‡‡æ ·ç›®æ ‡ï¼šé»˜è®¤ 8192ï¼ˆä¼˜å…ˆç”¨ cfg.rollout_T_bigï¼›å¦åˆ™é€€åŒ–ç”¨ cfg.rollout_Tï¼›å¦åˆ™ 8192ï¼‰
        rollout_T_big = int(getattr(self.cfg, "rollout_T_big", getattr(self.cfg, "rollout_T", 8192)))
        if rollout_T_big <= 0:
            rollout_T_big = 8192
        
        print(f"[Info] é‡‡æ ·ç›®æ ‡, rollout_T_big = {rollout_T_big}")

        # æ—©åœç›¸å…³
        best_reward = -float('inf')
        patience = getattr(self.cfg, 'patience', 15)
        stop_entropy = getattr(self.cfg, 'stop_entropy', 0.8)
        min_delta = 0.001
        early_stop_counter = 0

        # records ç¡®ä¿å­˜åœ¨
        if not hasattr(self, "records") or self.records is None:
            self.records = {
                "epoch": [], "reward": [], "avg_equity": [], "loss": [], "kl": [],
                "hold_ratio": [], "long_ratio": [], "short_ratio": [], "close_ratio": [],
                "actor_loss": [], "value_loss": [], "entropy": [],
                "ratio_0": [], "ratio_25": [], "ratio_50": [], "ratio_75": [], "ratio_100": [],
            }

        # checkpoint
        start_epoch = 0
        if from_check_point:
            start_epoch, best_reward = self.learner.load_checkpoint()

            print(f'[Info] ä»checkpointå¼€å§‹è®­ç»ƒ, last_epoch = {start_epoch}, last_best_reward = {best_reward}')

        # -------------------------
        # 1) é¢„å¤„ç†ï¼šæŒ‰ steps æ’åºï¼Œå¹¶æŒ‰ workers åˆ†ç»„
        # -------------------------
        # æ¯ä¸ª pair å¿…é¡»æœ‰ steps
        steps_arr = np.array([int(p.get("steps", 0)) for p in self.pairs], dtype=np.int64)
        if (steps_arr <= 0).any():
            bad = np.where(steps_arr <= 0)[0][:10].tolist()
            raise ValueError(f"[train] Found invalid steps<=0 at indices: {bad} (showing first 10).")

        sorted_ids = np.argsort(-steps_arr)  # desc

        groups = []          # list[list[int]]
        group_sum_steps = [] # list[int]  é‡‡æ ·æ¦‚ç‡ç”¨ï¼šç»„å†… steps æ€»å’Œï¼ˆç»„ä¸æ»¡ workers ä¹Ÿæ²¡é—®é¢˜ï¼‰
        group_max_steps = [] # list[int]  rollout_len ç”¨ï¼šç»„å†… max steps

        for i in range(0, total_pairs, workers):
            g = sorted_ids[i:i + workers].tolist()
            if len(g) == 0:
                continue
            groups.append(g)
            group_sum_steps.append(int(steps_arr[g].sum()))
            group_max_steps.append(int(steps_arr[g].max()))

        # æ¦‚ç‡ï¼ˆä¸ç»„å†… steps æ€»å’Œæˆæ­£æ¯”ï¼‰
        sum_all = float(sum(group_sum_steps))
        probs = [float(s) / (sum_all + 1e-12) for s in group_sum_steps]

        # ç”¨éšæœºæ•°é‡‡æ ·ç»„ï¼ˆä¸ä¾èµ– np.random.choiceï¼Œæ–¹ä¾¿ä½ è°ƒè¯•ï¼‰
        def sample_group_index():
            r = random.random()
            c = 0.0
            for idx, p in enumerate(probs):
                c += p
                if r <= c:
                    return idx
            return len(probs) - 1

        # -------------------------
        # 2) pad helperï¼šç»Ÿä¸€ traj é•¿åº¦åˆ° Tmaxï¼Œpadding éƒ¨åˆ† valid=False
        # -------------------------
        def pad_traj_to_T(tr, T_max: int):
            T0 = int(tr["raw_curr"].shape[0])
            if T0 == T_max:
                return tr

            def pad_arr(x, pad_value, dtype=None):
                x = np.asarray(x)
                if dtype is not None:
                    x = x.astype(dtype, copy=False)
                if x.ndim == 1:
                    out = np.full((T_max,), pad_value, dtype=x.dtype)
                    out[:T0] = x
                    return out
                if x.ndim == 2:
                    out = np.full((T_max, x.shape[1]), pad_value, dtype=x.dtype)
                    out[:T0, :] = x
                    return out
                if x.ndim == 3:
                    out = np.full((T_max, x.shape[1], x.shape[2]), pad_value, dtype=x.dtype)
                    out[:T0, :, :] = x
                    return out
                raise ValueError(f"Unsupported ndim={x.ndim} for padding.")

            out = dict(tr)
            out["raw_curr"] = pad_arr(tr["raw_curr"], 0.0, dtype=np.float32)
            out["raw_hist"] = pad_arr(tr["raw_hist"], 0.0, dtype=np.float32)

            out["actions"] = pad_arr(tr["actions"], A_HOLD, dtype=np.int64)
            out["w_idx"] = pad_arr(tr["w_idx"], 0, dtype=np.int64)
            out["w_val"] = pad_arr(tr["w_val"], 0.0, dtype=np.float32)

            out["logp_old"] = pad_arr(tr["logp_old"], 0.0, dtype=np.float32)
            out["value_old"] = pad_arr(tr["value_old"], 0.0, dtype=np.float32)
            out["rewards"] = pad_arr(tr["rewards"], 0.0, dtype=np.float32)

            out["done"] = pad_arr(np.asarray(tr["done"], np.bool_), True, dtype=np.bool_)
            out["valid"] = pad_arr(np.asarray(tr["valid"], np.bool_), False, dtype=np.bool_)

            # bootstrapï¼špadding åç›´æ¥ç½® 0ï¼ˆä¸å½±å“ï¼‰
            out["last_value"] = np.array([0.0], dtype=np.float32)
            return out

        # -------------------------
        # 3) ä¸»å¾ªç¯ï¼šæ¯ä¸ª ep åš 1 æ¬¡ PPO æ›´æ–°ï¼ˆä½† rollout å¯ä»¥æŠ½å¤šç»„ç´¯ç§¯ï¼‰
        # -------------------------
        sys.stdout.flush() # å¼ºåˆ¶å°†ç¼“å†²åŒºå†™å…¥ç£ç›˜

        for ep in range(self.cfg.epochs):
            if from_check_point and start_epoch is not None and ep <= start_epoch:
                print(f'[Skip] epoch = {ep}')
                continue

            t0 = time.time()
            
            if ep == 0 or ep == 1: # æ¯50è½®æ‰“å°å¹¶åŒæ­¥ä¸€æ¬¡å³å¯
                #  print(f"[Curriculum-warmup] Set Fee to {current_fee}")
                #  self.vec_env.set_fee_all(current_fee)
                 self.vec_env.set_fee_all(1.3)

            # æœ¬è½®ç´¯è®¡çš„ç»„ stepsï¼ˆæŒ‰ä½ è¦æ±‚ç”¨â€œç»„å†… steps æ€»å’Œâ€æ¥åˆ¤æ–­æ˜¯å¦å¤Ÿ 8192ï¼‰
            sampled_steps_sum = 0

            # æ”¶é›†åˆ°çš„ trajï¼ˆæ³¨æ„ï¼šæ¯æ¬¡æŠ½ç»„ä¼šè¿”å› workers æ¡ï¼Œä½†æœ€åä¸€ç»„å¯èƒ½ < workersï¼Œæˆ‘ä»¬ä¼šä¸¢å¼ƒ padding éƒ¨åˆ†ï¼‰
            collected_trajs = []
            epoch_all_env_sharpes = []

            # ç»Ÿè®¡ï¼ˆæŒ‰ valid ç»Ÿè®¡ï¼‰
            total_reward_sum = 0.0
            total_valid_steps = 0
            action_counts = np.zeros(4, dtype=np.int64)
            weight_counts = np.zeros(5, dtype=np.int64)
            equity_sum = 0.0
            equity_cnt = 0


            total_annual_ret_sum = 0.0
            count = 0
            task_sharpe_tracker = {} # {task_id: [sr1, sr2, ...]}
            # åå¤æŠ½ç»„ç›´åˆ°ç´¯è®¡ç»„ steps >= rollout_T_big
            while sampled_steps_sum < rollout_T_big:
                g_idx = sample_group_index()
                group = groups[g_idx]
                true_cnt = len(group)

                task_ids_raw = group.copy()

                # ã€æ–°å¢ã€‘é¢„åˆ¤é€»è¾‘ï¼šå¦‚æœåŠ ä¸Šè¿™ç»„ä¼šè¶…è¿‡ç›®æ ‡å¤ªå¤š(æ¯”å¦‚1.1å€)ï¼Œä¸”å½“å‰å·²ç»æœ‰ä¸å°‘æ•°æ®äº†ï¼Œå°±è·³è¿‡è¿™ç»„æˆ–ç›´æ¥ break
                # é˜²æ­¢å†…å­˜å‹åŠ›å¤ªå¤§
                if sampled_steps_sum > rollout_T_big * 0.8 and (sampled_steps_sum + group_sum_steps[g_idx]) > rollout_T_big * 1.2:
                     # è¿™ä¸€ç»„å¤ªå¤§ï¼Œå®¹æ˜“çˆ†å†…å­˜ï¼Œæ¢ä¸€ç»„å°çš„è¯•è¯•ï¼Œæˆ–è€…ç›´æ¥ break
                    if sampled_steps_sum >= rollout_T_big: 
                        break
                    continue


                # ç»„è´¡çŒ®çš„â€œé•¿åº¦â€ï¼ˆé‡‡æ ·æ¦‚ç‡/ç´¯è®¡æ­¥æ•°ä½¿ç”¨ sum stepsï¼‰
                sampled_steps_sum += int(group_sum_steps[g_idx])

                # rollout_len ç”¨ max stepsï¼Œç¡®ä¿ç»„å†…æ¯ä¸ª pair éƒ½èƒ½è·‘åˆ°è‡ªå·± doneï¼ˆå®Œæ•´å‘¨æœŸï¼‰
                rollout_len = int(group_max_steps[g_idx])


                # padding tasksï¼šå¡«æ»¡ workersï¼ˆé¿å… vec_env æ–­è¨€ï¼‰
                task_ids = group.copy()
                print(f"choose_task_ids = {task_ids}, sum_roll_len = {sampled_steps_sum}")

                mx = max(self.pairs[i]["steps"] for i in task_ids)
                mn = min(self.pairs[i]["steps"] for i in task_ids)
                # print("group steps min/max:", mn, mx)

                while len(task_ids) < workers:
                    task_ids.append(task_ids[-1])

                # 1) set tasks
                self.vec_env.set_tasks(task_ids)

                # 2) broadcast weights + norm snapshot
                payload = self.learner.export_payload()

                self.vec_env.set_weights_all(payload)

                # 3) one-shot rolloutï¼ˆä¸€æ¬¡ IPC æ”¶å…¨è½¨è¿¹ï¼‰
                trajs = self.vec_env.rollout(rollout_len)

                # åªä¿ç•™çœŸå®ç»„å†…çš„ true_cnt æ¡ï¼ˆä¸¢å¼ƒ padding é‡å¤ï¼‰
                trajs = trajs[:true_cnt]
                collected_trajs.extend(trajs)

            # âœ… æ–°å¢ï¼šå°†å¤æ™®å½’ç±»åˆ°å…·ä½“çš„ Task ID ä¸‹
                for tid, tr in zip(task_ids_raw, trajs):
                    sr = tr.get("env_sharpe", 0.0)
                    if tid not in task_sharpe_tracker:
                        task_sharpe_tracker[tid] = []
                    task_sharpe_tracker[tid].append(sr)
                    epoch_all_env_sharpes.append(sr)

                # å¹´åŒ–(252äº¤æ˜“æ—¥, 1å¤©8ä¸ª30åˆ†é’ŸKçº¿)
                STEPS_PER_YEAR = 252 * 8

                # ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡ valid çš„éƒ¨åˆ†ï¼‰
                for tr in trajs:
                    es = tr.get("env_sharpe", 0.0)
                    epoch_all_env_sharpes.append(es)

                    valid = np.asarray(tr["valid"], dtype=np.bool_)
                    valid_steps = valid.sum()

                    r = np.asarray(tr["rewards"], dtype=np.float32)

                    total_reward_sum += float(r[valid].sum())
                    total_valid_steps += int(valid.sum())

                    acts = np.asarray(tr["actions"], dtype=np.int64)[valid]
                    if acts.size > 0:
                        action_counts += np.bincount(acts, minlength=4)

                    widx = np.asarray(tr["w_idx"], dtype=np.int64)[valid]
                    if widx.size > 0:
                        weight_counts += np.bincount(widx, minlength=5)

                    if "equity_end" in tr:
                        eq = float(np.asarray(tr["equity_end"]).reshape(-1)[0])
                        if not np.isnan(eq):
                            equity_sum += eq
                            equity_cnt += 1

                            if valid_steps > 0:
                                abs_ret = (eq - self.cfg.init_capital) / self.cfg.init_capital
                                annual_ret = abs_ret * (STEPS_PER_YEAR / valid_steps)
                                total_annual_ret_sum += annual_ret
                                count += 1

            # --- ğŸ”¥ æ ¸å¿ƒæ”¹è¿›ï¼šåœ¨ while å¾ªç¯ç»“æŸåï¼Œè¿›è¡Œä»»åŠ¡ç”»åƒåˆ†æ ---
            task_analysis = {"golden": [], "gambling": [], "failing": [], "normal": []}
            task_avg_srs = {}
            for tid, srs in task_sharpe_tracker.items():
                avg_sr = np.mean(srs)
                task_avg_srs[tid] = avg_sr.item()
                if 1.0 <= avg_sr <= 2.5:
                    task_analysis["golden"].append(tid)
                elif avg_sr > 3.0:
                    task_analysis["gambling"].append(tid)
                elif avg_sr < 0.5:
                    task_analysis["failing"].append(tid)
                else:
                    task_analysis["normal"].append(tid)

            # æ‰“å°è¯¦ç»†ä»»åŠ¡æŠ¥å‘Š
            print(f"\n--- ğŸ“Š Task Performance Report (Epoch {ep+1}) ---")
            print(f"âœ… [Golden] (1.0<=SR<=2.5) æ•°é‡: {len(task_analysis['golden'])} | IDs: {task_analysis['golden']}")
            print(f"ğŸš€ [Gambling] (SR > 3.0) æ•°é‡: {len(task_analysis['gambling'])} | IDs: {task_analysis['gambling']}")
            print(f"ğŸ“‰ [Failing] (SR < 0.5) æ•°é‡: {len(task_analysis['failing'])} | IDs: {task_analysis['failing'][:10]}...")
            
            # æ‰¾åˆ°è¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ 3 ä¸ªç»„åˆ
            sorted_tasks = sorted(task_avg_srs.items(), key=lambda x: x[1], reverse=True)
            if sorted_tasks:
                print(f"ğŸ† é¡¶æ¢æŸ± Top 3 (ID, SR): {sorted_tasks[:3]}")
                print(f"ğŸ’€ æ‹–åè…¿ Bottom 3 (ID, SR): {sorted_tasks[-3:]}")
            print(f"--------------------------------------------\n")

            # ğŸ”¥ 3. åœ¨ while å¾ªç¯ç»“æŸåï¼Œè®¡ç®—å…¨ä½“ç»Ÿè®¡é‡
            if len(epoch_all_env_sharpes) > 0:
                sr_mean = np.mean(epoch_all_env_sharpes)
                sr_max = np.max(epoch_all_env_sharpes)
                sr_min = np.min(epoch_all_env_sharpes)
            else:
                sr_mean = sr_max = sr_min = 0.0   

            sp_cnt = 0
            for sp in epoch_all_env_sharpes:
                if sp >= 1.0 and sp <= 2.0:
                    sp_cnt += 1
            sp_cnt = sp_cnt / len(epoch_all_env_sharpes)

            # -------------------------
            # 4) PPO updateï¼šæŠŠæ‰€æœ‰ collected_trajs pad åˆ° Tmax åä¸€æ¬¡æ›´æ–°
            # -------------------------

            # 1. å…ˆè®¡ç®—æœ€å¤§é•¿åº¦ Tmax
            # æ³¨æ„ï¼šè¿™é‡Œåªè¯» shapeï¼Œä¸å¤åˆ¶æ•°æ®ï¼Œå¾ˆå¿«ä¸”ä¸å å†…å­˜
            Tmax = max(int(tr["raw_curr"].shape[0]) for tr in collected_trajs)

            # 2. é€ä¸ªå¤„ç†å¹¶é‡Šæ”¾æ—§æ•°æ® (å…³é”®ä¿®æ”¹)
            trajs_for_update = []
            
            # ä½¿ç”¨ while å¾ªç¯é…åˆ pop(0)ï¼Œå¤„ç†ä¸€ä¸ªå°±æ‰”æ‰ä¸€ä¸ªåŸå§‹æ•°æ®
            # è¿™æ ·å†…å­˜å§‹ç»ˆåªç»´æŒä¸€ä»½æ•°æ®ï¼Œä¸ä¼šç¿»å€
            while collected_trajs:
                # å¼¹å‡ºç¬¬ä¸€ä¸ªåŸå§‹è½¨è¿¹ (collected_trajs é•¿åº¦ -1)
                tr = collected_trajs.pop(0) 
                
                # ç”Ÿæˆ Padding åçš„æ–°è½¨è¿¹
                padded_tr = pad_traj_to_T(tr, Tmax)
                
                # åŠ å…¥æ–°åˆ—è¡¨
                trajs_for_update.append(padded_tr)
                
                # æ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œå‘Šè¯‰ Python "è¿™å—æ—§å†…å­˜å¯ä»¥é‡Šæ”¾äº†"
                del tr
            
            # 3. å¼ºåˆ¶è§¦å‘åƒåœ¾å›æ”¶ï¼Œæ•´ç†å†…å­˜ç¢ç‰‡
            import gc
            gc.collect()


            # trajs_for_update = [pad_traj_to_T(tr, Tmax) for tr in collected_trajs]

            valid_lens = [int(tr["valid"].sum()) for tr in trajs_for_update]
            # print("valid_len min/mean/max:", min(valid_lens), sum(valid_lens)/len(valid_lens), max(valid_lens))

            rs = np.concatenate([
                            tr["rewards"][tr["valid"].astype(bool)] 
                            for tr in trajs_for_update
                        ])
            print("[Info] Scaled reward mean/std/min/max:", rs.mean(), rs.std(), rs.min(), rs.max())

            if ep >= 50:
                self.k_epochs = 5

            loss, kl, a_loss, v_loss, ent, ev = self.learner.update_from_trajs(trajs_for_update)
            # loss, kl, a_loss, v_loss, ent = self.learner.update_from_trajs(trajs_for_update)

            if 'ev' not in self.records: self.records['ev'] = []
            self.records['ev'].append(float(ev))
            
            # -------------------------
            # 5) å†™ records + æ‰“å° + æ—©åœ
            # -------------------------
            avg_reward = total_reward_sum / (total_valid_steps + 1e-8)
            
            # avg_equity = equity_sum / max(1, equity_cnt)
            avg_anual_ret = total_annual_ret_sum / max(1, count) 
            avg_equity = (1 + avg_anual_ret) * self.cfg.init_capital

            act_total = int(action_counts.sum())
            if act_total == 0:
                hold_ratio = long_ratio = short_ratio = close_ratio = 0.0
            else:
                hold_ratio = float(action_counts[A_HOLD] / act_total)
                long_ratio = float(action_counts[A_LONG] / act_total)
                short_ratio = float(action_counts[A_SHORT] / act_total)
                close_ratio = float(action_counts[A_CLOSE] / act_total)

            w_total = int(weight_counts.sum())
            if w_total == 0:
                w_ratios = [0.0] * 5
            else:
                w_ratios = [float(weight_counts[k] / w_total) for k in range(5)]

            self.records["sharpe"].append(float(sr_mean))
            self.records["right_sharpe_ratio"].append(sp_cnt)
            self.records["epoch"].append(ep + 1)
            self.records["reward"].append(float(avg_reward))
            self.records["avg_equity"].append(float(avg_equity))
            self.records["loss"].append(float(loss))
            self.records["kl"].append(float(kl))

            self.records["hold_ratio"].append(float(hold_ratio))
            self.records["long_ratio"].append(float(long_ratio))
            self.records["short_ratio"].append(float(short_ratio))
            self.records["close_ratio"].append(float(close_ratio))

            self.records["actor_loss"].append(float(a_loss))
            self.records["value_loss"].append(float(v_loss))
            self.records["entropy"].append(float(ent))

            self.records["ratio_0"].append(float(w_ratios[0]))
            self.records["ratio_25"].append(float(w_ratios[1]))
            self.records["ratio_50"].append(float(w_ratios[2]))
            self.records["ratio_75"].append(float(w_ratios[3]))
            self.records["ratio_100"].append(float(w_ratios[4]))

            self.records["golden_task_count"].append(len(task_analysis["golden"]))
            self.records["gambling_task_count"].append(len(task_analysis["gambling"]))


            dt = time.time() - t0
            print(
                f"[Epoch {ep+1} / {self.cfg.epochs}] "
                f"sampled_group_steps={sampled_steps_sum} target={rollout_T_big} | "
                f"valid_steps={total_valid_steps} | "
                f"Reward:{avg_reward:.6f} | EV:{ev:.4f} | Market_value:{avg_equity:.2f} \n"
                f"Sharpe Ratio -> Mean: {sr_mean:.4f} | Max: {sr_max:.4f} | Min: {sr_min:.4f} | "
                f"Sharpe Ratio Percent(1.0~2.0) -> Ratio: {sp_cnt:.4f}\n"
                f"loss={loss:.4f} kl={kl:.4f} | "
                f"act(H/L/S/C)={hold_ratio:.2f}/{long_ratio:.2f}/{short_ratio:.2f}/{close_ratio:.2f} | "
                f"entropy={ent:.3f} time={dt:.1f}s"
            )

            # ä¿å­˜ Excel
            if getattr(self.cfg, "save_excel", False):
                if from_check_point is False:
                    pd.DataFrame(self.records).to_excel(self.cfg.excel_path, index=False)
                else:
                    df_old = pd.read_excel(self.cfg.excel_path)
                    df_combined = pd.concat([df_old, pd.DataFrame(self.records)], ignore_index=True)
                    df_combined.to_excel(self.cfg.excel_path, index=False)
            
            # ä¿å­˜æ–­ç‚¹
            self.learner.save(ep, best_reward)
            print(f'>>> ğŸŒŸ Model updated, saved, epoch = {ep}, best_ward = {best_reward}')

            # B. æ¯ 50 ä¸ª Epoch å¼ºåˆ¶å¤‡ä»½ä¸€ä¸ªå†å²ç‰ˆæœ¬ï¼Œæ–¹ä¾¿éšæ—¶å›æ»š
            if (ep + 1) % 50 == 0:
                history_path = f"{self.learner.check_path.replace('.pt', '')}_epoch_{ep+1}.pt"
                self.learner.save(ep, best_reward, path=history_path)
                print(f">>> ğŸ’¾ Historical Checkpoint Saved: {history_path}")

            # --- æ—©åœåˆ¤æ–­ ---
            if avg_reward > best_reward + min_delta:
                best_reward = avg_reward
                early_stop_counter = 0
                print(f"   >>> ğŸŒŸ Best Reward Updated: {best_reward:.4f} (Counter Reset)")
            else:
                early_stop_counter += 1
                print(f"   â³ [Patience] No improvement: {early_stop_counter}/{patience} | Best: {best_reward:.4f}")

            if early_stop_counter >= patience:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Reward has not improved for {patience} epochs.")
                print(f"   Final Best Reward: {best_reward:.4f}")
                break

            if ent < stop_entropy and avg_reward > 0:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Entropy ({ent:.4f}) is too low.")
                self.learner.save(ep, best_reward)
                break

            current_time = datetime.now()
            formatted_time_string = current_time.strftime("%Y-%m-%d %H:%M:%S")
            print(f'[Info] Finish train epoch {ep + 1} | Time-stamp: {formatted_time_string}')
            sys.stdout.flush() # å¼ºåˆ¶å°†ç¼“å†²åŒºå†™å…¥ç£ç›˜

        print(f"[Train] Finished. Data saved to {self.cfg.excel_path}")
        

    def close(self):
        self.vec_env.close()
        sys.stdout.flush() # å¼ºåˆ¶å°†ç¼“å†²åŒºå†™å…¥ç£ç›˜


def run_oos_test(model_path, test_pairs, cfg):
    """
    æ ·æœ¬å¤–æµ‹è¯• (Out-of-Sample Test)
    """
    # 1. åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒï¼ˆæ³¨æ„ is_test=Trueï¼‰
    test_cfg = cfg.__dict__.copy()
    test_cfg['is_test'] = True
    
    # 2. åŠ è½½ Learner å¹¶åŠ è½½æƒé‡
    learner = LearnerPPO(...) # æŒ‰å‚æ•°åˆå§‹åŒ–
    learner.load_checkpoint(model_path)
    learner.actor.eval() # å¼€å¯è¯„ä¼°æ¨¡å¼
    
    all_equities = []
    
    for pair in test_pairs:
        env = DynamicWindowEnv([pair], test_cfg)
        curr, hist, _ = env.reset()
        done = False
        equities = [cfg.init_capital]
        
        while not done:
            # è½¬ Tensor
            c_t = torch.from_numpy(curr).float().unsqueeze(0).to(learner.device)
            h_t = torch.from_numpy(hist).float().unsqueeze(0).to(learner.device)
            
            # ç¡®å®šæ€§æ¨ç†
            with torch.no_grad():
                _, s = learner._build_state(c_t, h_t, norm_update=False)
                # è°ƒç”¨ deterministic=True çš„é¢„æµ‹
                a, wi, wv, _, _ = learner.sample_action_weight(s, deterministic=True)
            
            # ç¯å¢ƒæ­¥è¿›
            curr, hist, r, term, trunc = env.step(a, wv)
            equities.append(env.account_controller.equity)
            done = term or trunc
            
        all_equities.append(equities)
        print(f"Task {pair['call']} Finished. Final Equity: {equities[-1]:.2f}")
    
    # ç»˜åˆ¶å‡€å€¼å›¾
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 6))
    for eq in all_equities:
        plt.plot(eq)
    plt.title("Out-of-Sample Performance")
    plt.show()

# =========================
# Entry
# =========================

if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)

    # 1. åŠ è½½åŸå§‹æ•°æ®
    dtype = {
        'call': str, 'put': str,
        'call_strike': float, 'put_strike': float,
        'call_open': str, 'call_expire': str,
    }
    df = pd.read_excel('./miniQMT/datasets/all_label_data/20251213_train.xlsx', dtype=dtype)

    # æ’é™¤åå•
    exclude_list = [
        '10007347', '10007466', '10007467', '10006436', '10007346', '10006435', '10007465', 
        '10007726', '10007725', '10007724', '10008052', '10007723', '10006434', '10007722', 
        '10008051', '10007345', '10007721', '10007464', '10007344', '10007988', '10006433', 
        '10006820', '10007720', '10007987', '10006746', '10006745', '10007463', '10006432', '10007719'
    ]
    
    all_pairs = []

    for index, row in df.iterrows():
        start = row['call_open']
        end = row['call_expire']

        start_time = datetime.strptime(start, "%Y%m%d")
        end_time = datetime.strptime(end, "%Y%m%d")
        days = (end_time - start_time).days

        if days <= 40:
            continue

        call = row['call']
        put = row['put']

        if call in exclude_list or put in exclude_list:
            continue
        end_time = start_time + timedelta(days=20)
        end_time = end_time.strftime('%Y%m%d')

        start_time = start + '100000'
        end_time = end_time + '150000'
        all_pairs.append({
            'call': call,
            'put': put,
            'start_time': start_time,
            'end_time': end_time
        })


    # 2. åŠ¨æ€åˆ†ç±»é‡‡æ ·é€»è¾‘
    all_pairs = []
    # åˆ†ç±»æ¡¶ï¼Œç›®æ ‡æ€»æ•°çº¦ 20 ä¸ª
    buckets = {"ITM": [], "ATM": [], "OTM": []}
    target_per_bucket = 20

    # é¢„åŠ è½½ä¸€ä¸ªè´¦æˆ·ç”¨äºè·å–åˆå§‹æ ‡çš„ä»·æ ¼
    temp_account = single_Account(100000, stockList=['510050'])

    # æ‰“ä¹±åŸå§‹æ•°æ®é¡ºåºï¼Œä¿è¯é‡‡æ ·çš„éšæœºæ€§
    # df = df.sample(frac=1).reset_index(drop=True)

    for index, row in df.iterrows():
        call, put = row['call'], row['put']
        if call in exclude_list or put in exclude_list:
            continue

        # æ—¶é—´é€»è¾‘
        start_str, expire_str = row['call_open'], row['call_expire']
        start_dt = datetime.strptime(start_str, "%Y%m%d")
        expire_dt = datetime.strptime(expire_str, "%Y%m%d")
        
        if (expire_dt - start_dt).days <= 40:
            continue

        # è®¡ç®—åˆå§‹ Moneyness (è¡Œæƒä»· / æ ‡çš„ä»·æ ¼)
        # å‡è®¾ start_time ä¸ºå¼€ç›˜ 10:00:00
        start_time_full = start_str + '100000'
        try:
            spot_price = temp_account.getOpenPrice('510050', start_time_full)
            if spot_price <= 0: continue
            
            # ä»¥ Call çš„è¡Œæƒä»·è®¡ç®—
            moneyness = row['call_strike'] / spot_price
            
            # ç®€å•åˆ†ç±»ï¼š0.98-1.02 ä¸ºå¹³å€¼ï¼Œå¤§äº 1.05 ä¸ºè™šå€¼ï¼Œå°äº 0.95 ä¸ºå®å€¼
            if 0.97 <= moneyness <= 1.03:
                cat = "ATM"
            elif moneyness > 1.03:
                cat = "OTM"
            else:
                cat = "ITM"
            
            # å¡«æ¡¶
            if len(buckets[cat]) < target_per_bucket:
                end_time_full = (expire_dt - timedelta(days=20)).strftime('%Y%m%d') + '150000'
                buckets[cat].append({
                    'call': call, 'put': put,
                    'start_time': start_time_full,
                    'end_time': end_time_full,
                    'steps': int(row['steps']),
                    'init_moneyness': moneyness
                })
        except:
            continue

        # if sum(len(v) for v in buckets.values()) >= 21:
        #     break

    # åˆå¹¶é‡‡æ ·ç»“æœ
    for cat_list in buckets.values():
        all_pairs.extend(cat_list)
    
    total_steps = sum(p['steps'] for p in all_pairs)
    print(f"[Info] é‡‡æ ·å®Œæˆã€‚ITM:{len(buckets['ITM'])}, ATM:{len(buckets['ATM'])}, OTM:{len(buckets['OTM'])}")
    print(f"[Info] æ€»ç»„åˆæ•°: {len(all_pairs)}, æ€»æ­¥æ•°: {total_steps}")


    # 3. é…ç½® Agent
    cfg = AgentConfig(
        option_pairs=all_pairs,
        window_size=32,
        pre_len=4,
        epochs=300,
        rollout_T=2048 * 12,     # æ¯æ¬¡æ›´æ–°é‡‡æ ·çš„åŸºç¡€é•¿åº¦
        num_workers=16,      # ç»´æŒ 12 çº¿ç¨‹
        save_excel=True,
        # --- æ ¸å¿ƒå‚æ•°è°ƒæ•´ ---
        hidden_dim=256,      # æå‡ç½‘ç»œå®½åº¦ä»¥é€‚åº”å¤æ‚é€»è¾‘
        adapter_dim=128,     # æå‡ç‰¹å¾æŠ•å½±ç»´åº¦
        mini_batch=2048 * 12, # ä¿æŒå¤§ Batch ç¨³å®šæ¢¯åº¦
        actor_lr=2e-4,
        critic_lr=5e-4,
        check_path='./miniQMT/DL/checkout/check_data_parallel.pt',
        k_epochs=10
    )

    agent = Agent(cfg)

    try:
        # å¦‚æœä½ ä¿®æ”¹äº† hidden_dimï¼Œå»ºè®®ä» False å¼€å§‹ï¼Œå› ä¸ºæ—§æƒé‡å½¢çŠ¶ä¸åŒ¹é…
        agent.train_dynamic(from_check_point=False) 
    finally:
        agent.close()
