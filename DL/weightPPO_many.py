"""
    PPOç®—æ³• (æ ‡å‡†ç²¾åº¦ Float32 ç‰ˆ) - åŠ¨æ€å¹¶è¡Œè®­ç»ƒé‡æ„ç‰ˆ (Full Optimized)
    åŒ…å«: Multiprocessing Parallellism + Excel Export + Dynamic Environment Loading + Data Caching (Shared Memory)
    ä¿®å¤: 
    1. DynamicWindowEnv å¢åŠ  close æ–¹æ³•ï¼Œä¿®å¤ AttributeErrorã€‚
    2. DataCache ä½¿ç”¨ multiprocessing.Manager å…±äº«å†…å­˜ï¼Œè§£å†³å¤šè¿›ç¨‹é‡å¤è¯»å–å¯¼è‡´çš„ Miss åˆ·å±ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.distributions import Categorical
from windowEnv_parallel_fast import windowEnv
import time, json
import sys
import pandas as pd
from datetime import datetime, timedelta

from tools.Norm import Normalization, RewardNormalization, RewardScaling
from preTrain.preMOE import PreMOE
from dataclasses import dataclass, field
import random
import multiprocessing as mp
from finTool.single_window_account import single_Account  # ç”¨äº DataCache è¯»å–æ•°æ®
import os

import warnings
# å¿½ç•¥æ‰€æœ‰ FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning)

# â€”â€” æ„é€ æ¯ä¸ªæ ·æœ¬çš„æƒé‡æ©ç  â€”â€” #
A_HOLD, A_LONG, A_SHORT, A_CLOSE = 0, 1, 2, 3
WEIGHT_BINS_CPU = torch.tensor([0.00, 0.25, 0.50, 0.75, 1.00])  # ç¦»æ•£æƒé‡

DESK_PATH = 'C:/Users/Flying/Desktop' # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
DESK_PATH = './miniQMT/DL/results'
# DESK_PATH = 'C:/Users/David/Desktop' 

# -----------------------------------------------------------
# å…¨å±€æ•°æ®ç¼“å­˜ (å…±äº«å†…å­˜ç‰ˆ)
# -----------------------------------------------------------
class DataCache:
    """
    å…¨å±€æ•°æ®ç¼“å­˜å·¥å…·ç±»ã€‚
    ä¸å†æŒæœ‰ç±»å˜é‡ï¼Œè€Œæ˜¯æ“ä½œä¼ å…¥çš„ shared_dict (Manager.dict)ã€‚
    """

    @staticmethod
    def clean_ts(ts_series):
        """å‘é‡åŒ–æ—¶é—´æ¸…æ´—"""
        if np.issubdtype(ts_series.dtype, np.datetime64):
            return ts_series.dt.strftime('%Y%m%d%H%M%S').values
        try:
            # å°è¯• Pandas å‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œ
            return ts_series.astype(str).str.replace(' ', '').str.replace('-', '').str.replace(':', '').values
        except:
            # å…œåº•
            return np.array([str(x).replace(' ', '').replace('-', '').replace(':', '') for x in ts_series])

    @classmethod
    def get_data(cls, shared_dict, shared_lock, benchmark, start_time, end_time, init_capital, fee):
        """
        è·å–æ•°æ®: å…ˆæŸ¥å…±äº«å†…å­˜ï¼Œæ²¡æœ‰å†è¯»ç›˜å¹¶å†™å…¥å…±äº«å†…å­˜
        """
        key = f"{benchmark}_{start_time}_{end_time}"
        
        # 1. æŸ¥å…±äº«å­—å…¸ (è¿›ç¨‹å®‰å…¨)
        if key in shared_dict:
            return shared_dict[key]
            
        # 2. æœªå‘½ä¸­ï¼Œè¯»å–æ•°æ®
        # åŠ ä¸Šè¿›ç¨‹åæ–¹ä¾¿è°ƒè¯•
        p_name = mp.current_process().name
        print(f"[DataCache][{p_name}] Miss! Loading {key}...")
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ å¼•å…¥å…±äº«é” ğŸ”¥ğŸ”¥ğŸ”¥
        with shared_lock:
            # å¿…é¡»äºŒæ¬¡æ£€æŸ¥ï¼šåœ¨ç­‰å¾…é”çš„è¿‡ç¨‹ä¸­ï¼Œå…¶ä»–è¿›ç¨‹å¯èƒ½å·²ç»åŠ è½½å¹¶å†™å…¥äº†æ•°æ®
            if key in shared_dict:
                print(f"[DataCache][{p_name}] Secondary Hit! Key {key} already loaded.")
                return shared_dict[key]

            # çœŸæ­£æœªå‘½ä¸­ï¼Œå¼€å§‹è¯»ç›˜
            
            # ä½¿ç”¨ä¸´æ—¶è´¦æˆ·è¯»å–
            temp_acct = single_Account(init_capital, fee, '30m', [benchmark])
            df = temp_acct.real_info_controller.get_bars_between_from_df(benchmark, start_time, end_time)
            
            # è½¬ Numpy
            close_arr = df['close'].values.astype(np.float32)
            ts_arr = cls.clean_ts(df['ts'])
            
            # å°è£…æ•°æ®åŒ…
            data_pack = {
                'close_arr': close_arr,
                'ts_arr': ts_arr,
                'benchmark': benchmark
            }
            
            # 3. å†™å…¥å…±äº«å­—å…¸ (å†™å…¥å®Œæˆåï¼Œé”è‡ªåŠ¨é‡Šæ”¾)
            shared_dict[key] = data_pack
            print(f"[DataCache][{p_name}] Loaded & Shared {len(close_arr)} steps. Lock Released.") 
            
            del temp_acct
            return data_pack

# è¾“å‡ºç±»
class outPut():
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.logfile = open(filename, "w", encoding="utf-8")
    def write(self, message):
        self.terminal.write(message)
        self.logfile.write(message)
    def flush(self):
        self.terminal.flush()
        self.logfile.flush()
    def close(self):
        self.logfile.close()

# å…±äº«å¹²è·¯ + åŒå¤´
class ActorDualHead(nn.Module):
    def __init__(self, state_dim, hidden_dim: int=256, n_actions: int = 4, n_weights: int = 5):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.action_head = nn.Linear(hidden_dim, n_actions)
        self.weight_head = nn.Linear(hidden_dim, n_weights)

    def forward(self, state):
        state = state.to(dtype=torch.float32)
        if state.dim() == 1:
            state = state.unsqueeze(0)
        z = self.backbone(state)
        return self.action_head(z), self.weight_head(z)

class Value(nn.Module):
    def __init__(self, state_dim, hidden_dim: int=256):
        super(Value, self).__init__()
        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    def forward(self, state):
        return self.value_net(state)

# -----------------------------------------------------------
# å¤šè¿›ç¨‹ç¯å¢ƒç›¸å…³ç±»
# -----------------------------------------------------------

class CloudpickleWrapper(object):
    def __init__(self, x):
        self.x = x
    def __getstate__(self):
        import cloudpickle
        return cloudpickle.dumps(self.x)
    def __setstate__(self, ob):
        import pickle
        self.x = pickle.loads(ob)

def worker(remote, parent_remote, env_fn_wrapper):
    parent_remote.close()
    env = env_fn_wrapper.x()
    try:
        while True:
            cmd, data = remote.recv()
            
            if cmd == 'step':
                action, weight = data
                nc, nh, r, term, trunc = env.step(action, weight)
                
                info = {}
                if hasattr(env, 'account_controller'):
                    info['equity'] = env.account_controller.equity
                
                if term or trunc:
                    info['final_equity'] = env.account_controller.equity
                    # æ³¨æ„ï¼šSubprocVectorEnv çš„ worker åœ¨ done åä¼šè‡ªåŠ¨ reset
                    # å¦‚æœä¹‹å‰è°ƒç”¨è¿‡ set_taskï¼Œè¿™é‡Œçš„ reset ä¾ç„¶ä¼šè·‘æŒ‡å®šçš„ç»„åˆ
                    # é™¤éä¸»è¿›ç¨‹å†æ¬¡è°ƒç”¨ set_task åˆ‡æ¢ä»»åŠ¡
                    nc, nh, _ = env.reset()
                
                remote.send((nc, nh, r, term, trunc, info))
            
            elif cmd == 'reset':
                nc, nh, _ = env.reset()
                info = {}
                if hasattr(env, 'account_controller'):
                    info['equity'] = env.account_controller.equity
                remote.send((nc, nh, info))
            
            # ğŸ”¥ [æ–°å¢] è®¾ç½®ä»»åŠ¡ç´¢å¼•
            elif cmd == 'set_task':
                idx = data
                # è°ƒç”¨ DynamicWindowEnv çš„ set_task æ–¹æ³•
                if hasattr(env, 'set_task'):
                    env.set_task(idx)
                remote.send(None) # å‘é€ç¡®è®¤ä¿¡å· (Ack)
            
            elif cmd == 'close':
                env.close()
                remote.close()
                break
            else:
                raise NotImplementedError
    except KeyboardInterrupt:
        print('SubprocEnv worker: got KeyboardInterrupt')
    except Exception as e:
        print(f'SubprocEnv worker error: {e}')
    finally:
        env.close()

class SubprocVectorEnv:
    def __init__(self, env_fns):
        self.waiting = False
        self.closed = False
        self.num_envs = len(env_fns)
        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(self.num_envs)])
        self.ps = []
        
        for work_remote, remote, env_fn in zip(self.work_remotes, self.remotes, env_fns):
            args = (work_remote, remote, CloudpickleWrapper(env_fn))
            p = mp.Process(target=worker, args=args, daemon=True) 
            p.start()
            self.ps.append(p)
            work_remote.close()

    def step(self, actions, weights):
        for remote, action, weight in zip(self.remotes, actions, weights):
            remote.send(('step', (action, weight)))
        results = [remote.recv() for remote in self.remotes]
        currents, histories, rewards, terms, truncs, infos = zip(*results)
        return np.stack(currents), np.stack(histories), np.stack(rewards), np.stack(terms), np.stack(truncs), infos

    def reset(self):
        for remote in self.remotes:
            remote.send(('reset', None))
        results = [remote.recv() for remote in self.remotes]
        currents, histories, infos = zip(*results)
        return np.stack(currents), np.stack(histories), infos

    # ğŸ”¥ [æ–°å¢] ç»™æ¯ä¸ª Worker åˆ†é…ç‰¹å®šçš„ä»»åŠ¡ç´¢å¼•
    def set_tasks(self, task_indices):
        """
        task_indices: list, é•¿åº¦å¿…é¡»ç­‰äº num_envs
        """
        assert len(task_indices) == self.num_envs, "ä»»åŠ¡æ•°å¿…é¡»åŒ¹é… Worker æ•°"
        
        # 1. å‘é€æŒ‡ä»¤
        for remote, idx in zip(self.remotes, task_indices):
            remote.send(('set_task', idx))
        
        # 2. ç­‰å¾…ç¡®è®¤ (åŒæ­¥)
        for remote in self.remotes:
            remote.recv()

    def close(self):
        if self.closed: return
        for remote in self.remotes:
            remote.send(('close', None))
        for p in self.ps:
            p.join()
        self.closed = True

# -----------------------------------------------------------
# åŠ¨æ€ç¯å¢ƒåŒ…è£…å™¨ (ä¿®å¤ Missing Close Method)
# -----------------------------------------------------------
class DynamicWindowEnv:
    def __init__(self, option_pairs, global_cfg, shared_cache, shared_lock, seed=0):
        self.all_pairs = option_pairs
        self.cfg = global_cfg
        self.shared_cache = shared_cache
        self.current_env = None
        # æ–°å¢ï¼šæŒ‡å®šå½“å‰è·‘ç¬¬å‡ ä¸ªç»„åˆ
        self.fixed_idx = None 

        self.shared_lock = shared_lock

    def set_task(self, idx):
        """æŒ‡å®šæ¥ä¸‹æ¥ reset è¦è·‘çš„ç»„åˆç´¢å¼•"""
        self.fixed_idx = idx

    def reset(self):
        if self.current_env is not None:
            self.current_env.close()
            self.current_env = None

        # æ ¸å¿ƒä¿®æ”¹ï¼šå¦‚æœæœ‰æŒ‡å®šä»»åŠ¡ï¼Œå°±è·‘æŒ‡å®šçš„ï¼›å¦åˆ™éšæœºï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
        if self.fixed_idx is not None:
            # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            idx = self.fixed_idx % len(self.all_pairs)
            pair_info = self.all_pairs[idx]
        else:
            pair_info = random.choice(self.all_pairs)
            
        t_start = pair_info.get('start_time', self.cfg.start_time)
        t_end = pair_info.get('end_time', self.cfg.end_time)
        
        preloaded_data = DataCache.get_data(
            self.shared_cache, self.shared_lock, '510050', t_start, t_end,
            self.cfg.init_capital, self.cfg.fee
        )
        
        self.current_env = windowEnv(
            init_capital=self.cfg.init_capital,
            call=pair_info['call'], put=pair_info['put'], fee=self.cfg.fee,
            start_time=t_start, end_time=t_end, benchmark='510050',
            timesteps=self.cfg.max_timesteps, preloaded_data=preloaded_data
        )
        return self.current_env.reset()
    
    # ... å…¶ä»–æ–¹æ³• (step, closeç­‰) ä¿æŒä¸å˜ ...
    def step(self, action, weight):
        if self.current_env is None: return self.reset()
        return self.current_env.step(action, weight)
    def close(self):
        if self.current_env: self.current_env.close()
    def get_raw_shape(self): # ...ä¸å˜
        if self.current_env is None: self.reset()
        return self.current_env.get_raw_shape()
    @property
    def account_controller(self): return self.current_env.account_controller
# -----------------------------------------------------------
# ç‰¹å¾é€‚é…å™¨
# -----------------------------------------------------------
class ViewProjector(nn.Module):
    def __init__(self, high_dim, low_dim, out_dim=64):
        super().__init__()
        self.high_net = nn.Sequential(
            nn.LayerNorm(high_dim),
            nn.Linear(high_dim, out_dim), 
        )
        self.low_net = nn.Sequential(
            nn.LayerNorm(low_dim),
            nn.Linear(low_dim, 32),       
            nn.GELU(),
        )
        self.fusion = nn.Sequential(
            nn.Linear(out_dim + 32, out_dim),
            nn.LayerNorm(out_dim)
        )
        
    def forward(self, x_high, x_low):
        h = self.high_net(x_high)
        l = self.low_net(x_low)
        return self.fusion(torch.cat([h, l], dim=-1))

class MultiViewAdapter(nn.Module):
    def __init__(self, dims_dict: dict, final_dim: int = 128):
        super().__init__()
        view_dim = 48
        self.varma_proj = ViewProjector(dims_dict['varma_high'], dims_dict['varma_low'], out_dim=view_dim)
        self.basis_proj = ViewProjector(dims_dict['basis_high'], dims_dict['basis_low'], out_dim=view_dim)
        self.itrans_proj = ViewProjector(dims_dict['itrans_high'], dims_dict['itrans_low'], out_dim=view_dim)
        self.router_proj = nn.Sequential(
            nn.LayerNorm(dims_dict['router']),
            nn.Linear(dims_dict['router'], 32)
        )
        self.final_net = nn.Sequential(
            nn.Linear(view_dim * 3 + 32, final_dim),
            nn.LayerNorm(final_dim)
        )
        
    def raw_forward(self, inputs: dict):
        v_varma = self.varma_proj(inputs['varma_h'], inputs['varma_l'])
        v_basis = self.basis_proj(inputs['basis_h'], inputs['basis_l'])
        v_itrans = self.itrans_proj(inputs['itrans_h'], inputs['itrans_l'])
        v_router = self.router_proj(inputs['router'])
        combined = torch.cat([v_varma, v_basis, v_itrans, v_router], dim=-1)
        return self.final_net(combined)
    
    def forward(self, inputs: dict, train: bool=True):
        if train: return self.raw_forward(inputs)
        with torch.no_grad(): return self.raw_forward(inputs)

# -----------------------------------------------------------
# PPO Agent ç±»
# -----------------------------------------------------------

class weightPPO:
    def __init__(self, action_dim: int, actor_lr: float=3e-4, value_lr: float=5e-4, 
                 gamma: float=0.99, clip_eps: float=0.1, k_epochs: int=5, 
                 device: str='cpu', check_path: str='./miniQMT/DL/checkout',
                 window_size: int=32, pre_len: int=4):
        
        self.device = device
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.k_epochs = k_epochs
        self.check_path = f'{check_path}/check_data_all.pt'
        self.actor_lr, self.value_lr = actor_lr, value_lr
        self.action_dim = action_dim
        self.window_size = window_size
        self.WEIGHT_BINS = WEIGHT_BINS_CPU.to(self.device)

        self.actor, self.value = None, None
        self.opt_a, self.opt_b = None, None
        
        self.pre_len = pre_len
        self.extractor = PreMOE(seq_len=self.window_size, pred_len=self.pre_len, n_variates=13, d_router=128).to(self.device)
        self.load_moe_parameters()

        # å†»ç»“ extractor å‚æ•°
        for p in self.extractor.parameters():
            p.requires_grad = False
        self.extractor.eval()
        
        self.feature_adapter = None
        self.opt_c = None
        self.state_norm = None
        self.reward_norm = None
        self.reward_norm_list = []

        print(f"[Info] Device: {self.device} | Precision: Float32 (AMP Removed)")

    def load_moe_parameters(self):
        try:
            SAVE_PATH = f'./miniQMT/DL/preTrain/weights/preMOE_best_dummy_data_{self.window_size}_{self.pre_len}.pth'
            state_dict = torch.load(SAVE_PATH, map_location=self.device)
            self.extractor.load_state_dict(state_dict)
            print(f"[Info] åŠ è½½MOEå‚æ•°æˆåŠŸ, device = {self.device}")
        except Exception as e:
            print(f"[Info] åŠ è½½MOEå‚æ•°å¤±è´¥, e = {e}")

    def init_norm_state(self, x: torch.Tensor):
        self.state_norm = Normalization(x.shape[1:]) 
    
    def init_norm_reward(self):
        self.reward_norm = RewardScaling(shape=(1,), gamma=self.gamma)

    def init_norm_reward_list(self, length: int):
        self.reward_norm_list = []
        for _ in range(length):
            self.reward_norm_list.append(RewardScaling(shape=(1,), gamma=self.gamma))
    
    def load_infer_parameters(self, check_path: str=None, device: str=None):
        device = device if device else self.device
        path = check_path if check_path else self.check_path
        data = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(data['actor_state'])
        self.value.load_state_dict(data['value_state'])
        self.feature_adapter.load_state_dict(data['features_adapter_state'])
        print(f"[Info: æ¨ç†é˜¶æ®µ] åŠ è½½ç½‘ç»œæƒé‡å®Œæˆ~")

    def exe_reward_norm(self, x: float):
        if self.reward_norm is None: 
            self.init_norm_reward()
        x = torch.tensor([x], dtype=torch.float32)
        return self.reward_norm(x).item()

    def extract_features_batch(self, current_state: torch.Tensor, history_state: torch.Tensor, cal_dim: bool = False):
        if current_state.device != torch.device(self.device): 
            current_state = current_state.to(self.device)
        if history_state.device != torch.device(self.device): 
            history_state = history_state.to(self.device)

        call_state, put_state = torch.chunk(history_state, chunks=2, dim=2)
        call_dict = self.extractor.encode_tokens(call_state)
        put_dict = self.extractor.encode_tokens(put_state)

        dims_dict = {
            'varma_high': call_dict['varma_h'].shape[-1],
            'varma_low': call_dict['varma_l'].shape[-1],
            'basis_high': call_dict['basis_h'].shape[-1],
            'basis_low': call_dict['basis_l'].shape[-1],
            'itrans_high': call_dict['itrans_h'].shape[-1],
            'itrans_low': call_dict['itrans_l'].shape[-1],
            'router': call_dict['router'].shape[-1]
        }

        if self.feature_adapter is None:
            self.feature_adapter = MultiViewAdapter(dims_dict, final_dim=128).to(self.device)
            self.opt_c = optim.Adam(self.feature_adapter.parameters(), lr=self.actor_lr)

        train = not cal_dim
        reduce_call = self.feature_adapter(call_dict, train=train)
        reduce_put = self.feature_adapter(put_dict, train=train)
        features = torch.cat([current_state, reduce_call, reduce_put], dim=-1).to(self.device)

        if cal_dim:
            return features

        if self.state_norm is None:
            self.init_norm_state(features)
        return self.state_norm(features, update=train)

    def set_actor_and_value(self, state_dim: int):
        self.actor = ActorDualHead(state_dim, n_actions=self.action_dim).to(self.device)
        self.value = Value(state_dim).to(self.device)
        self.opt_a = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.opt_b = optim.Adam(self.value.parameters(), lr=self.value_lr)

    @torch.no_grad()
    def selete_action_and_weight(self, state, test: bool=False):
        logits_a, logits_w = self.actor(state)
        dist_a = Categorical(logits=logits_a)
        a = torch.argmax(logits_a, dim=-1) if test else dist_a.sample()
        logp_a = dist_a.log_prob(a)

        K = a.shape[0]
        allowed = torch.zeros(K, 5, dtype=torch.bool, device=self.device)
        mask_ls = (a == A_LONG) | (a == A_SHORT) | (a == A_CLOSE)
        allowed[mask_ls, 1:] = True
        allowed[~mask_ls, 0] = True
        
        masked_logits_w = logits_w.clone()
        masked_logits_w[~allowed] = -1e9 
        
        dist_w = Categorical(logits=masked_logits_w)
        w_idx = torch.argmax(masked_logits_w, dim=-1) if test else dist_w.sample()
        logp_w = dist_w.log_prob(w_idx)
        w_val = self.WEIGHT_BINS[w_idx]

        need_w = ((a == A_LONG) | (a == A_SHORT) | (a == A_CLOSE)).float()
        logp_joint = logp_a + need_w * logp_w

        return a, w_idx, w_val, logp_a, logp_w, logp_joint
    
    def update_parallel(self, traces, target_kl=0.015, entropy_coef=0.01, value_coef=0.5):
        raw_curr = torch.stack(traces['raw_curr']).to(self.device)
        raw_hist = torch.stack(traces['raw_hist']).to(self.device)
        
        actions = torch.stack(traces['actions']).to(self.device)
        w_idx = torch.stack(traces['weight_idx']).to(self.device)
        old_logp = torch.stack(traces['logp_joint']).to(self.device)
        rewards = torch.stack(traces['rewards']).to(self.device)
        next_raw_curr = traces['next_raw_curr'].to(self.device)
        next_raw_hist = traces['next_raw_hist'].to(self.device)
        
        dones = torch.stack(traces['terminated']).to(self.device) | torch.stack(traces['truncated']).to(self.device)
        
        T, K, Dc = raw_curr.shape
        Dh = raw_hist.shape[-1]

        # --- 1. è®¡ç®— GAE (no_grad) ---
        with torch.no_grad():
            curr_flat = raw_curr.view(T*K, -1)
            hist_flat = raw_hist.view(T*K, -1, Dh)
            feat_tk = self.extract_features_batch(curr_flat, hist_flat)
            v_tk = self.value(feat_tk).view(T, K)
            
            next_feat = self.extract_features_batch(next_raw_curr, next_raw_hist)
            v_next = self.value(next_feat).squeeze(-1)
        
            v_tk_cpu = v_tk.float()
            v_next_cpu = v_next.float()
            rew_cpu = rewards.float()
            dones_cpu = dones.float()
            
            adv = torch.zeros_like(rew_cpu)
            last_gae = 0
            for t in reversed(range(T)):
                m = 1.0 - dones_cpu[t]
                v_tp1 = v_next_cpu if t == T-1 else v_tk_cpu[t+1]
                delta = rew_cpu[t] + self.gamma * v_tp1 * m - v_tk_cpu[t]
                last_gae = delta + self.gamma * 0.95 * m * last_gae
                adv[t] = last_gae
            
            returns = adv + v_tk_cpu
        
        curr_flat = raw_curr.view(T*K, -1)
        hist_flat = raw_hist.view(T*K, -1, Dh)
        a_flat = actions.view(-1)
        w_flat = w_idx.view(-1)
        old_logp_flat = old_logp.view(-1)
        adv_flat = adv.view(-1).to(self.device)
        ret_flat = returns.view(-1).to(self.device)
        
        adv_flat = (adv_flat - adv_flat.mean()) / (adv_flat.std() + 1e-8)

        # --- 2. è®­ç»ƒå¾ªç¯ ---
        for i in range(self.k_epochs):
            s_flat = self.extract_features_batch(curr_flat, hist_flat)
            logits_a, logits_w = self.actor(s_flat)
            
            dist_a = Categorical(logits=logits_a)
            new_logp_a = dist_a.log_prob(a_flat)
            ent_a = dist_a.entropy().mean()
            
            need_w = ((a_flat == A_LONG) | (a_flat == A_SHORT) | (a_flat == A_CLOSE)).float()
            
            lw = logits_w.clone()
            mask = torch.zeros_like(lw, dtype=torch.bool)
            mask[need_w.bool(), 1:] = True
            mask[~need_w.bool(), 0] = True
            lw[~mask] = -1e9
            
            dist_w = Categorical(logits=lw)
            new_logp_w = dist_w.log_prob(w_flat)
            ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)
            
            logp_new = new_logp_a + need_w * new_logp_w
            ratio = torch.exp(logp_new - old_logp_flat)
            
            surr1 = ratio * adv_flat
            surr2 = torch.clamp(ratio, 1-self.clip_eps, 1+self.clip_eps) * adv_flat
            loss_a = -torch.min(surr1, surr2).mean()
            
            v_pred = self.value(s_flat).squeeze(-1)
            loss_v = F.mse_loss(v_pred, ret_flat)
            loss = loss_a + value_coef * loss_v - entropy_coef * (ent_a + 0.5 * ent_w)

            self.opt_a.zero_grad()
            self.opt_b.zero_grad()
            if self.opt_c: self.opt_c.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.opt_a.step()
            self.opt_b.step()
            if self.opt_c: self.opt_c.step()

            last_actor_loss = loss_a.item()
            last_value_loss = loss_v.item()
            last_entropy = (ent_a + 0.5 * ent_w).item()

            kl = (old_logp_flat - logp_new).mean().abs()
            if kl > 1.5 * target_kl:
                print(f"Early stop at epoch {i} KL={kl.item():.4f}")
                break
    
        return loss.item(), kl.item(), last_actor_loss, last_value_loss, last_entropy
    
    def save(self, epoch: int = None, best_reward: float = None, path: str = None):
            save_path = path or self.check_path
            data = {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "actor_state": self.actor.state_dict(),
                "value_state": self.value.state_dict(),
                "opt_a_state": self.opt_a.state_dict(),
                "opt_b_state": self.opt_b.state_dict(),
                "features_adapter_state": self.feature_adapter.state_dict(),
                "h_params": {
                    "gamma": self.gamma,
                    "clip_eps": self.clip_eps,
                    "k_epochs": self.k_epochs,
                    "device": self.device,
                },
                "epoch": epoch,
                "best_reward": best_reward.item() if hasattr(best_reward, 'item') else best_reward,
                "state_norm": self.state_norm,
            }
            torch.save(data, save_path)
            print(f"[PPO] checkpoint saved to: {save_path}")
        
    def load_checkpoint(self, path: str=None):
        if path is None:
            path = self.check_path
        if not os.path.exists(path):
            print(f"[Warn] Checkpoint not found at {path}")
            return None, None
        
        print(f"[Resume] Loading checkpoint from {path}...")
        # è¿™é‡Œçš„ map_location éå¸¸é‡è¦ï¼Œé˜²æ­¢è·¨è®¾å¤‡åŠ è½½æŠ¥é”™
        data = torch.load(path, map_location=self.device)
        
        # 1. åŠ è½½ç½‘ç»œæƒé‡
        self.actor.load_state_dict(data['actor_state'])
        self.value.load_state_dict(data['value_state'])
        self.feature_adapter.load_state_dict(data['features_adapter_state'])
        
        # 2. åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if self.opt_a: self.opt_a.load_state_dict(data['opt_a_state'])
        if self.opt_b: self.opt_b.load_state_dict(data['opt_b_state'])
        if self.opt_c and 'features_adapter_opt_state' in data:
            st = data['features_adapter_opt_state']
            if st is not None: self.opt_c.load_state_dict(st)

        # 3. [æ–°å¢] åŠ è½½ Normalization çŠ¶æ€
        if 'state_norm' in data:
            # ç›´æ¥è¦†ç›–å½“å‰çš„ self.state_norm
            self.state_norm = data['state_norm']
            print(f"[Resume] State Norm loaded. (count={self.state_norm.running_ms.n if hasattr(self.state_norm.running_ms, 'n') else '?'})")
        else:
            print("[Resume] Warning: No state_norm in checkpoint! Training might be unstable.")

        # 4. [æ–°å¢] åŠ è½½ Reward Norm List (å¯é€‰)
        if 'reward_norm_list' in data:
            self.reward_norm_list = data['reward_norm_list']

        epoch = data.get('epoch', 0)
        best_reward = data.get('best_reward', -float('inf'))
        
        print(f"[Resume] Success! Resuming from Epoch {epoch + 1}, Best Reward: {best_reward:.4f}")
        return epoch, best_reward


# -----------------------------------------------------------
# ä¸» Agent ç±»
# -----------------------------------------------------------

@dataclass
class AgentConfig:
    action_dim: int
    option_pairs: list
    max_epochs: int=300
    max_timesteps: int=1000
    device: str='cuda' if torch.cuda.is_available() else 'cpu'
    print_interval: int=1
    
    # å…¨å±€é»˜è®¤æ—¶é—´ (å¦‚æœ option_pairs é‡Œæ²¡æŒ‡å®šåˆ™ç”¨è¿™ä¸ª)
    start_time: str='20250408100000'
    end_time: str='20250924150000'

    fee: float=1.3
    init_capital: float=100000.0
    mode: str='train'
    
    # è‡ªåŠ¨è®¡ç®— Worker æ•°é‡
    num_workers: int = field(default_factory=lambda: min(mp.cpu_count() - 2, 12)) 

class Agent:
    def __init__(self, config: AgentConfig):
        self.cfg = config
        self.device = config.device
        self.env = None
        self.env_fns = []

        if config.mode == 'train':
            self.init_train()
        
        self.records = {
            'epoch': [], 'reward': [], 'avg_equity': [], 'loss': [], 'kl': [],
            'hold_ratio': [], 'long_ratio': [], 'short_ratio': [], 'close_ratio': [],
            'actor_loss': [], 'value_loss': [], 'entropy': [],
            'ratio_0': [], 'ratio_25': [], 'ratio_50': [], 'ratio_75': [], 'ratio_100': [],
        }

        # å¹¶è¡Œä¸ªæ•°
        self.num_workers = None

    # è®­ç»ƒæ¨¡å¼åˆå§‹åŒ–
    def init_train(self):
        print(f'[Agent-init-train] æœŸæƒç»„åˆæ•°é‡ = {len(self.cfg.option_pairs)}')
        config = self.cfg
        self.ppo = weightPPO(config.action_dim, device=self.device)
        
        # 1. å¯åŠ¨ç®¡ç†å™¨ (å¿…é¡»åœ¨ä¸»è¿›ç¨‹)
        self.manager = mp.Manager()
        self.shared_cache = self.manager.dict() # åˆ›å»ºè·¨è¿›ç¨‹å…±äº«å­—å…¸
        self.share_lock = self.manager.Lock()
        
        # 2. ç¡®å®š Worker æ•°é‡
        num_workers = min(len(config.option_pairs), config.num_workers)
        if num_workers < 1: 
            num_workers = 1
        self.num_workers = num_workers
        print(f"[Init] Detect {mp.cpu_count()} CPUs, Launching {num_workers} workers.")

        # 3. æ„é€ ç¯å¢ƒç”Ÿæˆå™¨ (å…³é”®ä¿®æ”¹ï¼ï¼ï¼)
        self.env_fns = []
        
        # --- æ ¸å¿ƒä¿®å¤å¼€å§‹ ---
        # æå–ä¸ºå±€éƒ¨å˜é‡ï¼Œåˆ‡æ–­ä¸ self çš„è”ç³»
        # è¿™æ · pickle å°±åªä¼šæ‰“åŒ…è¿™å‡ ä¸ªå¯¹è±¡ï¼Œè€Œä¸ä¼šæ‰“åŒ…åŒ…å« self.manager çš„ Agent
        _cache = self.shared_cache
        _pairs = config.option_pairs
        _cfg = config 
        _lock = self.share_lock
        
        for i in range(num_workers):
            # ä½¿ç”¨é»˜è®¤å‚æ•°ç»‘å®š (seed=i, cache=_cache, ...) 
            # è¿™æ ·å‡½æ•°ä½“å†…éƒ¨å°±ä¸éœ€è¦å¼•ç”¨å¤–éƒ¨ä½œç”¨åŸŸçš„ self äº†
            def make_env(seed=i, cache=_cache, lock=_lock, pairs=_pairs, cfg=_cfg):
                return DynamicWindowEnv(pairs, cfg, cache, lock, seed=seed)
            
            self.env_fns.append(make_env)
        # --- æ ¸å¿ƒä¿®å¤ç»“æŸ ---
        
        # 4. é¢„çƒ­ç½‘ç»œ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸´æ—¶å®ä¾‹åŒ–ä¸€ä¸ª env æ¥æ‹¿ shapeï¼Œç”¨å®Œå³æ¯
        # è¿™é‡Œå¯ä»¥ç›´æ¥ç”¨å±€éƒ¨å˜é‡åˆå§‹åŒ–ï¼Œé¿å…è°ƒç”¨ self.env_fns[0] å¯¼è‡´ä¸å¿…è¦çš„å¤æ‚æ€§
        # æˆ–è€…ä¾ç„¶ç”¨ self.env_fns[0] ä¹Ÿå¯ä»¥ï¼Œå› ä¸ºç°åœ¨ make_env å·²ç»æ˜¯å¹²å‡€çš„äº†
        dummy_env = self.env_fns[0]() 
        c, h, _ = dummy_env.reset()
        dummy_env.close()
        
        c_b = torch.tensor([c], dtype=torch.float32, device=self.device)
        h_b = torch.tensor([h], dtype=torch.float32, device=self.device)
        feat = self.ppo.extract_features_batch(c_b, h_b, cal_dim=True)
        self.ppo.set_actor_and_value(feat.shape[-1])



    # æµ‹è¯•æˆªæ–­è®¾ç½®
    def set_env(self, env: windowEnv):
        self.env = env
        current_shape, history_shape = self.env.get_raw_shape()
        current_state = torch.zeros(current_shape)
        history_state = torch.zeros(history_shape)
        if self.ppo is None:
            self.ppo = weightPPO(self.action_dim, window_size=self.window_size, device=self.device)
            results = self.ppo.extract_features(current_state, history_state, cal_dim=True)
            _, state_dim = results.shape
            self.ppo.set_actor_and_value(state_dim)

    def set_norm(self, state_norm: Normalization):
        self.ppo.state_norm = state_norm
        print(f"[Info] Normè®¾ç½®å®Œæˆ | state.n = {self.ppo.state_norm.running_ms.n}")

    # åŠ¨æ€å¹¶è¡Œè®­ç»ƒå‡½æ•° (å…¨é‡è¦†ç›–ç‰ˆ)
    def train_parallel_modified_early_stop(self, from_check_point: bool=False):
        # 1. åˆå§‹åŒ–å¹¶è¡Œç¯å¢ƒ
        vec_env = SubprocVectorEnv(self.env_fns)
        print(f"[Train] Start Full-Coverage training on {self.device}...")
        
        best_reward = -float('inf')
        patience = getattr(self.cfg, 'patience', 30)
        stop_entropy = getattr(self.cfg, 'stop_entropy', 0.6)
        min_delta = 0.001
        early_stop_counter = 0

        # è·å–ä»»åŠ¡æ€»é‡ä¿¡æ¯
        total_pairs = len(self.cfg.option_pairs)
        num_workers = len(self.env_fns)


        if from_check_point:
            start_epoch, best_reward = self.ppo.load_checkpoint()

        for epoch in range(self.cfg.max_epochs):
            if from_check_point and start_epoch is not None:
                if epoch < start_epoch:
                    print(f'[Skip] epoch = {epoch}')
                    continue

            print(f'epoch = {epoch}')
            start_time = time.time()
            
            # --- å¤§å®¹å™¨ï¼šç”¨äºæ”¶é›†è¿™ä¸€è½® Epoch æ‰€æœ‰ç»„åˆçš„æ•°æ® ---
            # ç»“æ„ï¼škey -> list of (Tä¸ªæ—¶é—´æ­¥) -> list of (Batchæ‰¹æ¬¡)
            all_traces = {
                'raw_curr': [], 'raw_hist': [],
                'actions': [], 'weight_idx': [], 'logp_joint': [], 
                'rewards': [], 
                'terminated': [], 'truncated': [],
                'next_raw_curr': [], 'next_raw_hist': [] 
            }
            
            # ç»Ÿè®¡å˜é‡
            epoch_rewards_sum = 0.0
            epoch_equity_sum = 0.0
            total_steps_collected = 0
            
            # ==========================================
            # [æ ¸å¿ƒé€»è¾‘] åˆ†æ‰¹æ¬¡è·‘å®Œæ‰€æœ‰ç»„åˆ (Chunk Loop)
            # ==========================================
            # æ¯æ¬¡ stride = num_workers
            for i in range(0, total_pairs, num_workers):
                # 1. ç¡®å®šå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡ç´¢å¼•
                # ä¾‹å¦‚æ€»å…±93ä¸ªï¼ŒWorker=12ï¼Œåˆ™ indices=[0..11], [12..23]...
                indices = list(range(i, min(i + num_workers, total_pairs)))
                print(f'Start to collected pairs {indices[0]}-{indices[-1]}')
                valid_count = len(indices) # è¿™ä¸€æ‰¹å®é™…æœ‰æ•ˆçš„ä»»åŠ¡æ•°
                
                # å¦‚æœæœ€åä¸€æ‰¹ä¸è¶³ num_workers ä¸ª (ä¾‹å¦‚å‰© 5 ä¸ª)ï¼Œ
                # åé¢çš„ Worker ä¹Ÿè¦å¹²æ´»(é˜²æ­¢æ­»é”)ï¼Œå¯ä»¥éšæœºå¡«å……ä»»åŠ¡ï¼Œä½†æ•°æ®åé¢ä¼šä¸¢å¼ƒ
                run_indices = indices.copy()
                while len(run_indices) < num_workers:
                    run_indices.append(random.randint(0, total_pairs - 1))
                
                # 2. ä¸‹å‘ä»»åŠ¡
                vec_env.set_tasks(run_indices)
                
                # 3. åˆå§‹åŒ–è¿™ä¸€æ‰¹çš„ RewardScaler
                self.ppo.init_norm_reward_list(length=num_workers)

                # 4. Reset ç¯å¢ƒ
                curr_np, hist_np, infos = vec_env.reset()
                
                # æ‰¹æ¬¡ä¸´æ—¶å®¹å™¨ (ä¸å« next_state)
                batch_keys = ['raw_curr', 'raw_hist', 'actions', 'weight_idx', 'logp_joint', 'rewards', 'terminated', 'truncated']
                batch_data = {k: [] for k in batch_keys}
                
                batch_rewards_raw = [] # ç”¨äºè®¡ç®— log reward
                batch_equities = [0.0] * num_workers
                
                # --- é‡‡é›†å¾ªç¯ (Step Loop) ---
                for t in range(self.cfg.max_timesteps):
                    c_tensor = torch.as_tensor(curr_np, dtype=torch.float32, device=self.device)
                    h_tensor = torch.as_tensor(hist_np, dtype=torch.float32, device=self.device)
                    
                    with torch.no_grad():
                        state = self.ppo.extract_features_batch(c_tensor, h_tensor)
                        a, w_idx, w_val, _, _, logp_joint = self.ppo.selete_action_and_weight(state)
                    
                    actions_np = a.cpu().numpy()
                    weights_np = w_val.cpu().numpy()
                    
                    next_curr, next_hist, rews, terms, truncs, infos = vec_env.step(actions_np, weights_np)
                    
                    # å½’ä¸€åŒ–å¥–åŠ±
                    scaled_rewards = []
                    for k in range(num_workers):
                        r = rews[k].item()
                        r_norm = self.ppo.reward_norm_list[k]
                        scaled_rewards.append(r_norm(r))
                    
                    # è®°å½• Equity (åªè®°å½• valid_count å†…çš„)
                    for k in range(valid_count):
                         info = infos[k]
                         if isinstance(info, dict):
                            key = 'final_equity' if (terms[k] or truncs[k]) else 'equity'
                            if key in info: batch_equities[k] = info[key]

                    # è®°å½• Traces
                    if t > 0:
                        batch_data['rewards'].append(torch.as_tensor(scaled_rewards, dtype=torch.float32, device=self.device))
                        batch_rewards_raw.append(rews)

                    batch_data['raw_curr'].append(c_tensor)
                    batch_data['raw_hist'].append(h_tensor)
                    batch_data['actions'].append(a)
                    batch_data['weight_idx'].append(w_idx)
                    batch_data['logp_joint'].append(logp_joint)
                    batch_data['terminated'].append(torch.as_tensor(terms, device=self.device))
                    batch_data['truncated'].append(torch.as_tensor(truncs, device=self.device)) # ä¿®æ­£ä¹‹å‰çš„ key error

                    curr_np, hist_np = next_curr, next_hist
                
                # --- é‡‡é›†ç»“æŸï¼ŒSoft End è¡¥é½ ---
                hold_actions = np.zeros(num_workers, dtype=int)
                hold_weights = np.zeros(num_workers, dtype=float)
                _, _, final_rews, _, _, _ = vec_env.step(hold_actions, hold_weights)
                
                batch_data['rewards'].append(torch.as_tensor(final_rews, dtype=torch.float32, device=self.device))
                batch_rewards_raw.append(final_rews)
                
                # --- å°†æœ¬æ‰¹æ¬¡çš„ Valid æ•°æ®åˆå¹¶åˆ° all_traces ---
                # batch_data[key] æ˜¯ä¸€ä¸ª listï¼Œé•¿åº¦ Tã€‚å…ƒç´ æ˜¯ Tensor (Num_Workers, ...)
                # æˆ‘ä»¬éœ€è¦åˆ‡ç‰‡å–å‰ valid_count ä¸ª workerï¼Œå¹¶æ”¾å…¥ all_traces å¯¹åº”çš„æ—¶åˆ»åˆ—è¡¨ä¸­
                
                for key in batch_keys:
                    # ç¡®ä¿ all_traces[key] æœ‰è¶³å¤Ÿçš„ç©ºé—´ (å³ list of T lists)
                    if len(all_traces[key]) == 0:
                         all_traces[key] = [[] for _ in range(len(batch_data[key]))]
                    
                    for t_idx, tensor in enumerate(batch_data[key]):
                        # tensor shape: (Num_Workers, ...) -> åˆ‡ç‰‡ -> (Valid_Count, ...)
                        valid_part = tensor[:valid_count] 
                        all_traces[key][t_idx].append(valid_part)

                # å¤„ç† Next State (åªæœ‰ 1 ä¸ªæ—¶é—´æ­¥)
                next_c_valid = torch.as_tensor(curr_np[:valid_count], dtype=torch.float32, device=self.device)
                next_h_valid = torch.as_tensor(hist_np[:valid_count], dtype=torch.float32, device=self.device)
                all_traces['next_raw_curr'].append(next_c_valid)
                all_traces['next_raw_hist'].append(next_h_valid)
                
                # ç»Ÿè®¡
                # batch_rewards_raw æ˜¯ list of numpy (Worker,), å †å åæ±‚å’Œ valid éƒ¨åˆ†
                stacked_rewards = np.stack(batch_rewards_raw) # Shape: (T+1, Num_Workers)
                raw_rew_sum = np.sum(stacked_rewards[:, :valid_count])
                # raw_rew_sum = np.sum(np.concatenate(batch_rewards_raw)[:, :valid_count])
                epoch_rewards_sum += raw_rew_sum
                epoch_equity_sum += sum(batch_equities[:valid_count])
                total_steps_collected += self.cfg.max_timesteps * valid_count
                
                print(f"  > Batch {i//num_workers + 1}: Collected pairs {indices[0]}-{indices[-1]}")

            # ==========================================
            # [æ•°æ®æ•´ç†] æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®
            # ==========================================
            # æ­¤æ—¶ all_traces['raw_curr'][t] æ˜¯ä¸€ä¸ª listï¼ŒåŒ…å«äº†æ‰€æœ‰ batch åœ¨æ—¶åˆ» t çš„ tensor
            # æˆ‘ä»¬éœ€è¦æŠŠå®ƒ cat æˆä¸€ä¸ªå¤§ tensor (Total_Pairs, ...)
            
            final_traces = {}
            # å¤„ç†æ—¶é—´æ­¥åºåˆ—æ•°æ®
            keys_seq = ['raw_curr', 'raw_hist', 'actions', 'weight_idx', 'logp_joint', 'rewards', 'terminated', 'truncated']
            for key in keys_seq:
                final_traces[key] = []
                # éå†æ—¶é—´æ­¥ T
                for t_list in all_traces[key]:
                    # t_list æ˜¯ [Tensor(Batch1), Tensor(Batch2)...]
                    combined = torch.cat(t_list, dim=0) # -> Tensor(Total_Pairs, ...)
                    final_traces[key].append(combined)
            
            # å¤„ç† Next State (ç›´æ¥ Cat)
            final_traces['next_raw_curr'] = torch.cat(all_traces['next_raw_curr'], dim=0)
            final_traces['next_raw_hist'] = torch.cat(all_traces['next_raw_hist'], dim=0)
            
            # --- ç»Ÿä¸€æ›´æ–° ---
            print(f"[Epoch {epoch+1}] Updating on {total_steps_collected} steps (Coverage: {total_pairs} pairs)...")
            loss, kl, actor_loss, value_loss, entropy = self.ppo.update_parallel(final_traces)
            
            # --- Log & Excel ---
            end_time = time.time()
            # è¿™é‡Œçš„ FPS è®¡ç®—çš„æ˜¯æ¯ç§’é‡‡é›†å¤šå°‘æ­¥æœ‰æ•ˆæ•°æ®
            fps = total_steps_collected / (end_time - start_time + 1e-8)
            
            avg_rew = epoch_rewards_sum / (total_steps_collected + 1e-8) 
            avg_equity = epoch_equity_sum / total_pairs
            
            # åŠ¨ä½œç»Ÿè®¡ (åªç»Ÿè®¡æœ€åä¸€æ¬¡ batch çš„åŠ¨ä½œåˆ†å¸ƒä½œä¸ºå‚è€ƒï¼Œæˆ–è€…ç»Ÿè®¡å…¨éƒ¨å¤ªæ…¢)
            # ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œåªç»Ÿè®¡ all_traces['actions'] çš„ä¸€éƒ¨åˆ†é‡‡æ ·ï¼Œæˆ–è€…å…¨éƒ¨
            # è¿™é‡Œç®€å•ç»Ÿè®¡å…¨éƒ¨ (åœ¨ CPU ä¸Šåš)
            with torch.no_grad():
                # å±•å¹³æ‰€æœ‰åŠ¨ä½œ
                flat_actions = torch.cat(final_traces['actions']).cpu().numpy().flatten()
                counts = np.bincount(flat_actions, minlength=4)
                ratios = counts / (len(flat_actions) + 1e-8)

                flat_weights = torch.cat(final_traces['weight_idx']).cpu().numpy().flatten()
                w_counts = np.bincount(flat_weights, minlength=5)
                w_ratios = w_counts / (len(flat_weights) + 1e-8)

            self.records['epoch'].append(epoch + 1)
            self.records['reward'].append(avg_rew)
            self.records['avg_equity'].append(avg_equity)
            self.records['loss'].append(loss)
            self.records['kl'].append(kl)
            self.records['actor_loss'].append(actor_loss)
            self.records['value_loss'].append(value_loss)
            self.records['entropy'].append(entropy)
            
            self.records['hold_ratio'].append(ratios[0])
            self.records['long_ratio'].append(ratios[1])
            self.records['short_ratio'].append(ratios[2])
            self.records['close_ratio'].append(ratios[3])
            
            names = ['ratio_0', 'ratio_25', 'ratio_50', 'ratio_75', 'ratio_100']
            for k in range(len(names)):
                val = w_ratios[k] if k < len(w_ratios) else 0.0
                self.records[names[k]].append(val)

            df = pd.DataFrame(self.records)
            excel_path = f'{DESK_PATH}/PPO_training_data.xlsx'
            try:
                df.to_excel(excel_path, index=False)
            except Exception as e:
                print(f"[Warning] Save Excel failed: {e}")

            if (epoch + 1) % self.cfg.print_interval == 0:
                print(f"[Epoch {epoch+1}/{self.cfg.max_epochs}] "
                      f"Rew: {avg_rew:.4f} | "
                      f"Val: {avg_equity:.0f} | "
                      f"Act: H{ratios[0]:.2f}/L{ratios[1]:.2f}/S{ratios[2]:.2f}/C{ratios[3]:.2f} | "
                      f"Ent: {entropy:.2f} | "
                      f"KL: {kl:.4f} | "
                      f"FPS: {fps:.0f}")
            
            # --- æ—©åœåˆ¤æ–­ ---
            if avg_rew > best_reward + min_delta:
                best_reward = avg_rew
                early_stop_counter = 0 
                self.ppo.save(epoch, best_reward) 
                print(f"   >>> ğŸŒŸ Best Reward Updated: {best_reward:.4f} (Counter Reset)")
            else:
                early_stop_counter += 1
                print(f"   â³ [Patience] No improvement: {early_stop_counter}/{patience} | Best: {best_reward:.4f}")

            if early_stop_counter >= patience:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Reward has not improved for {patience} epochs.")
                print(f"   Final Best Reward: {best_reward:.4f}")
                break
            
            if entropy < stop_entropy and avg_rew > 0:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Entropy ({entropy:.4f}) is too low.")
                self.ppo.save(epoch, best_reward)
                break

        print(f"[Train] Finished. Data saved to {excel_path}")
        vec_env.close()

    # åŠ¨æ€å¹¶è¡Œè®­ç»ƒå‡½æ•°
    def old_train_parallel_modified_early_stop(self, from_check_point: bool=False):
        # 1. åˆå§‹åŒ–å¹¶è¡Œç¯å¢ƒ
        vec_env = SubprocVectorEnv(self.env_fns)
        print(f"[Train] Start dynamic parallel training on {self.device}...")
        
        best_reward = -float('inf')

        # æ—©åœå‚æ•°
        patience = getattr(self.cfg, 'patience', 30)          
        stop_entropy = getattr(self.cfg, 'stop_entropy', 0.6) 
        min_delta = 0.001                                     
        early_stop_counter = 0                                

        if from_check_point:
            start_epoch, best_reward = self.ppo.load_checkpoint()

        for epoch in range(self.cfg.max_epochs):
            if from_check_point and start_epoch is not None:
                if epoch < start_epoch:
                    print(f'[Skip] epoch = {epoch}')
                    continue
            print(f'epoch = {epoch}')
            start_time = time.time()
            
            self.ppo.init_norm_reward_list(length=len(self.env_fns))

            # Reset
            curr_np, hist_np, infos = vec_env.reset()
            
            traces = {
                'raw_curr': [], 'raw_hist': [],
                'actions': [], 'weight_idx': [], 'logp_joint': [], 
                'rewards': [], 
                'terminated': [], 'truncated': []
            }
            
            epoch_rewards = [] 
            current_equities = [0.0] * len(self.env_fns)
            
            # --- Rollout Loop ---
            for t in range(self.cfg.max_timesteps):
                
                c_tensor = torch.as_tensor(curr_np, dtype=torch.float32, device=self.device)
                h_tensor = torch.as_tensor(hist_np, dtype=torch.float32, device=self.device)
                
                with torch.no_grad():
                     state = self.ppo.extract_features_batch(c_tensor, h_tensor)
                     a, w_idx, w_val, _, _, logp_joint = self.ppo.selete_action_and_weight(state)
                
                actions_np = a.cpu().numpy()
                weights_np = w_val.cpu().numpy()
                
                next_curr, next_hist, rews, terms, truncs, infos = vec_env.step(actions_np, weights_np)
                
                scaled_rewards = []
                for num in range(len(self.env_fns)):
                    r = rews[num].item()
                    r_norm = self.ppo.reward_norm_list[num]
                    scaled_rewards.append(r_norm(r))
                
                for i, info in enumerate(infos):
                    if isinstance(info, dict):
                        key = 'final_equity' if (terms[i] or truncs[i]) else 'equity'
                        if key in info:
                            current_equities[i] = info[key]

                if t > 0:
                    rew_tensor = torch.as_tensor(scaled_rewards, dtype=torch.float32, device=self.device)
                    traces['rewards'].append(rew_tensor)
                    epoch_rewards.append(rews)

                traces['raw_curr'].append(c_tensor)
                traces['raw_hist'].append(h_tensor)
                traces['actions'].append(a)
                traces['weight_idx'].append(w_idx)
                traces['logp_joint'].append(logp_joint)
                traces['terminated'].append(torch.as_tensor(terms, device=self.device))
                traces['truncated'].append(torch.as_tensor(truncs, device=self.device))
                
                curr_np, hist_np = next_curr, next_hist

            # --- Soft End è¡¥é½ ---
            hold_actions = np.zeros(len(self.env_fns), dtype=int)
            hold_weights = np.zeros(len(self.env_fns), dtype=float)
            _, _, final_rews, _, _, _ = vec_env.step(hold_actions, hold_weights)
            
            rew_tensor = torch.as_tensor(final_rews, dtype=torch.float32, device=self.device)
            traces['rewards'].append(rew_tensor)
            epoch_rewards.append(final_rews)
            
            traces['next_raw_curr'] = torch.as_tensor(curr_np, dtype=torch.float32, device=self.device)
            traces['next_raw_hist'] = torch.as_tensor(hist_np, dtype=torch.float32, device=self.device)

            # --- Update ---
            n_act = len(traces['actions'])
            if n_act > 0:
                loss, kl, actor_loss, value_loss, entropy = self.ppo.update_parallel(traces)
            else:
                loss, kl, actor_loss, value_loss, entropy = 0, 0, 0, 0, 0

            # --- Log & Excel ---
            end_time = time.time()
            fps = (n_act * len(self.env_fns)) / (end_time - start_time + 1e-5)
            
            if len(epoch_rewards) > 0:
                avg_rew = np.mean(np.concatenate(epoch_rewards))
            else:
                avg_rew = 0.0
            avg_equity = np.mean(current_equities)

            if n_act > 0:
                all_actions = torch.stack(traces['actions']).cpu().numpy().flatten()
                counts = np.bincount(all_actions, minlength=4)
                ratios = counts / (len(all_actions) + 1e-8) 

                all_weights = torch.stack(traces['weight_idx']).cpu().numpy().flatten()
                weight_counts = np.bincount(all_weights, minlength=5) 
                weight_ratios = weight_counts / (len(all_weights) + 1e-8)
            else:
                ratios = [0, 0, 0, 0]
                weight_ratios = [0, 0, 0, 0, 0]

            self.records['epoch'].append(epoch + 1)
            self.records['reward'].append(avg_rew)
            self.records['avg_equity'].append(avg_equity)
            self.records['loss'].append(loss)
            self.records['kl'].append(kl)
            self.records['actor_loss'].append(actor_loss)
            self.records['value_loss'].append(value_loss)
            self.records['entropy'].append(entropy)
            
            self.records['hold_ratio'].append(ratios[0])
            self.records['long_ratio'].append(ratios[1])
            self.records['short_ratio'].append(ratios[2])
            self.records['close_ratio'].append(ratios[3])
            
            names = ['ratio_0', 'ratio_25', 'ratio_50', 'ratio_75', 'ratio_100']
            for k in range(len(names)):
                val = weight_ratios[k] if k < len(weight_ratios) else 0.0
                self.records[names[k]].append(val)

            df = pd.DataFrame(self.records)
            excel_path = f'{DESK_PATH}/PPO_training_data.xlsx'
            try:
                df.to_excel(excel_path, index=False)
            except Exception as e:
                print(f"[Warning] Save Excel failed: {e}")

            if (epoch + 1) % self.cfg.print_interval == 0:
                print(f"[Epoch {epoch+1}/{self.cfg.max_epochs}] "
                      f"Rew: {avg_rew:.2f} | "
                      f"Val: {avg_equity:.0f} | "
                      f"Act: H{ratios[0]:.2f}/L{ratios[1]:.2f}/S{ratios[2]:.2f}/C{ratios[3]:.2f} | "
                      f"Ent: {entropy:.2f} | "
                      f"KL: {kl:.4f} | "
                      f"FPS: {fps:.0f}")
            
            # --- æ—©åœåˆ¤æ–­ ---
            if avg_rew > best_reward + min_delta:
                best_reward = avg_rew
                early_stop_counter = 0 
                self.ppo.save(epoch, best_reward) 
                print(f"   >>> ğŸŒŸ Best Reward Updated: {best_reward:.2f} (Counter Reset)")
            else:
                early_stop_counter += 1
                print(f"   â³ [Patience] No improvement: {early_stop_counter}/{patience} | Best: {best_reward:.2f}")

            if early_stop_counter >= patience:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Reward has not improved for {patience} epochs.")
                print(f"   Final Best Reward: {best_reward:.2f}")
                break
            
            if entropy < stop_entropy and avg_rew > 0:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Entropy ({entropy:.4f}) is too low.")
                self.ppo.save(epoch, best_reward)
                break

        print(f"[Train] Finished. Data saved to {excel_path}")
        vec_env.close()


# -----------------------------------------------------------
# å…¥å£
# -----------------------------------------------------------
if __name__ == '__main__':
    if not torch.cuda.is_available():
        torch.set_num_threads(3)
        torch.set_num_interop_threads(3)

    mp.set_start_method('spawn', force=True) 

    # æ„é€ æµ·é‡æœŸæƒç»„åˆ (ç¤ºä¾‹)
    all_pairs = []
    dtype = {
        'call': str,
        'put': str,
        'call_strike': int,
        'put_strike': int,
        'call_open': str,
        'put_open': str,
        'call_expire': str,
        'put_expire': str,
    }
    df = pd.read_excel('./miniQMT/datasets/all_label_data/20251213_train.xlsx', dtype=dtype)


    for index, row in df.iterrows():
        start = row['call_open']
        end = row['call_expire']

        start_time = datetime.strptime(start, "%Y%m%d")
        end_time = datetime.strptime(end, "%Y%m%d")
        days = (end_time - start_time).days

        if days <= 40:
            continue

        call = row['call']
        put = row['put']
        end_time = start_time + timedelta(days=20)
        end_time = end_time.strftime('%Y%m%d')

        start_time = start + '100000'
        end_time = end_time + '150000'
        all_pairs.append({
            'call': call,
            'put': put,
            'start_time': start_time,
            'end_time': end_time
        })



    # all_pairs.append({
    #     'call': '10006819', 'put': '10006820', 
    #     'start_time': '20240201100000', 'end_time': '20240505150000'
    # })

    # all_pairs.append({
    #     'call': '10007866', 'put': '10007875', 
    #     'start_time': '20240926100000', 'end_time': '20241113150000'
    # })

    # all_pairs.append({
    #     'call': '10008545', 'put': '10008554', 
    #     'start_time': '20250317100000', 'end_time': '20250617150000'
    # })

    cfg = AgentConfig(
        action_dim=4, 
        option_pairs=all_pairs[0: 2], 
        max_epochs=500,
        max_timesteps=1000, 
        # num_workers=5      
    )

    if torch.cuda.is_available():
        cfg.num_workers = cfg.num_workers + 2

    

    agent = Agent(cfg)
    agent.train_parallel_modified_early_stop(from_check_point=False)