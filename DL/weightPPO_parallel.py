"""
    PPOç®—æ³• (æ ‡å‡†ç²¾åº¦ Float32 ç‰ˆ)
    åŒ…å«: Multiprocessing Parallellism + Excel Export
    å·²ç§»é™¤: Mixed Precision (AMP)
"""
from math import e
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.distributions import Categorical
from windowEnv import windowEnv
import time, json
import sys
import pandas as pd
from datetime import datetime
import pickle
from tools.Norm import Normalization, RewardNormalization, RewardScaling
from preTrain.preMOE import PreMOE
from dataclasses import dataclass
import random
import multiprocessing as mp
import traceback

# â€”â€” æ„é€ æ¯ä¸ªæ ·æœ¬çš„æƒé‡æ©ç  â€”â€” #
A_HOLD, A_LONG, A_SHORT, A_CLOSE = 0, 1, 2, 3
WEIGHT_BINS_CPU = torch.tensor([0.00, 0.25, 0.50, 0.75, 1.00])  # ç¦»æ•£æƒé‡

DESK_PATH = 'C:/Users/Flying/Desktop' # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
DESK_PATH = 'C:/Users/David/Desktop' # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹

# è¾“å‡ºç±»
class outPut():
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.logfile = open(filename, "w", encoding="utf-8")
    def write(self, message):
        self.terminal.write(message)
        self.logfile.write(message)
    def flush(self):
        self.terminal.flush()
        self.logfile.flush()
    def close(self):
        self.logfile.close()

# å…±äº«å¹²è·¯ + åŒå¤´
class ActorDualHead(nn.Module):
    def __init__(self, state_dim, hidden_dim: int=256, n_actions: int = 4, n_weights: int = 5):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.action_head = nn.Linear(hidden_dim, n_actions)
        self.weight_head = nn.Linear(hidden_dim, n_weights)

    def forward(self, state):
        # ç¡®ä¿è¾“å…¥æ˜¯ float32
        state = state.to(dtype=torch.float32)
        if state.dim() == 1:
            state = state.unsqueeze(0)
        z = self.backbone(state)
        return self.action_head(z), self.weight_head(z)

class Value(nn.Module):
    def __init__(self, state_dim, hidden_dim: int=256):
        super(Value, self).__init__()
        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    def forward(self, state):
        return self.value_net(state)

# -----------------------------------------------------------
# å¤šè¿›ç¨‹ç¯å¢ƒç›¸å…³ç±»
# -----------------------------------------------------------

class CloudpickleWrapper(object):
    def __init__(self, x):
        self.x = x
    def __getstate__(self):
        import cloudpickle
        return cloudpickle.dumps(self.x)
    def __setstate__(self, ob):
        import pickle
        self.x = pickle.loads(ob)

def worker(remote, parent_remote, env_fn_wrapper, worker_cfg: Dict[str, Any]):
    parent_remote.close()
    try:
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)
    except Exception:
        pass

    try:
        env = env_fn_wrapper.x()
    except Exception:
        tb = traceback.format_exc()
        remote.send(("__init_error__", tb))
        remote.close()
        return

    # build extractor (CPU)
    try:
        extractor = PreMOE(
            seq_len=worker_cfg["window_size"],
            pred_len=worker_cfg["pre_len"],
            n_variates=worker_cfg["n_variates"],
            d_router=worker_cfg["d_router"],
        ).to("cpu")
        if worker_cfg.get("pretrained_path") and os.path.exists(worker_cfg["pretrained_path"]):
            sd = torch.load(worker_cfg["pretrained_path"], map_location="cpu")
            extractor.load_state_dict(sd, strict=True)
        extractor.eval()
        for p in extractor.parameters():
            p.requires_grad = False
    except Exception:
        tb = traceback.format_exc()
        remote.send(("__init_error__", tb))
        remote.close()
        return

    feat = FeaturePipeline(extractor, device="cpu", adapter_dim=worker_cfg["adapter_dim"])
    actor: Optional[ActorDualHead] = None
    critic: Optional[ValueNet] = None

    pending_payload: Optional[Dict[str, Any]] = None
    adapter_dims: Optional[Dict[str, int]] = None

    def ensure_policy(curr_np: np.ndarray, hist_np: np.ndarray):
        nonlocal actor, critic, pending_payload, adapter_dims
        curr = torch.from_numpy(curr_np).float().unsqueeze(0)
        hist = torch.from_numpy(hist_np).float().unsqueeze(0)
        s, dims = feat.obs_to_state_normed(curr, hist, update_norm=False)
        adapter_dims = dims
        state_dim = int(s.shape[-1])
        if actor is None:
            actor = ActorDualHead(state_dim, hidden_dim=worker_cfg["hidden_dim"]).to("cpu")
            critic = ValueNet(state_dim, hidden_dim=worker_cfg["hidden_dim"]).to("cpu")
        if pending_payload is not None:
            apply_payload(pending_payload)
            pending_payload = None

    def apply_payload(payload: Dict[str, Any]):
        nonlocal actor, critic, adapter_dims
        if adapter_dims is None and payload.get("adapter_dims") is not None:
            adapter_dims = payload["adapter_dims"]

        if adapter_dims is not None:
            feat.build_adapter_from_dims(adapter_dims)

        if payload.get("norm_state") is not None:
            feat.build_norm_if_needed(int(payload["norm_state"]["mean"].numel()))
            assert feat.norm is not None
            feat.norm.load_state_dict(payload["norm_state"], device="cpu")

        if payload.get("adapter_state") is not None:
            assert feat.adapter is not None
            feat.adapter.load_state_dict(payload["adapter_state"], strict=True)

        if actor is not None and payload.get("actor_state") is not None:
            actor.load_state_dict(payload["actor_state"], strict=True)
        if critic is not None and payload.get("critic_state") is not None:
            critic.load_state_dict(payload["critic_state"], strict=True)

    def sample_action_weight(state_1d: torch.Tensor) -> Tuple[int, int, float, float, float]:
        """
        æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°æœ¬èº«ä¸åŒ… no_gradï¼Œä½†å®ƒåªä¼šåœ¨ rollout å¾ªç¯çš„ with torch.no_grad() å†…è¢«è°ƒç”¨
        æ‰€ä»¥ä¸ä¼šäº§ç”Ÿæ¢¯åº¦å›¾ï¼Œä¹Ÿä¸ä¼šå  GPU/CPU çš„åä¼ å¼€é”€ã€‚
        """
        assert actor is not None and critic is not None
        logits_a, logits_w = actor(state_1d)
        logits_a = logits_a.squeeze(0)
        logits_w = logits_w.squeeze(0)

        dist_a = Categorical(logits=logits_a)
        a = int(dist_a.sample().item())
        logp_a = float(dist_a.log_prob(torch.tensor(a)).item())

        allowed = torch.zeros(5, dtype=torch.bool)
        if a in (A_LONG, A_SHORT, A_CLOSE):
            allowed[1:] = True
            need_w = 1.0
        else:
            allowed[0] = True
            need_w = 0.0

        masked = logits_w.clone()
        masked[~allowed] = -1e9
        dist_w = Categorical(logits=masked)
        wi = int(dist_w.sample().item())
        logp_w = float(dist_w.log_prob(torch.tensor(wi)).item())

        wv = float(WEIGHT_BINS[wi])
        logp_joint = logp_a + need_w * logp_w
        v = float(critic(state_1d).squeeze(-1).item())
        return a, wi, wv, logp_joint, v

    # ç”¨äº rewardScaling çš„ gamma
    gamma = float(worker_cfg.get("gamma", 0.99))

    while True:
        try:
            cmd, data = remote.recv()
        except EOFError:
            break

        try:
            if cmd == "__ping__":
                remote.send(("__pong__", None))

            elif cmd == "set_task":
                idx = int(data)
                if hasattr(env, "set_task"):
                    env.set_task(idx)
                remote.send(("ok", None))

            elif cmd == "set_weights":
                payload = data
                if actor is None or critic is None or feat.adapter is None or feat.norm is None:
                    pending_payload = payload
                else:
                    apply_payload(payload)
                remote.send(("ok", None))

            elif cmd == "rollout":
                T = int(data["T"])

                # ---- reset env ----
                reset_out = env.reset()
                if isinstance(reset_out, (tuple, list)) and len(reset_out) >= 2:
                    curr_np, hist_np = reset_out[0], reset_out[1]
                else:
                    raise RuntimeError(f"env.reset unexpected: {type(reset_out)}")

                curr_np = np.asarray(curr_np, np.float32)
                hist_np = np.asarray(hist_np, np.float32)

                ensure_policy(curr_np, hist_np)

                Dc = int(curr_np.shape[-1])
                L = int(hist_np.shape[-2])
                Dh = int(hist_np.shape[-1])

                # ---- buffers ----
                raw_curr = np.zeros((T, Dc), np.float32)
                raw_hist = np.zeros((T, L, Dh), np.float32)
                actions = np.zeros((T,), np.int64)
                w_idx = np.zeros((T,), np.int64)
                w_val = np.zeros((T,), np.float32)
                logp_old = np.zeros((T,), np.float32)
                value_old = np.zeros((T,), np.float32)

                # rewards å¯¹é½ï¼šrewards[t] åº”è¯¥æ˜¯ â€œt åŠ¨ä½œâ€çš„å¥–åŠ±
                # ä½† env.step åœ¨ t è¿”å›çš„æ˜¯ (t-1) åŠ¨ä½œçš„å¥–åŠ±ï¼Œæ‰€ä»¥æˆ‘ä»¬å†™ rewards[t-1] = r_scaled
                rewards = np.zeros((T,), np.float32)

                done = np.zeros((T,), np.bool_)    # done[t] å¯¹åº” â€œt åŠ¨ä½œä¹‹åæ˜¯å¦ç»ˆæ­¢â€
                valid = np.zeros((T,), np.bool_)   # valid[t] è¡¨ç¤ºè¿™ä¸€æ­¥ transition æ˜¯å¦å¯ç”¨äºè®­ç»ƒï¼ˆå¿…é¡»æœ‰å¯¹é½åçš„ rewardï¼‰

                # ---- per-episode RewardScaling (æ¯ä¸ª worker/episode ç‹¬ç«‹) ----
                r_scaler = RewardScaling(shape=1, gamma=gamma)
                try:
                    r_scaler.reset()
                except Exception:
                    pass

                terminated_early = False

                # ä½ è¿™ä¸ªç¯å¢ƒçš„ reward å»¶è¿Ÿï¼št è¿”å› R_tï¼Œä½†å±äº t-1 åŠ¨ä½œ
                # å› æ­¤ï¼št=0 çš„ reward æ— æ„ä¹‰ï¼›æœ€åä¸€ä¸ªåŠ¨ä½œæ²¡æœ‰ rewardï¼ˆé™¤éä½ é¢å¤–å† step ä¸€æ¬¡ï¼‰
                for t in range(T):
                    # 1) record state at time t
                    raw_curr[t] = curr_np
                    raw_hist[t] = hist_np

                    if terminated_early:
                        # padding
                        actions[t] = A_HOLD
                        w_idx[t] = 0
                        w_val[t] = 0.0
                        logp_old[t] = 0.0
                        value_old[t] = 0.0
                        rewards[t] = 0.0
                        done[t] = True
                        valid[t] = False
                        continue

                    # 2) decide action using policy (NO GRAD)
                    with torch.no_grad():
                        curr = torch.from_numpy(curr_np).unsqueeze(0)
                        hist = torch.from_numpy(hist_np).unsqueeze(0)
                        s, _ = feat.obs_to_state_normed(curr, hist, update_norm=False)
                        a, wi, wv, lp, v = sample_action_weight(s)

                    # 3) env step
                    step_out = env.step(a, wv)
                    if isinstance(step_out, (tuple, list)) and len(step_out) >= 5:
                        next_curr, next_hist, r, term, trunc = step_out[0], step_out[1], step_out[2], step_out[3], step_out[4]
                    else:
                        raise RuntimeError(f"env.step return length={len(step_out)} unexpected")

                    d = bool(term or trunc)

                    # 4) write transition fields for time t (but reward for t will come at t+1)
                    actions[t] = a
                    w_idx[t] = wi
                    w_val[t] = wv
                    logp_old[t] = lp
                    value_old[t] = v
                    done[t] = d

                    # 5) reward alignment + rewardScaling:
                    # å½“å‰ step è¿”å›çš„ r å±äº (t-1) çš„åŠ¨ä½œï¼Œæ‰€ä»¥å†™å…¥ rewards[t-1]
                    # å¹¶ä¸” t=0 çš„ r ä¸¢å¼ƒ
                    try:
                        r_in = torch.as_tensor([float(r)], dtype=torch.float32)
                        r_scaled = float(r_scaler(r_in).item())
                    except Exception:
                        # å…œåº•ï¼šå¦‚æœ RewardScaling æ”¯æŒ float è¾“å…¥
                        r_scaled = float(r_scaler(float(r)))

                    if t > 0:
                        # åªæœ‰ (t-1) è¿™ä¸ª transition æ‰çœŸæ­£æ‹¿åˆ°äº†å±äºå®ƒçš„ rewardï¼Œæ‰€ä»¥æ‰ valid
                        rewards[t - 1] = r_scaled
                        # æ³¨æ„ï¼št-1 è¿™æ­¥æ˜¯å¦æœ‰æ•ˆï¼Œè¿˜å¾—çœ‹ t-1 è‡ªå·±æ˜¯å¦æ˜¯â€œæœ€åä¸€æ­¥â€/æ˜¯å¦è¢«æå‰ç»ˆæ­¢
                        # è¿™é‡Œåªä¿è¯ reward å·²å¯¹é½åˆ° t-1
                        valid[t - 1] = True

                    # å½“å‰ t è¿™æ­¥ï¼šreward è¿˜æ²¡æ¥ï¼Œæ‰€ä»¥å…ˆä¸ç½® valid[t]
                    # å¦‚æœè¿™ä¸€åˆ»ç»ˆæ­¢äº†ï¼Œé‚£ä¹ˆè¿™ä¸€æ­¥æ°¸è¿œç­‰ä¸åˆ° reward -> valid[t] å¿…é¡» False
                    if d:
                        terminated_early = True
                        valid[t] = False  # å¼ºåˆ¶
                        # ç»ˆæ­¢æ—¶ä¸å†æ›´æ–° curr_np/hist_np
                        continue

                    # 6) move to next state
                    curr_np = np.asarray(next_curr, np.float32)
                    hist_np = np.asarray(next_hist, np.float32)

                # ---- å…³é”®ï¼šæœ€åä¸€æ­¥æ°¸è¿œæ²¡æœ‰ä¸‹ä¸€æ­¥ rewardï¼ˆå»¶è¿Ÿæœºåˆ¶ä¸‹ï¼‰ï¼Œå¿…é¡» mask æ‰ ----
                valid[T - 1] = False
                rewards[T - 1] = 0.0

                # ---- bootstrap last_valueï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰ ----
                if not terminated_early:
                    with torch.no_grad():
                        curr = torch.from_numpy(curr_np).unsqueeze(0)
                        hist = torch.from_numpy(hist_np).unsqueeze(0)
                        s_last, _ = feat.obs_to_state_normed(curr, hist, update_norm=False)
                        assert critic is not None
                        last_value = float(critic(s_last).squeeze(-1).item())
                else:
                    last_value = 0.0

                # equity_endï¼šå°½é‡ä» env.account_controller å–
                equity_end = None
                try:
                    if hasattr(env, "account_controller") and hasattr(env.account_controller, "equity"):
                        equity_end = float(env.account_controller.equity)
                except Exception:
                    equity_end = None
                if equity_end is None:
                    equity_end = float("nan")

                remote.send(("traj", {
                    "raw_curr": raw_curr,
                    "raw_hist": raw_hist,
                    "actions": actions,
                    "w_idx": w_idx,
                    "w_val": w_val,
                    "logp_old": logp_old,
                    "value_old": value_old,
                    "rewards": rewards,
                    "done": done,
                    "valid": valid,
                    "last_value": np.array([last_value], np.float32),
                    "equity_end": np.array([equity_end], np.float32),
                }))

            elif cmd == "close":
                try:
                    env.close()
                except Exception:
                    pass
                remote.send(("ok", None))
                remote.close()
                break

            else:
                raise NotImplementedError(cmd)

        except Exception:
            tb = traceback.format_exc()
            try:
                remote.send(("error", tb))
            except Exception:
                pass
            break


def old_worker(remote, parent_remote, env_fn_wrapper):
    parent_remote.close()
    env = env_fn_wrapper.x()
    try:
        while True:
            cmd, data = remote.recv()
            if cmd == 'step':
                action, weight = data
                nc, nh, r, term, trunc = env.step(action, weight)
                
                info = {}
                if hasattr(env, 'account_controller'):
                    info['equity'] = env.account_controller.equity
                
                if term or trunc:
                    info['final_equity'] = env.account_controller.equity
                    nc, nh, _ = env.reset()
                
                remote.send((nc, nh, r, term, trunc, info))
            
            elif cmd == 'reset':
                nc, nh, _ = env.reset()
                info = {}
                if hasattr(env, 'account_controller'):
                    info['equity'] = env.account_controller.equity
                remote.send((nc, nh, info))
            
            elif cmd == 'close':
                env.close()
                remote.close()
                break
            else:
                raise NotImplementedError
    except KeyboardInterrupt:
        print('SubprocEnv worker: got KeyboardInterrupt')
    finally:
        env.close()

class SubprocVectorEnv:
    def __init__(self, env_fns):
        self.waiting = False
        self.closed = False
        self.num_envs = len(env_fns)
        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in range(self.num_envs)])
        self.ps = []
        
        for work_remote, remote, env_fn in zip(self.work_remotes, self.remotes, env_fns):
            args = (work_remote, remote, CloudpickleWrapper(env_fn))
            p = mp.Process(target=worker, args=args, daemon=True) 
            p.start()
            self.ps.append(p)
            work_remote.close()

    def step(self, actions, weights):
        for remote, action, weight in zip(self.remotes, actions, weights):
            remote.send(('step', (action, weight)))
        results = [remote.recv() for remote in self.remotes]
        currents, histories, rewards, terms, truncs, infos = zip(*results)
        return np.stack(currents), np.stack(histories), np.stack(rewards), np.stack(terms), np.stack(truncs), infos

    def reset(self):
        for remote in self.remotes:
            remote.send(('reset', None))
        results = [remote.recv() for remote in self.remotes]
        currents, histories, infos = zip(*results)
        return np.stack(currents), np.stack(histories), infos

    def close(self):
        if self.closed: return
        for remote in self.remotes:
            remote.send(('close', None))
        for p in self.ps:
            p.join()
        self.closed = True


class ViewProjector(nn.Module):
    """
    è´Ÿè´£å¤„ç†å•ä¸ªæ¨¡å‹çš„ (High-Dim, Low-Dim) å¹¶å°†å…¶èåˆ
    """
    def __init__(self, high_dim, low_dim, out_dim=64):
        super().__init__()
        
        # 1. å‹ç¼©é«˜ç»´ç‰¹å¾
        self.high_net = nn.Sequential(
            nn.LayerNorm(high_dim),
            nn.Linear(high_dim, out_dim), # çº¿æ€§å‹ç¼©
            # nn.Dropout(0.1)
        )
        
        # 2. åµŒå…¥ä½ç»´ç»Ÿè®¡é‡
        self.low_net = nn.Sequential(
            nn.LayerNorm(low_dim),
            nn.Linear(low_dim, 32),       # å‡ç»´å¢å¼º
            # nn.Tanh()                     # èµ‹äºˆéçº¿æ€§
            nn.GELU(),
        )
        
        # 3. è§†å›¾èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(out_dim + 32, out_dim),
            nn.LayerNorm(out_dim)
        )
        
    def forward(self, x_high, x_low):
        h = self.high_net(x_high)
        l = self.low_net(x_low)
        # æ‹¼æ¥åèåˆ
        return self.fusion(torch.cat([h, l], dim=-1))


class MultiViewAdapter(nn.Module):
    def __init__(self, 
                 dims_dict: dict,  # åŒ…å«å„æ¨¡å‹ç»´åº¦çš„å­—å…¸
                 final_dim: int = 128):
        super().__init__()
        
        # 1. å®šä¹‰ä¸‰ä¸ªç‹¬ç«‹çš„æŠ•å½±å™¨ (Trainable)
        # å‡è®¾æˆ‘ä»¬æƒ³è®©æ¯ä¸ªæ¨¡å‹è´¡çŒ® 48 ç»´çš„ç‰¹å¾
        view_dim = 48
        
        self.varma_proj = ViewProjector(
            high_dim=dims_dict['varma_high'], 
            low_dim=dims_dict['varma_low'], 
            out_dim=view_dim
        )
        
        self.basis_proj = ViewProjector(
            high_dim=dims_dict['basis_high'], 
            low_dim=dims_dict['basis_low'], 
            out_dim=view_dim
        )
        
        self.itrans_proj = ViewProjector(
            high_dim=dims_dict['itrans_high'], 
            low_dim=dims_dict['itrans_low'], 
            out_dim=view_dim
        )
        
        # Router çš„ç‰¹å¾ç›´æ¥å¤„ç† (å› ä¸ºå®ƒæœ¬æ¥å°±æ˜¯ä½ç»´é«˜è¯­ä¹‰)
        self.router_proj = nn.Sequential(
            nn.LayerNorm(dims_dict['router']),
            nn.Linear(dims_dict['router'], 32)
        )
        
        # 2. æœ€ç»ˆèåˆå±‚
        # è¾“å…¥ç»´åº¦ = 48*3 + 32 = 176
        self.final_net = nn.Sequential(
            nn.Linear(view_dim * 3 + 32, final_dim),
            nn.LayerNorm(final_dim) # å†æ¬¡å¼ºè°ƒï¼šä¸è¦ Tanhï¼Œç”¨ LayerNorm
        )
        
    def raw_forward(self, inputs: dict):
        # inputs æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡å‹çš„åŸå§‹è¾“å‡º
        
        # 1. å¹¶è¡Œå¤„ç†å„è§†å›¾
        v_varma = self.varma_proj(inputs['varma_h'], inputs['varma_l'])
        v_basis = self.basis_proj(inputs['basis_h'], inputs['basis_l'])
        v_itrans = self.itrans_proj(inputs['itrans_h'], inputs['itrans_l'])
        
        v_router = self.router_proj(inputs['router'])
        
        # 2. æ‹¼æ¥
        combined = torch.cat([v_varma, v_basis, v_itrans, v_router], dim=-1)
        
        # 3. è¾“å‡º
        return self.final_net(combined)
    
    def forward(self, inputs: dict, train: bool=True):
        if train:
            return self.raw_forward(inputs)
        with torch.no_grad():
            return self.raw_forward(inputs)


# -----------------------------------------------------------
# PPO Agent ç±» (æ ‡å‡† Float32 ç‰ˆ)
# -----------------------------------------------------------

class weightPPO:
    def __init__(self, action_dim: int, actor_lr: float=3e-4, value_lr: float=5e-4, 
                 gamma: float=0.99, clip_eps: float=0.1, k_epochs: int=5, 
                 device: str='cpu', check_path: str='./miniQMT/DL/checkout',
                 window_size: int=32, pre_len: int=4):
        
        self.device = device
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.k_epochs = k_epochs
        self.check_path = f'{check_path}/check_data_all.pt'
        self.actor_lr, self.value_lr = actor_lr, value_lr
        self.action_dim = action_dim
        self.window_size = window_size
        self.WEIGHT_BINS = WEIGHT_BINS_CPU.to(self.device)

        self.actor, self.value = None, None
        self.opt_a, self.opt_b = None, None
        
        self.pre_len = pre_len
        self.extractor = PreMOE(seq_len=self.window_size, pred_len=self.pre_len, n_variates=13, d_router=128).to(self.device)
        self.load_moe_parameters()

        # å†»ç»“ extractor å‚æ•°
        for p in self.extractor.parameters():
            p.requires_grad = False
        self.extractor.eval()
        
        self.feature_adapter = None
        self.opt_c = None
        self.state_norm = None
        self.reward_norm = None
        self.reward_norm_list = []

        print(f"[Info] Device: {self.device} | Precision: Float32 (AMP Removed)")

    def load_moe_parameters(self):
        try:
            SAVE_PATH = f'./miniQMT/DL/preTrain/weights/preMOE_best_dummy_data_{self.window_size}_{self.pre_len}.pth'
            state_dict = torch.load(SAVE_PATH, map_location=self.device)
            self.extractor.load_state_dict(state_dict)

            print(f"[Info] åŠ è½½MOEå‚æ•°æˆåŠŸ~")
        except Exception as e:
            print(f"[Info] åŠ è½½MOEå‚æ•°å¤±è´¥, e = {e}")

    def init_norm_state(self, x: torch.Tensor):
        self.state_norm = Normalization(x.shape[1:]) 
    
    def init_norm_reward(self):
        self.reward_norm = RewardScaling(shape=(1,), gamma=self.gamma)

    def init_norm_reward_list(self, length: int):
        self.reward_norm_list = []
        for _ in range(length):
            self.reward_norm_list.append(RewardScaling(shape=(1,), gamma=self.gamma))
    

    # ä»…ç”¨äºæ¨ç†
    def load_infer_parameters(self, check_path: str=None, device: str=None):
        device = device if device else self.device
        path = check_path if check_path else self.check_path
        data = torch.load(path, map_location=self.device)

        # åŠ è½½actor, valueç½‘ç»œå’Œç‰¹å¾æå–ç½‘ç»œ
        self.actor.load_state_dict(data['actor_state'])
        self.value.load_state_dict(data['value_state'])
        self.feature_adapter.load_state_dict(data['features_adapter_state'])

        print(f"[Info: æ¨ç†é˜¶æ®µ] åŠ è½½actor, valueç½‘ç»œå’Œç‰¹å¾æå–ç½‘ç»œ~")


    def exe_reward_norm(self, x: float):
        if self.reward_norm is None: 
            self.init_norm_reward()
        x = torch.tensor([x], dtype=torch.float32)
        return self.reward_norm(x).item()


    def extract_features_batch(self, current_state: torch.Tensor, history_state: torch.Tensor, cal_dim: bool = False):
        if current_state.device != torch.device(self.device): 
            current_state = current_state.to(self.device)
        if history_state.device != torch.device(self.device): 
            history_state = history_state.to(self.device)

        call_state, put_state = torch.chunk(history_state, chunks=2, dim=2)

        
        call_dict = self.extractor.encode_tokens(call_state)
        put_dict = self.extractor.encode_tokens(put_state)

        dims_dict = {
            'varma_high': call_dict['varma_h'].shape[-1],
            'varma_low': call_dict['varma_l'].shape[-1],
            'basis_high': call_dict['basis_h'].shape[-1],
            'basis_low': call_dict['basis_l'].shape[-1],
            'itrans_high': call_dict['itrans_h'].shape[-1],
            'itrans_low': call_dict['itrans_l'].shape[-1],
            'router': call_dict['router'].shape[-1]
        }


        if self.feature_adapter is None:
            self.feature_adapter = MultiViewAdapter(dims_dict, final_dim=128).to(self.device)
            self.opt_c = optim.Adam(self.feature_adapter.parameters(), lr=self.actor_lr)


        train = not cal_dim
        reduce_call = self.feature_adapter(call_dict, train=train)
        reduce_put = self.feature_adapter(put_dict, train=train)

        features = torch.cat([current_state, reduce_call, reduce_put], dim=-1)

        if self.state_norm is None:
            self.init_norm_state(features)
        return self.state_norm(features, update=train)

    def set_actor_and_value(self, state_dim: int):
        self.actor = ActorDualHead(state_dim, n_actions=self.action_dim).to(self.device)
        self.value = Value(state_dim).to(self.device)
        self.opt_a = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.opt_b = optim.Adam(self.value.parameters(), lr=self.value_lr)

    @torch.no_grad()
    def selete_action_and_weight(self, state, test: bool=False):
        # --- ç§»é™¤ Autocast ---
        logits_a, logits_w = self.actor(state)
        
        dist_a = Categorical(logits=logits_a)
        a = torch.argmax(logits_a, dim=-1) if test else dist_a.sample()
        logp_a = dist_a.log_prob(a)

        K = a.shape[0]
        allowed = torch.zeros(K, 5, dtype=torch.bool, device=self.device)
        mask_ls = (a == A_LONG) | (a == A_SHORT) | (a == A_CLOSE)
        allowed[mask_ls, 1:] = True
        allowed[~mask_ls, 0] = True
        
        masked_logits_w = logits_w.clone()
        # Float32 ä¸‹å¯ä»¥ç”¨ -1e9
        masked_logits_w[~allowed] = -1e9 
        
        dist_w = Categorical(logits=masked_logits_w)
        w_idx = torch.argmax(masked_logits_w, dim=-1) if test else dist_w.sample()
        logp_w = dist_w.log_prob(w_idx)
        w_val = self.WEIGHT_BINS[w_idx]

        need_w = ((a == A_LONG) | (a == A_SHORT) | (a == A_CLOSE)).float()
        logp_joint = logp_a + need_w * logp_w

        return a, w_idx, w_val, logp_a, logp_w, logp_joint
    
    def update_parallel(self, traces, target_kl=0.015, entropy_coef=0.01, value_coef=0.5):
        raw_curr = torch.stack(traces['raw_curr']).to(self.device)
        raw_hist = torch.stack(traces['raw_hist']).to(self.device)
        
        actions = torch.stack(traces['actions']).to(self.device)
        w_idx = torch.stack(traces['weight_idx']).to(self.device)
        old_logp = torch.stack(traces['logp_joint']).to(self.device)
        rewards = torch.stack(traces['rewards']).to(self.device)
        next_raw_curr = traces['next_raw_curr'].to(self.device)
        next_raw_hist = traces['next_raw_hist'].to(self.device)
        
        dones = torch.stack(traces['terminated']).to(self.device) | torch.stack(traces['truncated']).to(self.device)
        
        T, K, Dc = raw_curr.shape
        Dh = raw_hist.shape[-1]

  

        # --- 1. è®¡ç®— GAE (no_grad) ---
        with torch.no_grad():
            # æ‹æ‰, å¾—åˆ°æ‰€æœ‰è½¨è¿¹æ‰€æœ‰æ­¥çš„curr_state
            curr_flat = raw_curr.view(T*K, -1)

            # æ‹æ‰, å¾—åˆ°æ‰€æœ‰è½¨è¿¹æ‰€æœ‰æ­¥çš„high_state
            hist_flat = raw_hist.view(T*K, -1, Dh)

            # å¾—åˆ°é™ç»´ç‰¹å¾ä½œä¸ºPPOçš„çŠ¶æ€
            feat_tk = self.extract_features_batch(curr_flat, hist_flat)

            # è®¡ç®—Tä¸ªæ—¶é—´æ­¥, Kä¸ªå¹¶è¡Œç¯å¢ƒçš„çŠ¶æ€ä»·å€¼å‡½æ•°
            v_tk = self.value(feat_tk).view(T, K)
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€
            next_feat = self.extract_features_batch(next_raw_curr, next_raw_hist)
            v_next = self.value(next_feat).squeeze(-1)
        
            # GAE å…¬å¼
            v_tk_cpu = v_tk.float()
            v_next_cpu = v_next.float()
            rew_cpu = rewards.float()
            dones_cpu = dones.float()
            
            adv = torch.zeros_like(rew_cpu)
            last_gae = 0
            for t in reversed(range(T)):
                m = 1.0 - dones_cpu[t]
                v_tp1 = v_next_cpu if t == T-1 else v_tk_cpu[t+1]
                delta = rew_cpu[t] + self.gamma * v_tp1 * m - v_tk_cpu[t]
                last_gae = delta + self.gamma * 0.95 * m * last_gae
                adv[t] = last_gae
            
            returns = adv + v_tk_cpu
        
        # --- å‡†å¤‡è®­ç»ƒæ•°æ® ---
        curr_flat = raw_curr.view(T*K, -1)
        hist_flat = raw_hist.view(T*K, -1, Dh)
        a_flat = actions.view(-1)
        w_flat = w_idx.view(-1)
        old_logp_flat = old_logp.view(-1)
        adv_flat = adv.view(-1).to(self.device)
        ret_flat = returns.view(-1).to(self.device)
        
        adv_flat = (adv_flat - adv_flat.mean()) / (adv_flat.std() + 1e-8)

        # --- 2. è®­ç»ƒå¾ªç¯ ---
        for i in range(self.k_epochs):
            s_flat = self.extract_features_batch(curr_flat, hist_flat)
            
            logits_a, logits_w = self.actor(s_flat)
            
            dist_a = Categorical(logits=logits_a)
            new_logp_a = dist_a.log_prob(a_flat)
            ent_a = dist_a.entropy().mean()
            
            need_w = ((a_flat == A_LONG) | (a_flat == A_SHORT) | (a_flat == A_CLOSE)).float()
            
            # Mask
            lw = logits_w.clone()
            mask = torch.zeros_like(lw, dtype=torch.bool)
            mask[need_w.bool(), 1:] = True
            mask[~need_w.bool(), 0] = True
            
            # Float32 å®‰å…¨å€¼
            lw[~mask] = -1e9
            
            dist_w = Categorical(logits=lw)
            new_logp_w = dist_w.log_prob(w_flat)
            ent_w = (need_w * dist_w.entropy()).sum() / (need_w.sum() + 1e-6)
            
            logp_new = new_logp_a + need_w * new_logp_w
            ratio = torch.exp(logp_new - old_logp_flat)
            
            surr1 = ratio * adv_flat
            surr2 = torch.clamp(ratio, 1-self.clip_eps, 1+self.clip_eps) * adv_flat
            loss_a = -torch.min(surr1, surr2).mean()
            
            v_pred = self.value(s_flat).squeeze(-1)
            loss_v = F.mse_loss(v_pred, ret_flat)
            
            loss = loss_a + value_coef * loss_v - entropy_coef * (ent_a + 0.5 * ent_w)

            # --- æ ‡å‡†åå‘ä¼ æ’­ (æ—  Scaler) ---
            self.opt_a.zero_grad()
            self.opt_b.zero_grad()
            if self.opt_c: self.opt_c.zero_grad()
            
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            
            self.opt_a.step()
            self.opt_b.step()
            if self.opt_c: self.opt_c.step()

            # è®°å½•æœ€åä¸€æ­¥çš„æŒ‡æ ‡
            last_actor_loss = loss_a.item()
            last_value_loss = loss_v.item()
            last_entropy = (ent_a + 0.5 * ent_w).item()


            kl = (old_logp_flat - logp_new).mean().abs()
            if kl > 1.5 * target_kl:
                print(f"Early stop at epoch {i} KL={kl.item():.4f}")
                break
    
        return loss.item(), kl.item(), last_actor_loss, last_value_loss, last_entropy
    

    def save(self, epoch: int = None, best_reward: float = None, path: str = None):
            """
            ä¿å­˜å½“å‰ actor / value ä»¥åŠä¼˜åŒ–å™¨ç­‰ä¿¡æ¯
            """
            save_path = path or self.check_path
            # os.makedirs(os.path.dirname(save_path), exist_ok=True)

            data = {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "actor_state": self.actor.state_dict(),
                "value_state": self.value.state_dict(),
                "opt_a_state": self.opt_a.state_dict(),
                "opt_b_state": self.opt_b.state_dict(),
                "features_adapter_state": self.feature_adapter.state_dict(),
                "h_params": {
                    "gamma": self.gamma,
                    "clip_eps": self.clip_eps,
                    "k_epochs": self.k_epochs,
                    "device": self.device,
                },
                "epoch": epoch,
                "best_reward": best_reward.item(),
            }
            torch.save(data, save_path)
            # å¯é€‰ï¼šæ‰“å°ä¸€ä¸‹è·¯å¾„æ–¹ä¾¿ç¡®è®¤
            print(f"[PPO] checkpoint saved to: {save_path}")

# -----------------------------------------------------------
# ä¸» Agent ç±»
# -----------------------------------------------------------

@dataclass
class AgentConfig:
    action_dim: int
    option_pairs: list
    max_epochs: int=300
    max_timesteps: int=1000
    device: str='cuda' if torch.cuda.is_available() else 'cpu'
    print_interval: int=1
    start_time: str='20250408100000'
    end_time: str='20250924150000'

    # æœŸæƒç¯å¢ƒçš„æ‰‹ç»­è´¹
    fee: float=1.3

    # æœŸæƒç¯å¢ƒçš„åˆå§‹èµ„é‡‘
    init_capital: float=100000.0

    # æ¨¡å¼: è®­ç»ƒ/æµ‹è¯•
    mode: str='train'

class Agent:
    def __init__(self, config: AgentConfig):
        self.cfg = config
        self.device = config.device

        # å•ä¸ªæµ‹è¯•
        self.env = None

        # å¤šä¸ªè®­ç»ƒ
        self.env_fns = []

        if config.mode == 'train':
            self.init_train()
        

        # --- æ•°æ®è®°å½•å®¹å™¨ ---
        self.records = {
            'epoch': [],
            'reward': [],
            'avg_equity': [],
            'loss': [],
            'kl': [],
            'hold_ratio': [],
            'long_ratio': [],
            'short_ratio': [],
            'close_ratio': [],
            'actor_loss': [],
            'value_loss': [],
            'entropy': [],
            'ratio_0': [],
            'ratio_25': [],
            'ratio_50': [],
            'ratio_75': [],
            'ratio_100': [],
        }

    # è®­ç»ƒæ¨¡å¼åˆå§‹åŒ–
    def init_train(self):
        config = self.cfg
        self.ppo = weightPPO(config.action_dim, device=self.device)
        
        self.env_fns = []
        for pair in config.option_pairs:
            def make_env(c=pair['call'], p=pair['put']):
                return windowEnv(cfg.init_capital, c, p, config.max_timesteps, 
                                 cfg.fee, config.start_time, config.end_time, '510050')
            self.env_fns.append(make_env)
        
        dummy_env = self.env_fns[0]()
        c, h, _ = dummy_env.reset()
        dummy_env.close()
        
        c_b = torch.tensor([c], dtype=torch.float32, device=self.device)
        h_b = torch.tensor([h], dtype=torch.float32, device=self.device)
        feat = self.ppo.extract_features_batch(c_b, h_b, cal_dim=True)
        self.ppo.set_actor_and_value(feat.shape[-1])


    # æµ‹è¯•æ¨¡å¼è®¾ç½®ç¯å¢ƒ, ç¯å¢ƒçš„å‚æ•°ç”±å¤–éƒ¨æŒ‡å®š
    def set_env(self, env: windowEnv):
        print(f"[Info] è®¾ç½®env | call = {env.call}, put = {env.put}")
        self.env = env
        current_shape, history_shape = self.env.get_raw_shape()
        current_state = torch.zeros(current_shape)
        history_state = torch.zeros(history_shape)
        if self.ppo is None:
            self.ppo = weightPPO(self.action_dim, window_size=self.window_size, device=self.device)
            results = self.ppo.extract_features(current_state, history_state, cal_dim=True)
            _, state_dim = results.shape

            self.ppo.set_actor_and_value(state_dim)

    # è®¾ç½®å¯¹stateçš„å½’ä¸€åŒ–æ¨¡å—
    def set_norm(self, state_norm: Normalization):
        self.ppo.state_norm = state_norm
        print(f"[Info] Normè®¾ç½®å®Œæˆ | state.n = {self.ppo.state_norm.running_ms.n}")


    # å¸¦æ—©åœçš„è®­ç»ƒå‡½æ•°
    def train_parallel_modified_early_stop(self):
        # 1. åˆå§‹åŒ–å¹¶è¡Œç¯å¢ƒ
        vec_env = SubprocVectorEnv(self.env_fns)
        print(f"[Train] Start parallel training on {self.device}...")
        
        # 2. åˆå§‹åŒ–æœ€ä½³å¥–åŠ±è®°å½•
        best_reward = -float('inf')

        # ==========================================
        # [æ–°å¢] æ—©åœæœºåˆ¶å‚æ•° (å¦‚æœæ²¡æœ‰åœ¨cfgå®šä¹‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼)
        # ==========================================
        patience = getattr(self.cfg, 'patience', 30)         # å®¹å¿å¤šå°‘è½®ä¸æå‡
        stop_entropy = getattr(self.cfg, 'stop_entropy', 0.6) # ç†µä½äºå¤šå°‘åœæ­¢
        min_delta = 0.001                                    # è®¤ä¸ºæ˜¯æå‡çš„æœ€å°å¹…åº¦
        early_stop_counter = 0                               # è®¡æ•°å™¨
        # ==========================================

        for epoch in range(self.cfg.max_epochs):
            start_time = time.time()
            # æ¯ä¸€æ¬¡éœ€è¦é‡æ–°rewardScaling
            self.ppo.init_norm_reward_list(length=len(self.env_fns))

            # Reset ç¯å¢ƒ (æ¯ä¸ª Epoch é‡æ–°å¼€å§‹è·‘ä¸€éå®Œæ•´æ•°æ®)
            curr_np, hist_np, infos = vec_env.reset()
            
            traces = {
                'raw_curr': [], 'raw_hist': [],
                'actions': [], 'weight_idx': [], 'logp_joint': [], 
                'rewards': [], 
                'terminated': [], 'truncated': []
            }
            
            epoch_rewards = [] 
            current_equities = [0.0] * len(self.env_fns)
            
            # æ ‡è®°å˜é‡
            is_hard_end = False

            # --- Rollout Loop ---
            for t in range(self.cfg.max_timesteps):

                
                # 1. è®°å½• State
                c_tensor = torch.as_tensor(curr_np, dtype=torch.float32, device=self.device)
                h_tensor = torch.as_tensor(hist_np, dtype=torch.float32, device=self.device)
                
                # 2. å†³ç­–
                with torch.no_grad():
                     state = self.ppo.extract_features_batch(c_tensor, h_tensor)
                     a, w_idx, w_val, _, _, logp_joint = self.ppo.selete_action_and_weight(state)
                
                actions_np = a.cpu().numpy()
                weights_np = w_val.cpu().numpy()
                
                # 3. æ­¥è¿›
                next_curr, next_hist, rews, terms, truncs, infos = vec_env.step(actions_np, weights_np)

                                
                scaled_rewards = []
                for num in range(len(self.env_fns)):
                    r = rews[num].item()
                    r_norm = self.ppo.reward_norm_list[num]
                    scaled_rewards.append(r_norm(r))
                
                # æ›´æ–° Log
                for i, info in enumerate(infos):
                    if isinstance(info, dict):
                        key = 'final_equity' if (terms[i] or truncs[i]) else 'equity'
                        if key in info:
                            current_equities[i] = info[key]

                # === å¥–åŠ±å¯¹é½ ===
                # å½“å‰ rews æ˜¯ä¸Šä¸€æ­¥åŠ¨ä½œçš„æœï¼Œå¡«å…¥ traces['rewards']
                # æ³¨æ„ï¼šç¬¬ä¸€æ­¥(t=0)äº§ç”Ÿçš„ rews æ˜¯æ— æ„ä¹‰çš„(å±äºè¿‡å»)ï¼Œæ‰€ä»¥ t>0 æ‰å¡«
                if t > 0:
                    rew_tensor = torch.as_tensor(scaled_rewards, dtype=torch.float32, device=self.device)
                    traces['rewards'].append(rew_tensor)
                    epoch_rewards.append(rews)

                # 4. è®°å½•å½“å‰æ­¥æ•°æ® (Action, State)
                # æ­¤æ—¶æˆ‘ä»¬è¿˜æ²¡æ‹¿åˆ°å½“å‰ Action çš„ Reward
                traces['raw_curr'].append(c_tensor)
                traces['raw_hist'].append(h_tensor)
                traces['actions'].append(a)
                traces['weight_idx'].append(w_idx)
                traces['logp_joint'].append(logp_joint)
                traces['terminated'].append(torch.as_tensor(terms, device=self.device))
                traces['truncated'].append(torch.as_tensor(truncs, device=self.device))
                
                curr_np, hist_np = next_curr, next_hist
                
                # === æ£€æµ‹ Hard End (æ•°æ®è·‘å®Œäº†) ===
                if np.all(terms | truncs):
                    is_hard_end = True
                    # æ­¤æ—¶ rews åŒ…å« (A_{t-1} çš„ç›ˆäº + ç»ˆç‚¹ Bonus)
                    # è¿™ä¸ª rews å·²ç»åœ¨ä¸Šé¢ "if t > 0" é‡Œå¡«ç»™äº† A_{t-1}ï¼Œå¤„ç†å®Œæ¯•ã€‚
                    
                    # å…³é”®ï¼šå½“å‰çš„åŠ¨ä½œ A_t (æœ€åä¸€æ­¥) åˆšåˆšè¢« append è¿› tracesã€‚
                    # ä½†æ˜¯ç¯å¢ƒå·²ç» Done äº†ï¼ŒA_t æ²¡æœ‰æœªæ¥ï¼Œæ— æ³•ç»“ç®—ã€‚
                    # æˆ‘ä»¬å°†åœ¨å¾ªç¯å¤–æŠŠå®ƒåˆ æ‰ã€‚
                    break

            # --- å¾ªç¯åå¤„ç† (Post-processing) ---
            
            if is_hard_end:
                # === æƒ…å†µ A: Hard End ===
                # traces é‡Œå¤šè®°å½•äº†æœ€åä¸€æ­¥åŠ¨ä½œ A_tï¼Œä½†å®ƒæ²¡æœ‰ Reward (å› ä¸ºæ²¡æœ‰ T+1)
                # æˆ‘ä»¬å¿…é¡»æŠŠå®ƒåˆ æ‰ï¼Œè®©æ•°æ®å¯¹é½
                
                # 1. ä¿å­˜æœ€åä¸€æ­¥çš„çŠ¶æ€ä½œä¸º "Next State" (ç»™ A_{t-1} ç”¨)
                # æ­¤æ—¶ traces['raw_curr'][-1] å°±æ˜¯å‘ç”Ÿ Done æ—¶çš„çŠ¶æ€ S_t
                traces['next_raw_curr'] = traces['raw_curr'][-1]
                traces['next_raw_hist'] = traces['raw_hist'][-1]
                
                # 2. å¼¹å‡ºæ‰€æœ‰åˆ—è¡¨çš„æœ€åä¸€ä¸ªå…ƒç´  (åˆ é™¤ A_t)
                for k in ['raw_curr', 'raw_hist', 'actions', 'weight_idx', 'logp_joint', 'terminated', 'truncated']:
                    traces[k].pop()
                
                # æ­¤æ—¶ len(actions) å‡å°‘ 1ï¼Œä¸ len(rewards) å®Œç¾å¯¹é½
                
            else:
                # === æƒ…å†µ B: Soft End (åªæ˜¯æ—¶é—´åˆ°äº†) ===
                # æ•°æ®è¿˜æ²¡å®Œï¼Œæˆ‘ä»¬å¯ä»¥å†èµ°ä¸€æ­¥(Probe)æ¥ç»“ç®—æœ€åä¸€æ­¥åŠ¨ä½œ A_t
                
                # 1. æ¢æµ‹æ­¥
                hold_actions = np.zeros(len(self.env_fns), dtype=int)
                hold_weights = np.zeros(len(self.env_fns), dtype=float)
                _, _, final_rews, _, _, _ = vec_env.step(hold_actions, hold_weights)
                
                # 2. è¡¥é½ Reward (ç»™ A_t)
                rew_tensor = torch.as_tensor(final_rews, dtype=torch.float32, device=self.device)
                traces['rewards'].append(rew_tensor)
                epoch_rewards.append(final_rews)
                
                # 3. è®¾ç½® Next State
                traces['next_raw_curr'] = torch.as_tensor(curr_np, dtype=torch.float32, device=self.device)
                traces['next_raw_hist'] = torch.as_tensor(hist_np, dtype=torch.float32, device=self.device)

            # --- æœ€ç»ˆæ ¡éªŒ ---
            n_act = len(traces['actions'])
            n_rew = len(traces['rewards'])
            assert n_act == n_rew, f"å¯¹é½ä¸¥é‡é”™è¯¯: Act={n_act}, Rew={n_rew}, HardEnd={is_hard_end}"

            # 6. æ›´æ–° (å¦‚æœæ•°æ®æœ‰æ•ˆ)
            if n_act > 0:
                loss, kl, actor_loss, value_loss, entropy = self.ppo.update_parallel(traces)
            else:
                loss, kl, actor_loss, value_loss, entropy = 0, 0, 0, 0, 0

            # --- Log & Export ---
            end_time = time.time()
            fps = (n_act * len(self.env_fns)) / (end_time - start_time + 1e-5)
            
            if len(epoch_rewards) > 0:
                avg_rew = np.mean(np.concatenate(epoch_rewards))
            else:
                avg_rew = 0.0
            avg_equity = np.mean(current_equities)

            if n_act > 0:
                all_actions = torch.stack(traces['actions']).cpu().numpy().flatten()
                counts = np.bincount(all_actions, minlength=4)
                ratios = counts / (len(all_actions) + 1e-8)

                all_weights = torch.stack(traces['weight_idx']).cpu().numpy().flatten()
                weight_counts = np.bincount(all_weights, minlength=4)
                weight_ratios = weight_counts / (len(all_weights) + 1e-8)

            else:
                ratios = [0, 0, 0, 0]

            self.records['epoch'].append(epoch + 1)
            self.records['reward'].append(avg_rew)
            self.records['avg_equity'].append(avg_equity)
            self.records['loss'].append(loss)
            self.records['kl'].append(kl)
            self.records['actor_loss'].append(actor_loss)
            self.records['value_loss'].append(value_loss)
            self.records['entropy'].append(entropy)
            self.records['hold_ratio'].append(ratios[0])
            self.records['long_ratio'].append(ratios[1])
            self.records['short_ratio'].append(ratios[2])
            self.records['close_ratio'].append(ratios[3])
            
            names = ['ratio_0', 'ratio_25', 'ratio_50', 'ratio_75', 'ratio_100']
            for k in range(len(names)):
                self.records[names[k]].append(weight_ratios[k])

            df = pd.DataFrame(self.records)
            excel_path = f'{DESK_PATH}/PPO_training_data.xlsx'
            try:
                df.to_excel(excel_path, index=False)
                print(f"ğŸŒŸ Save records to {excel_path}")
            except:
                pass

            if (epoch + 1) % self.cfg.print_interval == 0:
                print(f"[Epoch {epoch+1}/{self.cfg.max_epochs}] "
                      f"Rew: {avg_rew:.2f} | "
                      f"Val: {avg_equity:.0f} | "
                      f"Act: H{ratios[0]:.2f}/L{ratios[1]:.2f}/S{ratios[2]:.2f}/C{ratios[3]:.2f} | "
                      f"Ent: {entropy:.2f} | "
                      f"KL: {kl:.4f} | "
                      f"FPS: {fps:.0f}")
            
            # ==========================================
            # [æ–°å¢] æ—©åœé€»è¾‘æ ¸å¿ƒå®ç°
            # ==========================================
            # 1. æ£€æŸ¥ Reward æ˜¯å¦åˆ›æ–°é«˜ (å¼•å…¥ min_delta é˜²æ­¢å¾®å°æŠ–åŠ¨)
            if avg_rew > best_reward + min_delta:
                best_reward = avg_rew
                early_stop_counter = 0 # é‡ç½®è€å¿ƒå€¼
                self.ppo.save(epoch, best_reward)
                print(f"   >>> ğŸŒŸ Best Reward Updated: {best_reward:.2f} (Counter Reset)")
            else:
                early_stop_counter += 1
                print(f"   â³ [Patience] No improvement: {early_stop_counter}/{patience} | Best: {best_reward:.2f}")

            # 2. è§¦å‘æ¡ä»¶ A: è€å¿ƒè€—å°½
            if early_stop_counter >= patience:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Reward has not improved for {patience} epochs.")
                print(f"   Final Best Reward: {best_reward:.2f}")
                break
            
            # 3. è§¦å‘æ¡ä»¶ B: ç†µè¿‡ä½ (ç­–ç•¥å›ºåŒ–)
            if entropy < stop_entropy and avg_rew > 0:
                print(f"\nğŸ›‘ [Early Stop] Triggered! Entropy ({entropy:.4f}) is too low, policy has converged.")
                break
            # ==========================================

        print(f"[Train] Finished. Data saved to {excel_path}")
        vec_env.close()


    # æµ‹è¯•
    def test(self, epochs: int = 5, alpha: float = 0.5, test_mode: bool = False):
        res = []
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        for epoch in range(epochs):
            # å’Œ train ä¸€æ ·çš„ reset é€»è¾‘
            current_state, history_state, _ = self.env.reset()

            # ç¬¬ä¸€æ¬¡å¯ä»¥ç”¨é»˜è®¤ cal_dim=Trueï¼Œåç»­ç”¨ cal_dim=False
            current_state = torch.as_tensor(current_state, dtype=torch.float32, device=device)
            history_state = torch.as_tensor(history_state, dtype=torch.float32, device=device)
            
            if current_state.dim() == 1:
                current_state =  current_state.unsqueeze(0)
            
            if history_state.dim() == 2:
                history_state = history_state.unsqueeze(0)

            state = self.ppo.extract_features_batch(current_state, history_state, cal_dim=True)

            done = False
            total_reward = 0.0

            action_list = []
            weight_list = []

            for t in range(self.cfg.max_timesteps):
                # å’Œ train ä¸€æ ·ï¼šç›´æ¥ç”¨ç‰¹å¾ state ä½œä¸ºè¾“å…¥
                # å¤šä¼ ä¸€ä¸ª test=test_modeï¼Œç”¨äºâ€œæµ‹è¯•/è¯„ä¼°æ¨¡å¼â€ï¼ˆä¾‹å¦‚ç”¨å‡å€¼è€Œä¸æ˜¯é‡‡æ ·ï¼‰
                a, w_idx, w_val, _, _, _ = self.ppo.selete_action_and_weight(
                    state, test=test_mode
                )

                # è½¬æˆ Python æ ‡é‡ï¼Œå’Œ train é€»è¾‘ä¿æŒä¸€è‡´
                action = int(a.item()) if hasattr(a, "item") else int(a)
                w_val_scalar = float(w_val.item()) if hasattr(w_val, "item") else float(w_val)

                # è®°å½•ä¸€ä¸‹ï¼Œåé¢ç»Ÿè®¡åŠ¨ä½œé¢‘ç‡ç”¨
                action_list.append(action)
                weight_list.append(w_val_scalar)

                # ç¯å¢ƒåƒâ€œå®é™…æƒé‡å€¼â€ï¼ˆè€Œä¸æ˜¯ç´¢å¼•ï¼‰
                current_state, history_state, reward, terminated, truncated = \
                    self.env.step(action, w_val_scalar)
                
                current_state = torch.as_tensor(current_state, dtype=torch.float32, device=device)
                history_state = torch.as_tensor(history_state, dtype=torch.float32, device=device)
                
                if current_state.dim() == 1:
                    current_state =  current_state.unsqueeze(0)
                
                if history_state.dim() == 2:
                    history_state = history_state.unsqueeze(0)


                done = terminated or truncated

                # æå–ä¸‹ä¸€æ­¥ç‰¹å¾ï¼ˆè¿™é‡Œ cal_dim=Falseï¼Œå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                next_state = self.ppo.extract_features_batch(
                    current_state, history_state, cal_dim=False
                )

                # å¦‚æœç»“æŸï¼ŒåŠ ä¸ŠåŸºäºæœ€ç»ˆæƒç›Šçš„é¢å¤– reward
                if done:
                    e = self.env.account_controller.equity
                    init_capital = self.env.account_controller.init_capital
                    reward += alpha * (e - init_capital) / init_capital

                total_reward += reward
                state = next_state

                if done:
                    break

            # ===== åŠ¨ä½œç»Ÿè®¡ï¼ˆç”¨æ•´æ¡è½¨è¿¹ï¼Œè€Œä¸æ˜¯æœ€åä¸€æ­¥ï¼‰ =====
            if len(action_list) > 0:
                unique, counts = np.unique(action_list, return_counts=True)
                freq = {int(u): int(c) for u, c in zip(unique, counts)}
            else:
                freq = {}

            maps = {
                0: 'HOLD',
                1: 'LONG',
                2: 'SHORT',
                3: 'CLOSE'
            }
            for k, v in freq.items():
                print(f"{maps.get(k, k)} -> {v} | ", end='')
            print("\n")

            value = self.env.account_controller.equity
            print(f"[Info: Test model] epoch: {epoch + 1} | "
                  f"Reward: {total_reward:.4f} | Market-Value: {value}")
            res.append((value, total_reward))

        self.env.close()
        return res



def save_norm(agent, filepath: str='./miniQMT/DL/checkout/norm.pkl'):
    data = {
        'reward_norm': agent.ppo.reward_norm,
        'state_norm': agent.ppo.state_norm,
    }
    
    torch.save(data, filepath)


def read_norm(filepath: str = './miniQMT/DL/checkout/norm_fixed.pkl'):
    """
    åŠ è½½ç”± torch.save ä¿å­˜çš„å½’ä¸€åŒ–å‚æ•°
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    loaded_data = torch.load(filepath, map_location=device, weights_only=False)
    
    return loaded_data['reward_norm'], loaded_data['state_norm']

if __name__ == '__main__':
    mp.set_start_method('spawn', force=True) 

    pairs = []
    pairs.append({'call': '10007709', 'call_strike': 2.25, 'call_expire': '20241023', 'put': '10007718', 'put_strike': 2.25, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007710', 'call_strike': 2.3, 'call_expire': '20241023', 'put': '10007719', 'put_strike': 2.3, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007711', 'call_strike': 2.35, 'call_expire': '20241023', 'put': '10007720', 'put_strike': 2.35, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007712', 'call_strike': 2.4, 'call_expire': '20241023', 'put': '10007721', 'put_strike': 2.4, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007713', 'call_strike': 2.45, 'call_expire': '20241023', 'put': '10007722', 'put_strike': 2.45, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007714', 'call_strike': 2.5, 'call_expire': '20241023', 'put': '10007723', 'put_strike': 2.5, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007715', 'call_strike': 2.55, 'call_expire': '20241023', 'put': '10007724', 'put_strike': 2.55, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007716', 'call_strike': 2.6, 'call_expire': '20241023', 'put': '10007725', 'put_strike': 2.6, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10007717', 'call_strike': 2.65, 'call_expire': '20241023', 'put': '10007726', 'put_strike': 2.65, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'})
    pairs.append({'call': '10008505', 'call_strike': 2.5, 'call_expire': '20250122', 'put': '10008514', 'put_strike': 2.5, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008506', 'call_strike': 2.55, 'call_expire': '20250122', 'put': '10008515', 'put_strike': 2.55, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008507', 'call_strike': 2.6, 'call_expire': '20250122', 'put': '10008516', 'put_strike': 2.6, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008508', 'call_strike': 2.65, 'call_expire': '20250122', 'put': '10008517', 'put_strike': 2.65, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008509', 'call_strike': 2.7, 'call_expire': '20250122', 'put': '10008518', 'put_strike': 2.7, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008510', 'call_strike': 2.75, 'call_expire': '20250122', 'put': '10008519', 'put_strike': 2.75, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008511', 'call_strike': 2.8, 'call_expire': '20250122', 'put': '10008520', 'put_strike': 2.8, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008512', 'call_strike': 2.85, 'call_expire': '20250122', 'put': '10008521', 'put_strike': 2.85, 'put_expire': '20250122', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008545', 'call_strike': 2.7, 'call_expire': '20250625', 'put': '10008554', 'put_strike': 2.7, 'put_expire': '20250625', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008547', 'call_strike': 2.8, 'call_expire': '20250625', 'put': '10008556', 'put_strike': 2.8, 'put_expire': '20250625', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008548', 'call_strike': 2.85, 'call_expire': '20250625', 'put': '10008557', 'put_strike': 2.85, 'put_expire': '20250625', 'call_open': '20241202', 'put_open': '20241202'})
    pairs.append({'call': '10008573', 'call_strike': 2.55, 'call_expire': '20250226', 'put': '10008582', 'put_strike': 2.55, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008574', 'call_strike': 2.6, 'call_expire': '20250226', 'put': '10008583', 'put_strike': 2.6, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008575', 'call_strike': 2.65, 'call_expire': '20250226', 'put': '10008584', 'put_strike': 2.65, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008576', 'call_strike': 2.7, 'call_expire': '20250226', 'put': '10008585', 'put_strike': 2.7, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008577', 'call_strike': 2.75, 'call_expire': '20250226', 'put': '10008586', 'put_strike': 2.75, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008578', 'call_strike': 2.8, 'call_expire': '20250226', 'put': '10008587', 'put_strike': 2.8, 'put_expire': '20250226', 'call_open': '20241226', 'put_open': '20241226'})
    pairs.append({'call': '10008676', 'call_strike': 2.5, 'call_expire': '20250226', 'put': '10008678', 'put_strike': 2.5, 'put_expire': '20250226', 'call_open': '20250103', 'put_open': '20250103'})
    pairs.append({'call': '10008801', 'call_strike': 2.8, 'call_expire': '20250924', 'put': '10008810', 'put_strike': 2.8, 'put_expire': '20250924', 'call_open': '20250123', 'put_open': '20250123'})
    pairs.append({'call': '10008885', 'call_strike': 2.85, 'call_expire': '20250924', 'put': '10008886', 'put_strike': 2.85, 'put_expire': '20250924', 'call_open': '20250127', 'put_open': '20250127'})
    pairs.append({'call': '10008895', 'call_strike': 2.9, 'call_expire': '20250924', 'put': '10008896', 'put_strike': 2.9, 'put_expire': '20250924', 'call_open': '20250211', 'put_open': '20250211'})
    pairs.append({'call': '10008905', 'call_strike': 2.95, 'call_expire': '20250924', 'put': '10008906', 'put_strike': 2.95, 'put_expire': '20250924', 'call_open': '20250224', 'put_open': '20250224'})
    pairs.append({'call': '10009039', 'call_strike': 3.0, 'call_expire': '20250924', 'put': '10009040', 'put_strike': 3.0, 'put_expire': '20250924', 'call_open': '20250317', 'put_open': '20250317'})
    pairs.append({'call': '10009325', 'call_strike': 2.65, 'call_expire': '20250723', 'put': '10009334', 'put_strike': 2.65, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009326', 'call_strike': 2.7, 'call_expire': '20250723', 'put': '10009335', 'put_strike': 2.7, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009327', 'call_strike': 2.75, 'call_expire': '20250723', 'put': '10009336', 'put_strike': 2.75, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009328', 'call_strike': 2.8, 'call_expire': '20250723', 'put': '10009337', 'put_strike': 2.8, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009329', 'call_strike': 2.85, 'call_expire': '20250723', 'put': '10009338', 'put_strike': 2.85, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009330', 'call_strike': 2.9, 'call_expire': '20250723', 'put': '10009339', 'put_strike': 2.9, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009331', 'call_strike': 2.95, 'call_expire': '20250723', 'put': '10009340', 'put_strike': 2.95, 'put_expire': '20250723', 'call_open': '20250529', 'put_open': '20250529'})
    pairs.append({'call': '10009495', 'call_strike': 3.1, 'call_expire': '20250924', 'put': '10009496', 'put_strike': 3.1, 'put_expire': '20250924', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009511', 'call_strike': 2.75, 'call_expire': '20250827', 'put': '10009520', 'put_strike': 2.75, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009512', 'call_strike': 2.8, 'call_expire': '20250827', 'put': '10009521', 'put_strike': 2.8, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009513', 'call_strike': 2.85, 'call_expire': '20250827', 'put': '10009522', 'put_strike': 2.85, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009514', 'call_strike': 2.9, 'call_expire': '20250827', 'put': '10009523', 'put_strike': 2.9, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009515', 'call_strike': 2.95, 'call_expire': '20250827', 'put': '10009524', 'put_strike': 2.95, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009516', 'call_strike': 3.0, 'call_expire': '20250827', 'put': '10009525', 'put_strike': 3.0, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009517', 'call_strike': 3.1, 'call_expire': '20250827', 'put': '10009526', 'put_strike': 3.1, 'put_expire': '20250827', 'call_open': '20250626', 'put_open': '20250626'})
    pairs.append({'call': '10009619', 'call_strike': 3.2, 'call_expire': '20250827', 'put': '10009620', 'put_strike': 3.2, 'put_expire': '20250827', 'call_open': '20250721', 'put_open': '20250721'})
    pairs.append({'call': '10009621', 'call_strike': 3.2, 'call_expire': '20250924', 'put': '10009622', 'put_strike': 3.2, 'put_expire': '20250924', 'call_open': '20250721', 'put_open': '20250721'})


    # 202406~202412
    new_option_pairs = [
        {'call': '10006421', 'call_strike': 2.35, 'call_expire': '20240626', 'put': '10006430', 'put_strike': 2.35, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006422', 'call_strike': 2.4, 'call_expire': '20240626', 'put': '10006431', 'put_strike': 2.4, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006423', 'call_strike': 2.45, 'call_expire': '20240626', 'put': '10006432', 'put_strike': 2.45, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006424', 'call_strike': 2.5, 'call_expire': '20240626', 'put': '10006433', 'put_strike': 2.5, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006425', 'call_strike': 2.55, 'call_expire': '20240626', 'put': '10006434', 'put_strike': 2.55, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006426', 'call_strike': 2.6, 'call_expire': '20240626', 'put': '10006435', 'put_strike': 2.6, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006427', 'call_strike': 2.65, 'call_expire': '20240626', 'put': '10006436', 'put_strike': 2.65, 'put_expire': '20240626', 'call_open': '20231127', 'put_open': '20231127'},
        {'call': '10006732', 'call_strike': 2.25, 'call_expire': '20240925', 'put': '10006741', 'put_strike': 2.25, 'put_expire': '20240925', 'call_open': '20240125', 'put_open': '20240125'},
        {'call': '10006734', 'call_strike': 2.35, 'call_expire': '20240925', 'put': '10006743', 'put_strike': 2.35, 'put_expire': '20240925', 'call_open': '20240125', 'put_open': '20240125'},
        {'call': '10006735', 'call_strike': 2.4, 'call_expire': '20240925', 'put': '10006744', 'put_strike': 2.4, 'put_expire': '20240925', 'call_open': '20240125', 'put_open': '20240125'},
        {'call': '10006736', 'call_strike': 2.45, 'call_expire': '20240925', 'put': '10006745', 'put_strike': 2.45, 'put_expire': '20240925', 'call_open': '20240125', 'put_open': '20240125'},
        {'call': '10006737', 'call_strike': 2.5, 'call_expire': '20240925', 'put': '10006746', 'put_strike': 2.5, 'put_expire': '20240925', 'call_open': '20240125', 'put_open': '20240125'},
        {'call': '10006819', 'call_strike': 2.55, 'call_expire': '20240925', 'put': '10006820', 'put_strike': 2.55, 'put_expire': '20240925', 'call_open': '20240126', 'put_open': '20240126'},
        {'call': '10007333', 'call_strike': 2.35, 'call_expire': '20240724', 'put': '10007342', 'put_strike': 2.35, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007334', 'call_strike': 2.4, 'call_expire': '20240724', 'put': '10007343', 'put_strike': 2.4, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007335', 'call_strike': 2.45, 'call_expire': '20240724', 'put': '10007344', 'put_strike': 2.45, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007336', 'call_strike': 2.5, 'call_expire': '20240724', 'put': '10007345', 'put_strike': 2.5, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007337', 'call_strike': 2.55, 'call_expire': '20240724', 'put': '10007346', 'put_strike': 2.55, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007338', 'call_strike': 2.6, 'call_expire': '20240724', 'put': '10007347', 'put_strike': 2.6, 'put_expire': '20240724', 'call_open': '20240523', 'put_open': '20240523'},
        {'call': '10007452', 'call_strike': 2.3, 'call_expire': '20240828', 'put': '10007461', 'put_strike': 2.3, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007453', 'call_strike': 2.35, 'call_expire': '20240828', 'put': '10007462', 'put_strike': 2.35, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007454', 'call_strike': 2.4, 'call_expire': '20240828', 'put': '10007463', 'put_strike': 2.4, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007455', 'call_strike': 2.45, 'call_expire': '20240828', 'put': '10007464', 'put_strike': 2.45, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007456', 'call_strike': 2.5, 'call_expire': '20240828', 'put': '10007465', 'put_strike': 2.5, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007457', 'call_strike': 2.55, 'call_expire': '20240828', 'put': '10007466', 'put_strike': 2.55, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007458', 'call_strike': 2.6, 'call_expire': '20240828', 'put': '10007467', 'put_strike': 2.6, 'put_expire': '20240828', 'call_open': '20240627', 'put_open': '20240627'},
        {'call': '10007709', 'call_strike': 2.25, 'call_expire': '20241023', 'put': '10007718', 'put_strike': 2.25, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007710', 'call_strike': 2.3, 'call_expire': '20241023', 'put': '10007719', 'put_strike': 2.3, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007711', 'call_strike': 2.35, 'call_expire': '20241023', 'put': '10007720', 'put_strike': 2.35, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007712', 'call_strike': 2.4, 'call_expire': '20241023', 'put': '10007721', 'put_strike': 2.4, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007713', 'call_strike': 2.45, 'call_expire': '20241023', 'put': '10007722', 'put_strike': 2.45, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007714', 'call_strike': 2.5, 'call_expire': '20241023', 'put': '10007723', 'put_strike': 2.5, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007715', 'call_strike': 2.55, 'call_expire': '20241023', 'put': '10007724', 'put_strike': 2.55, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007716', 'call_strike': 2.6, 'call_expire': '20241023', 'put': '10007725', 'put_strike': 2.6, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007717', 'call_strike': 2.65, 'call_expire': '20241023', 'put': '10007726', 'put_strike': 2.65, 'put_expire': '20241023', 'call_open': '20240829', 'put_open': '20240829'},
        {'call': '10007866', 'call_strike': 2.55, 'call_expire': '20241127', 'put': '10007875', 'put_strike': 2.55, 'put_expire': '20241127', 'call_open': '20240926', 'put_open': '20240926'},
        {'call': '10007867', 'call_strike': 2.6, 'call_expire': '20241127', 'put': '10007876', 'put_strike': 2.6, 'put_expire': '20241127', 'call_open': '20240926', 'put_open': '20240926'},
        {'call': '10007868', 'call_strike': 2.65, 'call_expire': '20241127', 'put': '10007877', 'put_strike': 2.65, 'put_expire': '20241127', 'call_open': '20240926', 'put_open': '20240926'},
        {'call': '10007869', 'call_strike': 2.7, 'call_expire': '20241127', 'put': '10007878', 'put_strike': 2.7, 'put_expire': '20241127', 'call_open': '20240926', 'put_open': '20240926'},
        {'call': '10007955', 'call_strike': 2.75, 'call_expire': '20241127', 'put': '10007957', 'put_strike': 2.75, 'put_expire': '20241127', 'call_open': '20240927', 'put_open': '20240927'},
        {'call': '10007956', 'call_strike': 2.8, 'call_expire': '20241127', 'put': '10007958', 'put_strike': 2.8, 'put_expire': '20241127', 'call_open': '20240927', 'put_open': '20240927'},
        {'call': '10007985', 'call_strike': 2.85, 'call_expire': '20241127', 'put': '10007987', 'put_strike': 2.85, 'put_expire': '20241127', 'call_open': '20240930', 'put_open': '20240930'},
        {'call': '10007986', 'call_strike': 2.9, 'call_expire': '20241127', 'put': '10007988', 'put_strike': 2.9, 'put_expire': '20241127', 'call_open': '20240930', 'put_open': '20240930'},
        {'call': '10008047', 'call_strike': 2.95, 'call_expire': '20241127', 'put': '10008051', 'put_strike': 2.95, 'put_expire': '20241127', 'call_open': '20241008', 'put_open': '20241008'},
        {'call': '10008048', 'call_strike': 3.0, 'call_expire': '20241127', 'put': '10008052', 'put_strike': 3.0, 'put_expire': '20241127', 'call_open': '20241008', 'put_open': '20241008'}
    ]



    start_time: str='20250317100000'
    end_time: str='20250617150000'
    option_pairs = []
    option_pairs.append({'call': '10008545', 'put': '10008554'})
    option_pairs.append({'call': '10008547', 'put': '10008556'})
    option_pairs.append({'call': '10008548', 'put': '10008557'})
    option_pairs.append({'call': '10008801', 'put': '10008810'})
    option_pairs.append({'call': '10008885', 'put': '10008886'})
    option_pairs.append({'call': '10008895', 'put': '10008896'})
    option_pairs.append({'call': '10008905', 'put': '10008906'})
    option_pairs.append({'call': '10009039', 'put': '10009040'})

    cfg = AgentConfig(action_dim=4, option_pairs=option_pairs, start_time=start_time, end_time=end_time)
    agent = Agent(cfg)
    agent.train_parallel_modified_early_stop()
    # save_norm(agent)

    # æµ‹è¯•
    # idx = 2
    # call, put = option_pairs[idx]['call'], option_pairs[idx]['put']

    # call, put = '10006038', '10006029'
    # start_time = '20230928100000'
    # end_time = '20231122150000'

    # calls = [calls[1]]
    # puts = [puts[1]]

    print(f"[PPO-Agent] å¼€å§‹æµ‹è¯•......")
    # for idx in range(len(option_pairs)):
    # call, put = option_pairs[idx]['call'], option_pairs[idx]['put']
    # start_time = option_pairs[idx]['call_open'] + '100000'
    # end_time = option_pairs[idx]['call_expire'] + '150000'

    call, put = '10006819', '10006820'
    start_time = '20240201100000'
    end_time = '20240505150000'

    call, put = '10007866', '10007875'
    start_time = '20240926100000'
    end_time = '20241113150000'

    # call, put = '10008545', '10008554'
    # start_time: str='20250317100000'
    # end_time: str='20250617150000'

    call, put = '10008934', '10008943'
    start_time = '20250227100000'
    end_time = '20250410150000'


    # start_time: str='20250317100000'
    # end_time: str='20250617150000'
    # option_pairs = []
    # option_pairs.append({'call': '10008545', 'put': '10008554'})
    option_pairs = [{'call': call, 'put': put}]
    cfg = AgentConfig(action_dim=4, option_pairs=option_pairs, start_time=start_time, end_time=end_time)
    agent = Agent(cfg)

    max_timesteps = 1000
    fee = 1.3
    benchmark ='510050'
    env = windowEnv(100000, call, put, max_timesteps, fee, start_time, end_time, benchmark, normalize_reward=False)
    reward_norm, state_norm = read_norm()
    
    agent.set_env(env)
    agent.set_norm(state_norm)
    # åŠ è½½æ¨¡å‹æƒé‡
    agent.ppo.load_infer_parameters()
    
    res = agent.test(test_mode=False, epochs=3)

    print(0 / 0)