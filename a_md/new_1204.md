---
marp: true
# 可选设置：定义主题或背景
# theme: uncover
math: katex
---

# 基于 PAB-VarTST 与 Skill-PPO 的期权波动率交易策略研究
## 硕士研究生学位论文开题报告

**汇报人：** 许骏飞 ｜ **日期：** 2025.12.04

---

# 目录（Agenda）

1. **研究背景、问题定位与研究意义**
2. **研究现状与关键不足**
3. **核心方法论：PAB-VarTST 与 Skill-PPO**
4. **状态融合与工程实现**
5. **工作进展与下一步计划**

---

# Ⅰ. 研究背景、问题定位与工程思考

## 1.1 选题原因与研究意义（1）

- **期权交易的复杂性**
  - 高杠杆、非线性收益结构
  - 多元风险敞口（**Greeks：$\Delta, \Gamma, \Theta, \nu$** 等）
- **传统量化模型的局限**
  - 难以处理**高维时序特征**与**强噪声环境**
  - 对**稀疏收益信号**适应性不足
- **研究目标**
  - 构建一个**可泛化、可解释**的深度强化学习（DRL）框架
  - 面向**期权波动率交易策略**的系统性工程研究

---

# Ⅰ. 研究背景、问题定位与工程思考

## 1.1 选题原因与研究意义（2）

本研究重点解决三类**核心工程挑战**：

1. **特征与决策解耦**
   - 问题：PPO 稀疏奖励难以直接训练高参数量的时序特征提取器  
   - 目标：通过**预训练 + 对比学习**，先学特征、再接 PPO

2. **策略可控性与风格稳定**
   - 问题：端到端 DRL 策略往往**不可解释、风格漂移严重**
   - 目标：引入 **Skill-PPO + Greeks 对齐**，约束风险暴露风格

---
# Ⅰ. 研究背景、问题定位与工程思考
3. **多组合泛化能力**
   - 问题：不同期权组合之间结构差异大，策略泛化困难
   - 目标：形成可处理**变长/固定数量组合**的通用策略框架
---
# Ⅰ. 研究背景、问题定位与工程思考
## 1.2 研究现状与不足

| 领域 | 现有研究现状 | 存在的主要不足 |
| :--- | :--- | :--- |
| **时序特征提取** | Transformer / iTransformer / BasisFormer：利用自注意力机制捕捉序列依赖。 | 对**多体制、高噪声金融序列**适应性不足，易退化，泛化性不稳定。 |
| **深度强化学习** | PPO / SAC / CDT：用于一般序列决策；Skill-PPO：用于分层与技能学习。 | 奖励稀疏时收敛困难；缺乏将决策与**金融风险**显式对齐的机制。 |
| **策略建模** | 多基于 Black-Scholes、GARCH 或简单 MLP / LSTM。 | 难以处理**海量历史窗口**和**多变量非线性关系**，难以刻画完整风险面。 |

---

# Ⅱ. 核心方法论与训练框架

## 总体思路

- **PAB-VarTST 特征学习器**
  - 利用**异构模型融合**（VARMA + BasisFormer + iTransformer）
  - 通过 **Policy-Adaptive** 机制对齐 RL 目标

- **Skill-PPO 分层决策架构**
  - 结合 **CDT 预训练 + Manager-Expert 分层执行**
  - 采用 **Greeks 对齐 Reward Shaping** 提升可解释性与风格控制

---

# Ⅱ. 核心方法论与训练框架

## 2.1 PAB-VarTST 特征学习器：整体概念

**PAB-VarTST**（Policy-Adaptive Basis with VARMA + Inverted TST）目标：

- 通过**异构模型融合**和**策略自适应对齐**，
- 提取具有**高鲁棒性、高可解释性**的市场状态表示。

关键思路：

1. 融合多种**归纳偏置**，避免单一模型退化；
2. 使用 **RL-weighted InfoNCE** 将预训练目标与 RL 收益显式绑定；
3. 将输出作为 PPO 的**共享特征空间**，供后续策略模块调用。

---

# Ⅱ. 核心方法论与训练框架

## 2.1 PAB-VarTST：特征融合架构（MoE）

采用 **MoE（Mixture of Experts）** 思路，融合多类专家：

- **VARMA-VFE 专家**
  - 捕获短期的**局部统计依赖**（AR/MA 动态）
  - 对应**高频波动与局部噪声**的建模

- **BasisFormer 专家**
  - 利用可学习的**基底函数**捕捉中长期趋势与结构
  - 提供对**周期性、缓慢漂移**的解释能力

---
# Ⅱ. 核心方法论与训练框架

## 2.1 PAB-VarTST：特征融合架构（MoE）
- **iTransformer 范式专家**
  - 对输入进行**变量维度转置**
  - 聚焦于多变量特征（**Greeks、IV、TTM** 等）之间的**相关性结构**

---

# Ⅱ. 核心方法论与训练框架

## 2.1 PAB-VarTST：策略自适应对齐（RL-weighted InfoNCE）

- **问题**：传统预训练目标（如预测误差）与 RL 收益目标之间存在**目标错配**。
- **方法**：  
  - 在预训练阶段引入 **RL-weighted InfoNCE Loss**：
    - InfoNCE 中的样本权重来自 PPO 的**优势函数 $A(s, a)$**。
  - 直观理解：  
    - 被策略证明为“**高价值状态**”在对比学习中权重更大。
- **效果**：
  - 强制特征提取器**关注对收益更敏感的状态区域**
  - 弥合预训练与 RL 目标之间的鸿沟，实现真正的 **Policy-Adaptive Basis**。

---

# Ⅱ. 核心方法论与训练框架

## 2.2 Skill-PPO 分层决策架构：整体框架

为提升策略的**可控性与可解释性**，采用 **Skill-PPO** 分层架构：

1. **策略初始化（CDT Pre-training）**
   - 使用历史交易轨迹，通过 **Causal Decision Transformer (CDT)** 进行预训练
   - 利用 **Return-to-Go** 信号进行序列建模，为 PPO 提供**良好初始化**

2. **分层执行（Manager-Expert）**
   - **Manager（管理者）**：根据当前市场状态选择合适的 **Expert（专家技能）**
   - **Experts（工人）**：每个 Expert 对应一种预设的**交易风格**或**风控偏好**
---
# Ⅱ. 核心方法论与训练框架

## 2.2 Skill-PPO 分层决策架构：整体框架
3. **Greeks 对齐奖励（Reward Shaping）**
   - 在奖励函数中加入对 **Greeks 暴露**的软约束

---

# Ⅱ. 核心方法论与训练框架

## 2.2 Skill-PPO：Greeks 对齐 Reward Shaping

- **核心思想**：  
  将每个 Expert 的行为与其目标 **Greeks 风格**显式绑定，例如：
  - **$\Delta$ 中性专家**：惩罚高 $\Delta$ 暴露；
  - **Long Vega 专家**：鼓励在低成本条件下获得正 Vega；
  - **Short Gamma 专家**：配合严格风控，控制极端波动风险。

- **Reward Shaping 形式（示意）**：
  - $$R_{\text{total}} = R_{\text{PnL}} - \lambda_\Delta \cdot |\Delta - \Delta^*| - \lambda_\nu \cdot |\nu - \nu^*| - \dots$$
---
# Ⅱ. 核心方法论与训练框架

## 2.2 Skill-PPO：Greeks 对齐 Reward Shaping

- **优势**：
  - 提升策略的**可解释性**（每个 Expert 对应清晰的风险风格）
  - 增强对**风险敞口**的精细控制，符合期权交易实际需求

---

# Ⅲ. 状态融合与工程实现

## 3.1 状态融合架构设计：总体思路

遵循 **“分层处理、高维缩减、低维保留”** 原则，设计 PPO 最终状态：

$$
\mathbf{S}_{\text{Final}} = 
[\mathbf{S}_{\text{Adapter}} \oplus \mathbf{S}_{\text{Portfolio}} \oplus \mathbf{S}_{\text{Account}}]
$$

- **$\mathbf{S}_{\text{Adapter}}$**：来自高维期权特征的降维表示  
- **$\mathbf{S}_{\text{Portfolio}}$**：组合级别的交易与持仓信息  
- **$\mathbf{S}_{\text{Account}}$**：账户级别的资金与收益约束信息  

---

# Ⅲ. 状态融合与工程实现
## 3.1 状态融合架构设计：信息源与处理方式

| 信息源 | 维度 / 特性 | 处理方式 | 目的 |
| :--- | :--- | :--- | :--- |
| **期权基本信息** $\mathbf{O}_{\text{MoE}}$ | 高维(包含多窗口、跨合约) | $\mathbf{O}_{\text{MoE}} \xrightarrow{\text{Adapter (MLP)}} \mathbf{S}_{\text{Adapter}}$（降维） | 压缩冗余，提取**市场认知**与结构特征。 |
| **期权交易信息** $\mathbf{S}_{\text{Portfolio}}$ | 低维，高语义 | 直接拼接（不缩减） | 保留**局部决策信息**|
| **账户整体信息** $\mathbf{S}_{\text{Account}}$ | 极低维（可用资金比例、冻结资金等） | 直接拼接（不缩减） | 提供**全局风险与流动性约束**，指导风险控制。 |

---
# Ⅲ. 状态融合与工程实现

## 3.1 状态融合架构设计：工程考量

- 将**高维特征缩减**集中在 **MoE Adapter** 阶段完成：
  - 减少 PPO 主网络的输入维度；
  - 降低训练噪声和不稳定性。
- 保证 PPO 网络接收的：
  - **$\mathbf{S}_{\text{Portfolio}}$** 与  **$\mathbf{S}_{\text{Account}}$**  
  具有**清晰语义与稳定分布**，从而：
  - 加速 PPO 收敛；
  - 提高策略在实盘场景中的可解释性与可调试性。

---

# Ⅲ. 状态融合与工程实现

## 3.2 多组合处理策略（聚焦当前）

考虑到**信号微弱**与**收敛难度**，采用 **渐进式扩展策略**：

1. **核心训练阶段**
   - PPO 策略仅处理**单个期权组合**；
   - 奖励信号为该组合产生的**局部收益**，信号强且清晰。

2. **初步扩展阶段**
   - 部署时，将总账户资金平分给 $N$ 个策略副本；
   - 通过**策略复用**实现多组合管理。
---
# Ⅲ. 状态融合与工程实现

## 3.2 多组合处理策略（聚焦当前）
3. **高级扩展阶段**
   - 引入独立、轻量级的 **MLP 分配网络（Allocator）**；
   - 学习动态资金分配策略，奖励信号为账户**总收益**。

> 该设计有效避免了在 PPO 训练初期就引入复杂结构和微弱信号导致的**收敛失败**。

---

# Ⅵ. 工作进展与下一步计划

## 4.1 已完成的工作

- **基础 RL 框架**
  - 基于 PPO 算法，完成支持 **MoE / Adapter 模式** 的训练框架搭建。
- **环境类封装**
  - 实现 `single_window_account` 类：
    - 支持单期权组合的账户逻辑；
    - 支持组合的设置、修改与动态跟踪。
- **特征预训练框架**
  - 搭建 **VARMAformer 的 VFE 模块** 与 **BasisFormer 基本结构**；
  - 完成独立预训练与测试，验证预测能力。

---
# Ⅵ. 工作进展与下一步计划

## 4.1 已完成的工作
- **数据处理流水线**
  - 完成期权数据的**相对化处理**与**滚动抽象机制**实现。

---

# Ⅵ. 工作进展与下一步计划

## 4.2 当前挑战与下一步计划

**主要挑战**

1. **PAB-VarTST 联合训练**
   - 如何高效实现 MoE Router，将预训练专家权重无缝接入 PPO；
   - 在此基础上实现 **RL-weighted InfoNCE** 的联合微调。

2. **Skill 奖励设计**
   - 设计合理的 **基于 Greeks 暴露的 Reward Shaping 函数**；
   - 验证各 Experts 是否能够学习到预期的风险风格（如 Long Vega / Delta Neutral 等）。

---

# Ⅵ. 工作进展与下一步计划

##  下一步计划（Roadmap）

- **短期计划**
  - 完成 **MoE Router** 的实现与 PAB-VarTST 联合训练实验；
  - 构建“**无预训练 vs. 完整 PAB-VarTST**”等消融实验基线。

- **中期计划**
  - 实现完整的 **Skill-PPO 架构**：
    - Manager-Expert 分层路由；
    - 初步的 Greeks 对齐 Reward Shaping。

---
# Ⅵ. 工作进展与下一步计划

##  下一步计划（Roadmap）
- **长期计划**
  - 将单组合策略平滑扩展到 **多组合、多账户场景**；
  - 探索在真实或半真实市场数据上的**稳健性与实用性**评估。

---
